{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Spark on Kubernetes (Apache Spark 3.1.1-rc1) \u00b6 Welcome to The Internals of Spark on Kubernetes online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark on Kubernetes \ud83d\udd25","title":"Welcome"},{"location":"#the-internals-of-spark-on-kubernetes-apache-spark-311-rc1","text":"Welcome to The Internals of Spark on Kubernetes online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark on Kubernetes \ud83d\udd25","title":"The Internals of Spark on Kubernetes (Apache Spark 3.1.1-rc1)"},{"location":"BasicDriverFeatureStep/","text":"BasicDriverFeatureStep \u00b6 BasicDriverFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 BasicDriverFeatureStep takes the following to be created: KubernetesDriverConf BasicDriverFeatureStep is created when: KubernetesDriverBuilder is requested for the driver pod spec Name of Driver Pod \u00b6 BasicDriverFeatureStep uses the spark.kubernetes.driver.pod.name configuration property (if defined) or the resourceNamePrefix (of the given KubernetesDriverConf ) as the name of the driver pod: [resourceNamePrefix]-driver Driver Memory \u00b6 BasicDriverFeatureStep calculates the driver memory for the driver pod based on the Base Driver Memory with the extra Non-Heap Memory Overhead . Driver memory is the quantity (in Mi s) of the driver memory resource (the request and limit). Demo: Running Spark Examples on minikube Use kubectl get po [driverPod] to review the memory resource spec. Learn more in Demo: Running Spark Examples on minikube . Base Driver Memory \u00b6 BasicDriverFeatureStep uses spark.driver.memory configuration property (default: 1g ) for the base driver memory . Memory Overhead Factor \u00b6 BasicDriverFeatureStep uses different memory overhead factors based on the MainAppResource (of the given KubernetesDriverConf ): For JVM applications (Java, Scala, etc.), it is spark.kubernetes.memoryOverheadFactor configuration property For non-JVM applications (Python and R applications), it is spark.kubernetes.memoryOverheadFactor configuration property (if defined) or 0.4 The memory overhead factor is used for the following: Additional System Properties Memory Overhead Non-Heap Memory Overhead \u00b6 BasicDriverFeatureStep defines the memory overhead for a non-heap memory to be allocated per driver in cluster mode (in MiB s) based on the following: spark.driver.memoryOverhead configuration property (if defined) Maximum of the Memory Overhead Factor multiplied by the Base Driver Memory and 384 Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional properties: Name Value spark.kubernetes.submitInDriver true spark.kubernetes.driver.pod.name driverPodName spark.kubernetes.memoryOverheadFactor overheadFactor spark.app.id appId (of the KubernetesDriverConf ) In the end, getAdditionalPodSystemProperties uploads local files (in spark.jars and spark.files configuration properties) to a Hadoop-compatible file system and adds their target Hadoop paths to the additional properties. spark.kubernetes.file.upload.path Configuration Property The Hadoop DFS path to upload local files to is defined using spark.kubernetes.file.upload.path configuration property. Driver Container Image Name \u00b6 BasicDriverFeatureStep uses spark.kubernetes.driver.container.image for the name of the container image for drivers. The name must be defined or BasicDriverFeatureStep throws an SparkException : Must specify the driver container image driverContainerImage is used when requested for configurePod . Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#basicdriverfeaturestep","text":"BasicDriverFeatureStep is a KubernetesFeatureConfigStep .","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#creating-instance","text":"BasicDriverFeatureStep takes the following to be created: KubernetesDriverConf BasicDriverFeatureStep is created when: KubernetesDriverBuilder is requested for the driver pod spec","title":"Creating Instance"},{"location":"BasicDriverFeatureStep/#name-of-driver-pod","text":"BasicDriverFeatureStep uses the spark.kubernetes.driver.pod.name configuration property (if defined) or the resourceNamePrefix (of the given KubernetesDriverConf ) as the name of the driver pod: [resourceNamePrefix]-driver","title":" Name of Driver Pod"},{"location":"BasicDriverFeatureStep/#driver-memory","text":"BasicDriverFeatureStep calculates the driver memory for the driver pod based on the Base Driver Memory with the extra Non-Heap Memory Overhead . Driver memory is the quantity (in Mi s) of the driver memory resource (the request and limit). Demo: Running Spark Examples on minikube Use kubectl get po [driverPod] to review the memory resource spec. Learn more in Demo: Running Spark Examples on minikube .","title":" Driver Memory"},{"location":"BasicDriverFeatureStep/#base-driver-memory","text":"BasicDriverFeatureStep uses spark.driver.memory configuration property (default: 1g ) for the base driver memory .","title":" Base Driver Memory"},{"location":"BasicDriverFeatureStep/#memory-overhead-factor","text":"BasicDriverFeatureStep uses different memory overhead factors based on the MainAppResource (of the given KubernetesDriverConf ): For JVM applications (Java, Scala, etc.), it is spark.kubernetes.memoryOverheadFactor configuration property For non-JVM applications (Python and R applications), it is spark.kubernetes.memoryOverheadFactor configuration property (if defined) or 0.4 The memory overhead factor is used for the following: Additional System Properties Memory Overhead","title":" Memory Overhead Factor"},{"location":"BasicDriverFeatureStep/#non-heap-memory-overhead","text":"BasicDriverFeatureStep defines the memory overhead for a non-heap memory to be allocated per driver in cluster mode (in MiB s) based on the following: spark.driver.memoryOverhead configuration property (if defined) Maximum of the Memory Overhead Factor multiplied by the Base Driver Memory and 384","title":" Non-Heap Memory Overhead"},{"location":"BasicDriverFeatureStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional properties: Name Value spark.kubernetes.submitInDriver true spark.kubernetes.driver.pod.name driverPodName spark.kubernetes.memoryOverheadFactor overheadFactor spark.app.id appId (of the KubernetesDriverConf ) In the end, getAdditionalPodSystemProperties uploads local files (in spark.jars and spark.files configuration properties) to a Hadoop-compatible file system and adds their target Hadoop paths to the additional properties. spark.kubernetes.file.upload.path Configuration Property The Hadoop DFS path to upload local files to is defined using spark.kubernetes.file.upload.path configuration property.","title":" Additional System Properties"},{"location":"BasicDriverFeatureStep/#driver-container-image-name","text":"BasicDriverFeatureStep uses spark.kubernetes.driver.container.image for the name of the container image for drivers. The name must be defined or BasicDriverFeatureStep throws an SparkException : Must specify the driver container image driverContainerImage is used when requested for configurePod .","title":" Driver Container Image Name"},{"location":"BasicDriverFeatureStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Driver Pod"},{"location":"BasicExecutorFeatureStep/","text":"BasicExecutorFeatureStep \u00b6 BasicExecutorFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 BasicExecutorFeatureStep takes the following to be created: KubernetesExecutorConf SecurityManager (Apache Spark) BasicExecutorFeatureStep is created when: KubernetesExecutorBuilder is requested to build a pod spec for executors Executor Container Image Name \u00b6 BasicExecutorFeatureStep asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a SparkException : Must specify the executor container image Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#basicexecutorfeaturestep","text":"BasicExecutorFeatureStep is a KubernetesFeatureConfigStep .","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#creating-instance","text":"BasicExecutorFeatureStep takes the following to be created: KubernetesExecutorConf SecurityManager (Apache Spark) BasicExecutorFeatureStep is created when: KubernetesExecutorBuilder is requested to build a pod spec for executors","title":"Creating Instance"},{"location":"BasicExecutorFeatureStep/#executor-container-image-name","text":"BasicExecutorFeatureStep asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a SparkException : Must specify the executor container image","title":" Executor Container Image Name"},{"location":"BasicExecutorFeatureStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Pod"},{"location":"Client/","text":"Client \u00b6 Client submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status). Creating Instance \u00b6 Client takes the following to be created: KubernetesDriverConf KubernetesDriverBuilder Kubernetes' KubernetesClient LoggingPodStatusWatcher Client is created when: KubernetesClientApplication is requested to start Running Driver Pod \u00b6 run () : Unit run requests the KubernetesDriverBuilder to build a KubernetesDriverSpec . run requests the KubernetesDriverConf for the resourceNamePrefix to be used for the name of the driver's config map: [resourceNamePrefix]-driver-conf-map run builds a ConfigMap (with the name and the system properties of the KubernetesDriverSpec ). run creates a driver container (based on the KubernetesDriverSpec ) and adds the following: SPARK_CONF_DIR env var as /opt/spark/conf spark-conf-volume volume mount as /opt/spark/conf run creates a driver pod (based on the KubernetesDriverSpec ) with the driver container and a new spark-conf-volume volume for the ConfigMap . run requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher ) and, when available, attaches the ConfigMap . run is used when: KubernetesClientApplication is requested to start addDriverOwnerReference \u00b6 addDriverOwnerReference ( driverPod : Pod , resources : Seq [ HasMetadata ]) : Unit addDriverOwnerReference ...FIXME Building ConfigMap \u00b6 buildConfigMap ( configMapName : String , conf : Map [ String , String ]) : ConfigMap buildConfigMap builds a Kubernetes ConfigMap with the given conf key-value pairs stored as spark.properties data and the given configMapName name. The stored data uses an extra comment: Java properties built from Kubernetes config map with name: [configMapName] Kubernetes Documentation Learn more about ConfigMaps in the official Kubernetes Documentation . Demo: Spark and Local Filesystem in minikube Learn more about ConfigMaps and volumes in Demo: Spark and Local Filesystem in minikube .","title":"Client"},{"location":"Client/#client","text":"Client submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status).","title":"Client"},{"location":"Client/#creating-instance","text":"Client takes the following to be created: KubernetesDriverConf KubernetesDriverBuilder Kubernetes' KubernetesClient LoggingPodStatusWatcher Client is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"Client/#running-driver-pod","text":"run () : Unit run requests the KubernetesDriverBuilder to build a KubernetesDriverSpec . run requests the KubernetesDriverConf for the resourceNamePrefix to be used for the name of the driver's config map: [resourceNamePrefix]-driver-conf-map run builds a ConfigMap (with the name and the system properties of the KubernetesDriverSpec ). run creates a driver container (based on the KubernetesDriverSpec ) and adds the following: SPARK_CONF_DIR env var as /opt/spark/conf spark-conf-volume volume mount as /opt/spark/conf run creates a driver pod (based on the KubernetesDriverSpec ) with the driver container and a new spark-conf-volume volume for the ConfigMap . run requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher ) and, when available, attaches the ConfigMap . run is used when: KubernetesClientApplication is requested to start","title":" Running Driver Pod"},{"location":"Client/#adddriverownerreference","text":"addDriverOwnerReference ( driverPod : Pod , resources : Seq [ HasMetadata ]) : Unit addDriverOwnerReference ...FIXME","title":" addDriverOwnerReference"},{"location":"Client/#building-configmap","text":"buildConfigMap ( configMapName : String , conf : Map [ String , String ]) : ConfigMap buildConfigMap builds a Kubernetes ConfigMap with the given conf key-value pairs stored as spark.properties data and the given configMapName name. The stored data uses an extra comment: Java properties built from Kubernetes config map with name: [configMapName] Kubernetes Documentation Learn more about ConfigMaps in the official Kubernetes Documentation . Demo: Spark and Local Filesystem in minikube Learn more about ConfigMaps and volumes in Demo: Spark and Local Filesystem in minikube .","title":" Building ConfigMap"},{"location":"ClientArguments/","text":"ClientArguments \u00b6 ClientArguments represents a KubernetesClientApplication to start. Creating Instance \u00b6 ClientArguments takes the following to be created: MainAppResource Name of the main class of a Spark application to run Driver Arguments ClientArguments is created (via fromCommandLineArgs utility) when: KubernetesClientApplication is requested to start fromCommandLineArgs Utility \u00b6 fromCommandLineArgs ( args : Array [ String ]) : ClientArguments fromCommandLineArgs slices the input args into key-value pairs and creates a ClientArguments as follows: --primary-java-resource , --primary-py-file or --primary-r-file keys are used for the MainAppResource --main-class for the name of the main class --arg for the driver arguments Important The main class must be specified via --main-class or fromCommandLineArgs throws an IllegalArgumentException . fromCommandLineArgs is used when: KubernetesClientApplication is requested to start","title":"ClientArguments"},{"location":"ClientArguments/#clientarguments","text":"ClientArguments represents a KubernetesClientApplication to start.","title":"ClientArguments"},{"location":"ClientArguments/#creating-instance","text":"ClientArguments takes the following to be created: MainAppResource Name of the main class of a Spark application to run Driver Arguments ClientArguments is created (via fromCommandLineArgs utility) when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"ClientArguments/#fromcommandlineargs-utility","text":"fromCommandLineArgs ( args : Array [ String ]) : ClientArguments fromCommandLineArgs slices the input args into key-value pairs and creates a ClientArguments as follows: --primary-java-resource , --primary-py-file or --primary-r-file keys are used for the MainAppResource --main-class for the name of the main class --arg for the driver arguments Important The main class must be specified via --main-class or fromCommandLineArgs throws an IllegalArgumentException . fromCommandLineArgs is used when: KubernetesClientApplication is requested to start","title":" fromCommandLineArgs Utility"},{"location":"DriverCommandFeatureStep/","text":"DriverCommandFeatureStep \u00b6 DriverCommandFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 DriverCommandFeatureStep takes the following to be created: KubernetesDriverConf DriverCommandFeatureStep is created when: KubernetesDriverBuilder is requested to build a KubernetesDriverSpec from features Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod branches off based on the MainAppResource (of the KubernetesDriverConf ): For JavaMainAppResource , configurePod configures a pod for Java with the primary resource (if defined) or uses spark-internal special value For PythonMainAppResource , configurePod configures a pod for Python with the primary resource For RMainAppResource , configurePod configures a pod for R with the primary resource Configuring Pod for Java Application \u00b6 configureForJava ( pod : SparkPod , res : String ) : SparkPod configureForJava builds the base driver container for the given SparkPod and the primary resource. In the end, configureForJava creates another SparkPod (for the pod of the given SparkPod ) and the driver container. Configuring Pod for Python Application \u00b6 configureForPython ( pod : SparkPod , res : String ) : SparkPod configureForPython ...FIXME Configuring Pod for R Application \u00b6 configureForR ( pod : SparkPod , res : String ) : SparkPod configureForR ...FIXME Base Driver ContainerBuilder \u00b6 baseDriverContainer ( pod : SparkPod , resource : String ) : ContainerBuilder baseDriverContainer renames the given primary resource if the MainAppResource is a JavaMainAppResource . Otherwise, baseDriverContainer leaves the primary resource as-is. baseDriverContainer creates a ContainerBuilder (for the pod of the given SparkPod ) and adds the following arguments (in that order): driver --properties-file with /opt/spark/conf/spark.properties --class with the mainClass of the KubernetesDriverConf the primary resource (possibly renamed if a MainAppResource ) appArgs of the KubernetesDriverConf Note The arguments are then used by the default entrypoint.sh of the official Docker image of Apache Spark (in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/ ). Use the following kubectl command to see the arguments: kubectl get po [driverPod] -o=jsonpath='{.spec.containers[0].args}'","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#drivercommandfeaturestep","text":"DriverCommandFeatureStep is a KubernetesFeatureConfigStep .","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#creating-instance","text":"DriverCommandFeatureStep takes the following to be created: KubernetesDriverConf DriverCommandFeatureStep is created when: KubernetesDriverBuilder is requested to build a KubernetesDriverSpec from features","title":"Creating Instance"},{"location":"DriverCommandFeatureStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod branches off based on the MainAppResource (of the KubernetesDriverConf ): For JavaMainAppResource , configurePod configures a pod for Java with the primary resource (if defined) or uses spark-internal special value For PythonMainAppResource , configurePod configures a pod for Python with the primary resource For RMainAppResource , configurePod configures a pod for R with the primary resource","title":" Configuring Pod"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-java-application","text":"configureForJava ( pod : SparkPod , res : String ) : SparkPod configureForJava builds the base driver container for the given SparkPod and the primary resource. In the end, configureForJava creates another SparkPod (for the pod of the given SparkPod ) and the driver container.","title":" Configuring Pod for Java Application"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-python-application","text":"configureForPython ( pod : SparkPod , res : String ) : SparkPod configureForPython ...FIXME","title":" Configuring Pod for Python Application"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-r-application","text":"configureForR ( pod : SparkPod , res : String ) : SparkPod configureForR ...FIXME","title":" Configuring Pod for R Application"},{"location":"DriverCommandFeatureStep/#base-driver-containerbuilder","text":"baseDriverContainer ( pod : SparkPod , resource : String ) : ContainerBuilder baseDriverContainer renames the given primary resource if the MainAppResource is a JavaMainAppResource . Otherwise, baseDriverContainer leaves the primary resource as-is. baseDriverContainer creates a ContainerBuilder (for the pod of the given SparkPod ) and adds the following arguments (in that order): driver --properties-file with /opt/spark/conf/spark.properties --class with the mainClass of the KubernetesDriverConf the primary resource (possibly renamed if a MainAppResource ) appArgs of the KubernetesDriverConf Note The arguments are then used by the default entrypoint.sh of the official Docker image of Apache Spark (in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/ ). Use the following kubectl command to see the arguments: kubectl get po [driverPod] -o=jsonpath='{.spec.containers[0].args}'","title":" Base Driver ContainerBuilder"},{"location":"DriverKubernetesCredentialsFeatureStep/","text":"DriverKubernetesCredentialsFeatureStep \u00b6 DriverKubernetesCredentialsFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 DriverKubernetesCredentialsFeatureStep takes the following to be created: KubernetesConf DriverKubernetesCredentialsFeatureStep is created when: KubernetesDriverBuilder is requested for the driver pod spec Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":"DriverKubernetesCredentialsFeatureStep"},{"location":"DriverKubernetesCredentialsFeatureStep/#driverkubernetescredentialsfeaturestep","text":"DriverKubernetesCredentialsFeatureStep is a KubernetesFeatureConfigStep .","title":"DriverKubernetesCredentialsFeatureStep"},{"location":"DriverKubernetesCredentialsFeatureStep/#creating-instance","text":"DriverKubernetesCredentialsFeatureStep takes the following to be created: KubernetesConf DriverKubernetesCredentialsFeatureStep is created when: KubernetesDriverBuilder is requested for the driver pod spec","title":"Creating Instance"},{"location":"DriverKubernetesCredentialsFeatureStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Driver Pod"},{"location":"DriverServiceFeatureStep/","text":"DriverServiceFeatureStep \u00b6 DriverServiceFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 DriverServiceFeatureStep takes the following to be created: KubernetesDriverConf Clock (default: SystemClock ) DriverServiceFeatureStep is created when: KubernetesDriverBuilder is requested for a driver pod spec Service Name \u00b6 DriverServiceFeatureStep defines Service Name based on the Preferred Service Name (if fits the name limit) or generates one: spark-[randomServiceId]-driver-svc DriverServiceFeatureStep prints out the following WARN message when generating the service name: Driver's hostname would preferably be [preferredServiceName], but this is too long (must be <= 63 characters). Falling back to use $shorterServiceName as the driver service's name. The service name is used for the following: Additional System Properties (as spark.driver.host ) Additional Kubernetes Resources Preferred Service Name \u00b6 DriverServiceFeatureStep uses the resourceNamePrefix (of the given KubernetesDriverConf ) with -driver-svc postfix as the Preferred Service Name . [resourceNamePrefix]-driver-svc The preferred service name becomes the resolved service name only when shorter than 63 characters. Additional Kubernetes Resources \u00b6 getAdditionalKubernetesResources () : Seq [ HasMetadata ] getAdditionalKubernetesResources is part of the KubernetesFeatureConfigStep abstraction. getAdditionalKubernetesResources defines a Kubernetes service with the Service Name . kubectl get services Use k get services to list Kubernetes services. Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional Spark properties: Name Value spark.driver.host serviceName . namespace .svc spark.driver.port driverPort spark.driver.blockManager.port driverBlockManagerPort Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep=ALL Refer to Logging .","title":"DriverServiceFeatureStep"},{"location":"DriverServiceFeatureStep/#driverservicefeaturestep","text":"DriverServiceFeatureStep is a KubernetesFeatureConfigStep .","title":"DriverServiceFeatureStep"},{"location":"DriverServiceFeatureStep/#creating-instance","text":"DriverServiceFeatureStep takes the following to be created: KubernetesDriverConf Clock (default: SystemClock ) DriverServiceFeatureStep is created when: KubernetesDriverBuilder is requested for a driver pod spec","title":"Creating Instance"},{"location":"DriverServiceFeatureStep/#service-name","text":"DriverServiceFeatureStep defines Service Name based on the Preferred Service Name (if fits the name limit) or generates one: spark-[randomServiceId]-driver-svc DriverServiceFeatureStep prints out the following WARN message when generating the service name: Driver's hostname would preferably be [preferredServiceName], but this is too long (must be <= 63 characters). Falling back to use $shorterServiceName as the driver service's name. The service name is used for the following: Additional System Properties (as spark.driver.host ) Additional Kubernetes Resources","title":" Service Name"},{"location":"DriverServiceFeatureStep/#preferred-service-name","text":"DriverServiceFeatureStep uses the resourceNamePrefix (of the given KubernetesDriverConf ) with -driver-svc postfix as the Preferred Service Name . [resourceNamePrefix]-driver-svc The preferred service name becomes the resolved service name only when shorter than 63 characters.","title":" Preferred Service Name"},{"location":"DriverServiceFeatureStep/#additional-kubernetes-resources","text":"getAdditionalKubernetesResources () : Seq [ HasMetadata ] getAdditionalKubernetesResources is part of the KubernetesFeatureConfigStep abstraction. getAdditionalKubernetesResources defines a Kubernetes service with the Service Name . kubectl get services Use k get services to list Kubernetes services.","title":" Additional Kubernetes Resources"},{"location":"DriverServiceFeatureStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional Spark properties: Name Value spark.driver.host serviceName . namespace .svc spark.driver.port driverPort spark.driver.blockManager.port driverBlockManagerPort","title":" Additional System Properties"},{"location":"DriverServiceFeatureStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Driver Pod"},{"location":"DriverServiceFeatureStep/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep=ALL Refer to Logging .","title":"Logging"},{"location":"EnvSecretsFeatureStep/","text":"EnvSecretsFeatureStep \u00b6 EnvSecretsFeatureStep is...FIXME","title":"EnvSecretsFeatureStep"},{"location":"EnvSecretsFeatureStep/#envsecretsfeaturestep","text":"EnvSecretsFeatureStep is...FIXME","title":"EnvSecretsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/","text":"ExecutorKubernetesCredentialsFeatureStep \u00b6 ExecutorKubernetesCredentialsFeatureStep is...FIXME spark.kubernetes.authenticate.executor.serviceAccountName \u00b6 ExecutorKubernetesCredentialsFeatureStep uses spark.kubernetes.authenticate.executor.serviceAccountName configuration property when requested to configure a pod . Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod buildPodWithServiceAccount when...FIXME","title":"ExecutorKubernetesCredentialsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#executorkubernetescredentialsfeaturestep","text":"ExecutorKubernetesCredentialsFeatureStep is...FIXME","title":"ExecutorKubernetesCredentialsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#sparkkubernetesauthenticateexecutorserviceaccountname","text":"ExecutorKubernetesCredentialsFeatureStep uses spark.kubernetes.authenticate.executor.serviceAccountName configuration property when requested to configure a pod .","title":" spark.kubernetes.authenticate.executor.serviceAccountName"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod buildPodWithServiceAccount when...FIXME","title":" Configuring Pod"},{"location":"ExecutorPodsAllocator/","text":"ExecutorPodsAllocator \u00b6 ExecutorPodsAllocator is responsible for allocating pods for executors (possibly dynamic ) in a Spark application. ExecutorPodsAllocator is used to create a KubernetesClusterSchedulerBackend . Creating Instance \u00b6 ExecutorPodsAllocator takes the following to be created: SparkConf SecurityManager KubernetesExecutorBuilder KubernetesClient ExecutorPodsSnapshotsStore Clock ExecutorPodsAllocator is created when: KubernetesClusterManager is requested for a SchedulerBackend Executor Pod Allocation Timeout \u00b6 ExecutorPodsAllocator defines Executor Pod Allocation Timeout that is the maximum of the following values: 5 times of spark.kubernetes.allocation.batch.delay configuration property spark.kubernetes.allocation.executor.timeout configuration property ExecutorPodsAllocator uses the allocation timeout to detect \"old\" executor pod requests when handling executor pods snapshots . spark.dynamicAllocation.enabled \u00b6 ExecutorPodsAllocator uses spark.dynamicAllocation.enabled configuration property to turn dynamic allocation of executors on and off. The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark . Driver Pod \u00b6 driverPod : Option [ Pod ] driverPod is a driver pod with the name of spark.kubernetes.driver.pod.name configuration property (if defined). ExecutorPodsAllocator throws a SparkException when the driver pod could not be found in a Kubernetes cluster: No pod was found named [kubernetesDriverPodName] in the cluster in the namespace [namespace] (this was supposed to be the driver pod.). spark.kubernetes.driver.pod.name \u00b6 ExecutorPodsAllocator uses spark.kubernetes.driver.pod.name configuration property to look up the driver pod by name when created . spark.kubernetes.allocation.batch.size \u00b6 ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.size configuration property in the following: onNewSnapshots spark.kubernetes.allocation.batch.delay \u00b6 ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.delay configuration property for the following: podCreationTimeout Registering a subscriber spark.kubernetes.executor.deleteOnTermination \u00b6 ExecutorPodsAllocator uses spark.kubernetes.executor.deleteOnTermination configuration property. Starting \u00b6 start ( applicationId : String ) : Unit start requests the ExecutorPodsSnapshotsStore to subscribe this ExecutorPodsAllocator to be notified about new snapshots (with pod allocation delay based on spark.kubernetes.allocation.batch.delay configuration property). start is used when: KubernetesClusterSchedulerBackend is requested to start Processing Executor Pods Snapshots \u00b6 onNewSnapshots ( applicationId : String , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots removes the executor IDs (of the executor pods in the given snapshots) from the newlyCreatedExecutors internal registry. onNewSnapshots finds timed-out executor IDs (in the newlyCreatedExecutors internal registry) whose creation time exceeded some podCreationTimeout threshold. For the other executor IDs, onNewSnapshots prints out the following DEBUG message to the logs: Executor with id [execId] was not found in the Kubernetes cluster since it was created [time] milliseconds ago. For any timed-out executor IDs, onNewSnapshots prints out the following WARN message to the logs: Executors with ids [ids] were not detected in the Kubernetes cluster after [podCreationTimeout] ms despite the fact that a previous allocation attempt tried to create them. The executors may have been deleted but the application missed the deletion event. onNewSnapshots removes ( forgets ) the timed-out executor IDs (from the newlyCreatedExecutors internal registry). With the shouldDeleteExecutors flag enabled, onNewSnapshots requests the KubernetesClient to delete pods with the following labels: spark-app-selector with the given applicationId spark-role = executor spark-exec-id for all timed-out executor IDs onNewSnapshots updates the lastSnapshot internal registry with the last ExecutorPodsSnapshot among the given snapshots if available. onNewSnapshots counts running executor pods in the lastSnapshot internal registry. onNewSnapshots finds pending executor IDs in the lastSnapshot internal registry. For non-empty input snapshots , onNewSnapshots prints out the following DEBUG message to the logs: Pod allocation status: [currentRunningCount] running, [currentPendingExecutors] pending, [newlyCreatedExecutors] unacknowledged. onNewSnapshots ...FIXME In the end, with DEBUG logging enabled or the input snapshots is empty, onNewSnapshots prints out the following DEBUG messages. With the number of the executor pods currently running higher than the total expected executors but no dynamicAllocationEnabled , onNewSnapshots prints out the following: Current number of running executors is equal to the number of requested executors. Not scaling up further. Otherwise, when there are executor pods pending ( outstanding ), onNewSnapshots prints out the following: Still waiting for [outstanding] executors before requesting more. Total Expected Executors \u00b6 totalExpectedExecutors : AtomicInteger ExecutorPodsAllocator uses a Java AtomicInteger to track the total expected number of executors. Starts from 0 and is set to a fixed number of the total expected executors in setTotalExpectedExecutors Used in onNewSnapshots Changing Total Expected Executors \u00b6 setTotalExpectedExecutors ( total : Int ) : Unit setTotalExpectedExecutors sets totalExpectedExecutors internal registry to the input total . With no hasPendingPods , setTotalExpectedExecutors requests the ExecutorPodsSnapshotsStore to notifySubscribers . setTotalExpectedExecutors is used when: KubernetesClusterSchedulerBackend is requested to start and doRequestTotalExecutors Registries \u00b6 newlyCreatedExecutors \u00b6 newlyCreatedExecutors : Map [ Long , Long ] ExecutorPodsAllocator uses newlyCreatedExecutors internal registry to track executor IDs (with the timestamps they were created) that have been requested from Kubernetes but have not been detected in any snapshot yet. Used in onNewSnapshots EXECUTOR_ID_COUNTER \u00b6 ExecutorPodsAllocator uses a Java AtomicLong for the missing executor IDs that are going to be requested (in onNewSnapshots ) when the number of running executor pods is below the total expected executors . hasPendingPods Flag \u00b6 hasPendingPods : AtomicBoolean ExecutorPodsAllocator uses a Java AtomicBoolean as a flag to avoid notifying subscribers. Starts as false and is updated every onNewSnapshots Used in setTotalExpectedExecutors (only when false ) Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL Refer to Logging .","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#executorpodsallocator","text":"ExecutorPodsAllocator is responsible for allocating pods for executors (possibly dynamic ) in a Spark application. ExecutorPodsAllocator is used to create a KubernetesClusterSchedulerBackend .","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#creating-instance","text":"ExecutorPodsAllocator takes the following to be created: SparkConf SecurityManager KubernetesExecutorBuilder KubernetesClient ExecutorPodsSnapshotsStore Clock ExecutorPodsAllocator is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsAllocator/#executor-pod-allocation-timeout","text":"ExecutorPodsAllocator defines Executor Pod Allocation Timeout that is the maximum of the following values: 5 times of spark.kubernetes.allocation.batch.delay configuration property spark.kubernetes.allocation.executor.timeout configuration property ExecutorPodsAllocator uses the allocation timeout to detect \"old\" executor pod requests when handling executor pods snapshots .","title":" Executor Pod Allocation Timeout"},{"location":"ExecutorPodsAllocator/#sparkdynamicallocationenabled","text":"ExecutorPodsAllocator uses spark.dynamicAllocation.enabled configuration property to turn dynamic allocation of executors on and off. The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark .","title":" spark.dynamicAllocation.enabled"},{"location":"ExecutorPodsAllocator/#driver-pod","text":"driverPod : Option [ Pod ] driverPod is a driver pod with the name of spark.kubernetes.driver.pod.name configuration property (if defined). ExecutorPodsAllocator throws a SparkException when the driver pod could not be found in a Kubernetes cluster: No pod was found named [kubernetesDriverPodName] in the cluster in the namespace [namespace] (this was supposed to be the driver pod.).","title":" Driver Pod"},{"location":"ExecutorPodsAllocator/#sparkkubernetesdriverpodname","text":"ExecutorPodsAllocator uses spark.kubernetes.driver.pod.name configuration property to look up the driver pod by name when created .","title":" spark.kubernetes.driver.pod.name"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchsize","text":"ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.size configuration property in the following: onNewSnapshots","title":" spark.kubernetes.allocation.batch.size"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchdelay","text":"ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.delay configuration property for the following: podCreationTimeout Registering a subscriber","title":" spark.kubernetes.allocation.batch.delay"},{"location":"ExecutorPodsAllocator/#sparkkubernetesexecutordeleteontermination","text":"ExecutorPodsAllocator uses spark.kubernetes.executor.deleteOnTermination configuration property.","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"ExecutorPodsAllocator/#starting","text":"start ( applicationId : String ) : Unit start requests the ExecutorPodsSnapshotsStore to subscribe this ExecutorPodsAllocator to be notified about new snapshots (with pod allocation delay based on spark.kubernetes.allocation.batch.delay configuration property). start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsAllocator/#processing-executor-pods-snapshots","text":"onNewSnapshots ( applicationId : String , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots removes the executor IDs (of the executor pods in the given snapshots) from the newlyCreatedExecutors internal registry. onNewSnapshots finds timed-out executor IDs (in the newlyCreatedExecutors internal registry) whose creation time exceeded some podCreationTimeout threshold. For the other executor IDs, onNewSnapshots prints out the following DEBUG message to the logs: Executor with id [execId] was not found in the Kubernetes cluster since it was created [time] milliseconds ago. For any timed-out executor IDs, onNewSnapshots prints out the following WARN message to the logs: Executors with ids [ids] were not detected in the Kubernetes cluster after [podCreationTimeout] ms despite the fact that a previous allocation attempt tried to create them. The executors may have been deleted but the application missed the deletion event. onNewSnapshots removes ( forgets ) the timed-out executor IDs (from the newlyCreatedExecutors internal registry). With the shouldDeleteExecutors flag enabled, onNewSnapshots requests the KubernetesClient to delete pods with the following labels: spark-app-selector with the given applicationId spark-role = executor spark-exec-id for all timed-out executor IDs onNewSnapshots updates the lastSnapshot internal registry with the last ExecutorPodsSnapshot among the given snapshots if available. onNewSnapshots counts running executor pods in the lastSnapshot internal registry. onNewSnapshots finds pending executor IDs in the lastSnapshot internal registry. For non-empty input snapshots , onNewSnapshots prints out the following DEBUG message to the logs: Pod allocation status: [currentRunningCount] running, [currentPendingExecutors] pending, [newlyCreatedExecutors] unacknowledged. onNewSnapshots ...FIXME In the end, with DEBUG logging enabled or the input snapshots is empty, onNewSnapshots prints out the following DEBUG messages. With the number of the executor pods currently running higher than the total expected executors but no dynamicAllocationEnabled , onNewSnapshots prints out the following: Current number of running executors is equal to the number of requested executors. Not scaling up further. Otherwise, when there are executor pods pending ( outstanding ), onNewSnapshots prints out the following: Still waiting for [outstanding] executors before requesting more.","title":" Processing Executor Pods Snapshots"},{"location":"ExecutorPodsAllocator/#total-expected-executors","text":"totalExpectedExecutors : AtomicInteger ExecutorPodsAllocator uses a Java AtomicInteger to track the total expected number of executors. Starts from 0 and is set to a fixed number of the total expected executors in setTotalExpectedExecutors Used in onNewSnapshots","title":" Total Expected Executors"},{"location":"ExecutorPodsAllocator/#changing-total-expected-executors","text":"setTotalExpectedExecutors ( total : Int ) : Unit setTotalExpectedExecutors sets totalExpectedExecutors internal registry to the input total . With no hasPendingPods , setTotalExpectedExecutors requests the ExecutorPodsSnapshotsStore to notifySubscribers . setTotalExpectedExecutors is used when: KubernetesClusterSchedulerBackend is requested to start and doRequestTotalExecutors","title":" Changing Total Expected Executors"},{"location":"ExecutorPodsAllocator/#registries","text":"","title":"Registries"},{"location":"ExecutorPodsAllocator/#newlycreatedexecutors","text":"newlyCreatedExecutors : Map [ Long , Long ] ExecutorPodsAllocator uses newlyCreatedExecutors internal registry to track executor IDs (with the timestamps they were created) that have been requested from Kubernetes but have not been detected in any snapshot yet. Used in onNewSnapshots","title":" newlyCreatedExecutors"},{"location":"ExecutorPodsAllocator/#executor_id_counter","text":"ExecutorPodsAllocator uses a Java AtomicLong for the missing executor IDs that are going to be requested (in onNewSnapshots ) when the number of running executor pods is below the total expected executors .","title":" EXECUTOR_ID_COUNTER"},{"location":"ExecutorPodsAllocator/#haspendingpods-flag","text":"hasPendingPods : AtomicBoolean ExecutorPodsAllocator uses a Java AtomicBoolean as a flag to avoid notifying subscribers. Starts as false and is updated every onNewSnapshots Used in setTotalExpectedExecutors (only when false )","title":" hasPendingPods Flag"},{"location":"ExecutorPodsAllocator/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsLifecycleManager/","text":"ExecutorPodsLifecycleManager \u00b6 Creating Instance \u00b6 ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ). Configuration Properties \u00b6 spark.kubernetes.executor.eventProcessingInterval \u00b6 ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME spark.kubernetes.executor.deleteOnTermination \u00b6 ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState . Missing Pod Timeout \u00b6 ExecutorPodsLifecycleManager defines Missing Pod Timeout based on the spark.kubernetes.executor.missingPodDetectDelta configuration property. ExecutorPodsLifecycleManager uses the timeout to detect lost executor pods when handling executor pods snapshots . Starting \u00b6 start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when KubernetesClusterSchedulerBackend is started . Processing Executor Pods Snapshots \u00b6 onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots creates an empty execIdsRemovedInThisRound collection of executors to be removed. onNewSnapshots walks over the input ExecutorPodsSnapshot s and branches off based on ExecutorPodState : For PodDeleted , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots removeExecutorFromSpark and adds the executor ID to the execIdsRemovedInThisRound local collection. For PodFailed , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. For PodSucceeded , onNewSnapshots requests the input KubernetesClusterSchedulerBackend to isExecutorActive . If so, onNewSnapshots prints out the following INFO message to the logs: Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed. Otherwise, onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName]. onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. onFinalNonDeletedState \u00b6 onFinalNonDeletedState ( podState : FinalPodState , execId : Long , schedulerBackend : KubernetesClusterSchedulerBackend , deleteFromK8s : Boolean ) : Boolean onFinalNonDeletedState removeExecutorFromSpark (and records the deleted return flag to be returned in the end). With the given deleteFromK8s flag enabled, onFinalNonDeletedState removeExecutorFromK8s . removeExecutorFromSpark \u00b6 removeExecutorFromSpark ( schedulerBackend : KubernetesClusterSchedulerBackend , podState : FinalPodState , execId : Long ) : Unit removeExecutorFromSpark ...FIXME removeExecutorFromK8s \u00b6 removeExecutorFromK8s ( execId : Long , updatedPod : Pod ) : Unit removeExecutorFromK8s ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL Refer to Logging .","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#executorpodslifecyclemanager","text":"","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#creating-instance","text":"ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ).","title":"Creating Instance"},{"location":"ExecutorPodsLifecycleManager/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutoreventprocessinginterval","text":"ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutordeleteontermination","text":"ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState .","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"ExecutorPodsLifecycleManager/#missing-pod-timeout","text":"ExecutorPodsLifecycleManager defines Missing Pod Timeout based on the spark.kubernetes.executor.missingPodDetectDelta configuration property. ExecutorPodsLifecycleManager uses the timeout to detect lost executor pods when handling executor pods snapshots .","title":" Missing Pod Timeout"},{"location":"ExecutorPodsLifecycleManager/#starting","text":"start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when KubernetesClusterSchedulerBackend is started .","title":" Starting"},{"location":"ExecutorPodsLifecycleManager/#processing-executor-pods-snapshots","text":"onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots creates an empty execIdsRemovedInThisRound collection of executors to be removed. onNewSnapshots walks over the input ExecutorPodsSnapshot s and branches off based on ExecutorPodState : For PodDeleted , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots removeExecutorFromSpark and adds the executor ID to the execIdsRemovedInThisRound local collection. For PodFailed , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. For PodSucceeded , onNewSnapshots requests the input KubernetesClusterSchedulerBackend to isExecutorActive . If so, onNewSnapshots prints out the following INFO message to the logs: Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed. Otherwise, onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName]. onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection.","title":" Processing Executor Pods Snapshots"},{"location":"ExecutorPodsLifecycleManager/#onfinalnondeletedstate","text":"onFinalNonDeletedState ( podState : FinalPodState , execId : Long , schedulerBackend : KubernetesClusterSchedulerBackend , deleteFromK8s : Boolean ) : Boolean onFinalNonDeletedState removeExecutorFromSpark (and records the deleted return flag to be returned in the end). With the given deleteFromK8s flag enabled, onFinalNonDeletedState removeExecutorFromK8s .","title":" onFinalNonDeletedState"},{"location":"ExecutorPodsLifecycleManager/#removeexecutorfromspark","text":"removeExecutorFromSpark ( schedulerBackend : KubernetesClusterSchedulerBackend , podState : FinalPodState , execId : Long ) : Unit removeExecutorFromSpark ...FIXME","title":" removeExecutorFromSpark"},{"location":"ExecutorPodsLifecycleManager/#removeexecutorfromk8s","text":"removeExecutorFromK8s ( execId : Long , updatedPod : Pod ) : Unit removeExecutorFromK8s ...FIXME","title":" removeExecutorFromK8s"},{"location":"ExecutorPodsLifecycleManager/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsPollingSnapshotSource/","text":"ExecutorPodsPollingSnapshotSource \u00b6 ExecutorPodsPollingSnapshotSource manages pollingFuture (for PollRunnable s) to synchronize executor pods state every spark.kubernetes.executor.apiPollingInterval (by requesting the ExecutorPodsSnapshotsStore to replace executor pods snapshot ). Creating Instance \u00b6 ExecutorPodsPollingSnapshotSource takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Java ScheduledExecutorService ExecutorPodsPollingSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ) spark.kubernetes.executor.apiPollingInterval \u00b6 ExecutorPodsPollingSnapshotSource uses spark.kubernetes.executor.apiPollingInterval configuration property when started (to schedule a PollRunnable for regular executor pod state synchronization). pollingFuture \u00b6 pollingFuture : Future [ _ ] pollingFuture ...FIXME Starting \u00b6 start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs (with the pollingInterval ): Starting to check for executor pod state every [pollingInterval] ms. start throws an IllegalArgumentException when started twice (i.e. pollingFuture has already been initialized): Cannot start polling more than once. start is used when: KubernetesClusterSchedulerBackend is requested to start Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL Refer to Logging .","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#executorpodspollingsnapshotsource","text":"ExecutorPodsPollingSnapshotSource manages pollingFuture (for PollRunnable s) to synchronize executor pods state every spark.kubernetes.executor.apiPollingInterval (by requesting the ExecutorPodsSnapshotsStore to replace executor pods snapshot ).","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#creating-instance","text":"ExecutorPodsPollingSnapshotSource takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Java ScheduledExecutorService ExecutorPodsPollingSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend )","title":"Creating Instance"},{"location":"ExecutorPodsPollingSnapshotSource/#sparkkubernetesexecutorapipollinginterval","text":"ExecutorPodsPollingSnapshotSource uses spark.kubernetes.executor.apiPollingInterval configuration property when started (to schedule a PollRunnable for regular executor pod state synchronization).","title":" spark.kubernetes.executor.apiPollingInterval"},{"location":"ExecutorPodsPollingSnapshotSource/#pollingfuture","text":"pollingFuture : Future [ _ ] pollingFuture ...FIXME","title":" pollingFuture"},{"location":"ExecutorPodsPollingSnapshotSource/#starting","text":"start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs (with the pollingInterval ): Starting to check for executor pod state every [pollingInterval] ms. start throws an IllegalArgumentException when started twice (i.e. pollingFuture has already been initialized): Cannot start polling more than once. start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsPollingSnapshotSource/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsSnapshot/","text":"ExecutorPodsSnapshot \u00b6 ExecutorPodsSnapshot is an immutable view ( snapshot ) of the executor pods running in a Kubernetes cluster. ExecutorPodsSnapshot tracks the state of executor pods and can be updated partially or fully . Creating Instance \u00b6 ExecutorPodsSnapshot takes the following to be created: ExecutorPodState by Executor ID ( Map[Long, ExecutorPodState] ) Full Snapshot Timestamp ExecutorPodsSnapshot is created (indirectly using apply utility) when: ExecutorPodsAllocator is created ExecutorPodsSnapshotsStoreImpl is created and requested to replace a snapshot ExecutorPodsSnapshot is requested to update Full Snapshot Timestamp \u00b6 ExecutorPodsSnapshot keeps track of the time of the full snapshot of executor pods. The time is given (via apply ) when: ExecutorPodsSnapshotsStoreImpl is requested to replace a snapshot (and basically do a full synchronization) shouldCheckAllContainers Flag \u00b6 shouldCheckAllContainers : Boolean ExecutorPodsSnapshot uses shouldCheckAllContainers internal flag to control whether or not to include all containers in a running pod when requested for the ExecutorPodState . shouldCheckAllContainers is controlled by spark.kubernetes.executor.checkAllContainers configuration property (when KubernetesClusterManager is requested for a SchedulerBackend ). apply Utility \u00b6 apply () : ExecutorPodsSnapshot apply ( executorPods : Seq [ Pod ], fullSnapshotTs : Long ) : ExecutorPodsSnapshot apply creates a ExecutorPodsSnapshot with the given arguments. apply is used when: ExecutorPodsAllocator is created ExecutorPodsSnapshotsStoreImpl is created and requested to replace a snapshot Updating ExecutorPodsSnapshot \u00b6 withUpdate ( updatedPod : Pod ) : ExecutorPodsSnapshot withUpdate creates a new ExecutorPodsSnapshot with the executorPods updated based on the given executor pod update ( converted ). withUpdate is used when: ExecutorPodsSnapshotsStoreImpl is requested to updatePod toStatesByExecutorId \u00b6 toStatesByExecutorId ( executorPods : Seq [ Pod ]) : Map [ Long , ExecutorPodState ] toStatesByExecutorId ...FIXME toStatesByExecutorId is used when: ExecutorPodsSnapshot is requested to withUpdate and apply toState \u00b6 toState ( pod : Pod ) : ExecutorPodState toState ...FIXME isDeleted \u00b6 isDeleted ( pod : Pod ) : Boolean isDeleted ...FIXME","title":"ExecutorPodsSnapshot"},{"location":"ExecutorPodsSnapshot/#executorpodssnapshot","text":"ExecutorPodsSnapshot is an immutable view ( snapshot ) of the executor pods running in a Kubernetes cluster. ExecutorPodsSnapshot tracks the state of executor pods and can be updated partially or fully .","title":"ExecutorPodsSnapshot"},{"location":"ExecutorPodsSnapshot/#creating-instance","text":"ExecutorPodsSnapshot takes the following to be created: ExecutorPodState by Executor ID ( Map[Long, ExecutorPodState] ) Full Snapshot Timestamp ExecutorPodsSnapshot is created (indirectly using apply utility) when: ExecutorPodsAllocator is created ExecutorPodsSnapshotsStoreImpl is created and requested to replace a snapshot ExecutorPodsSnapshot is requested to update","title":"Creating Instance"},{"location":"ExecutorPodsSnapshot/#full-snapshot-timestamp","text":"ExecutorPodsSnapshot keeps track of the time of the full snapshot of executor pods. The time is given (via apply ) when: ExecutorPodsSnapshotsStoreImpl is requested to replace a snapshot (and basically do a full synchronization)","title":" Full Snapshot Timestamp"},{"location":"ExecutorPodsSnapshot/#shouldcheckallcontainers-flag","text":"shouldCheckAllContainers : Boolean ExecutorPodsSnapshot uses shouldCheckAllContainers internal flag to control whether or not to include all containers in a running pod when requested for the ExecutorPodState . shouldCheckAllContainers is controlled by spark.kubernetes.executor.checkAllContainers configuration property (when KubernetesClusterManager is requested for a SchedulerBackend ).","title":" shouldCheckAllContainers Flag"},{"location":"ExecutorPodsSnapshot/#apply-utility","text":"apply () : ExecutorPodsSnapshot apply ( executorPods : Seq [ Pod ], fullSnapshotTs : Long ) : ExecutorPodsSnapshot apply creates a ExecutorPodsSnapshot with the given arguments. apply is used when: ExecutorPodsAllocator is created ExecutorPodsSnapshotsStoreImpl is created and requested to replace a snapshot","title":" apply Utility"},{"location":"ExecutorPodsSnapshot/#updating-executorpodssnapshot","text":"withUpdate ( updatedPod : Pod ) : ExecutorPodsSnapshot withUpdate creates a new ExecutorPodsSnapshot with the executorPods updated based on the given executor pod update ( converted ). withUpdate is used when: ExecutorPodsSnapshotsStoreImpl is requested to updatePod","title":" Updating ExecutorPodsSnapshot"},{"location":"ExecutorPodsSnapshot/#tostatesbyexecutorid","text":"toStatesByExecutorId ( executorPods : Seq [ Pod ]) : Map [ Long , ExecutorPodState ] toStatesByExecutorId ...FIXME toStatesByExecutorId is used when: ExecutorPodsSnapshot is requested to withUpdate and apply","title":" toStatesByExecutorId"},{"location":"ExecutorPodsSnapshot/#tostate","text":"toState ( pod : Pod ) : ExecutorPodState toState ...FIXME","title":" toState"},{"location":"ExecutorPodsSnapshot/#isdeleted","text":"isDeleted ( pod : Pod ) : Boolean isDeleted ...FIXME","title":" isDeleted"},{"location":"ExecutorPodsSnapshotsStore/","text":"ExecutorPodsSnapshotsStore \u00b6 ExecutorPodsSnapshotsStore is an abstraction of executor pods snapshots stores that subscribers can subscribe to and be notified about single pod or full snapshots updates . Contract \u00b6 Registering Subscriber \u00b6 addSubscriber ( processBatchIntervalMillis : Long )( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit Registers a new subscriber to be notified about new ExecutorPodsSnapshot s every processBatchIntervalMillis interval Used when: ExecutorPodsAllocator is requested to start ExecutorPodsLifecycleManager is requested to start Notifying Subscribers \u00b6 notifySubscribers () : Unit Used when: ExecutorPodsAllocator is requested to change the total expected executors Full Executor Pod State Synchronization \u00b6 replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit Used when: PollRunnable is requested to start (every spark.kubernetes.executor.apiPollingInterval when ExecutorPodsPollingSnapshotSource is started ) Stopping \u00b6 stop () : Unit Used when: KubernetesClusterSchedulerBackend is requested to stop Single Executor Pod State Update \u00b6 updatePod ( updatedPod : Pod ) : Unit Used when: ExecutorPodsWatcher is requested to eventReceived Implementations \u00b6 ExecutorPodsSnapshotsStoreImpl","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#executorpodssnapshotsstore","text":"ExecutorPodsSnapshotsStore is an abstraction of executor pods snapshots stores that subscribers can subscribe to and be notified about single pod or full snapshots updates .","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#contract","text":"","title":"Contract"},{"location":"ExecutorPodsSnapshotsStore/#registering-subscriber","text":"addSubscriber ( processBatchIntervalMillis : Long )( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit Registers a new subscriber to be notified about new ExecutorPodsSnapshot s every processBatchIntervalMillis interval Used when: ExecutorPodsAllocator is requested to start ExecutorPodsLifecycleManager is requested to start","title":" Registering Subscriber"},{"location":"ExecutorPodsSnapshotsStore/#notifying-subscribers","text":"notifySubscribers () : Unit Used when: ExecutorPodsAllocator is requested to change the total expected executors","title":" Notifying Subscribers"},{"location":"ExecutorPodsSnapshotsStore/#full-executor-pod-state-synchronization","text":"replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit Used when: PollRunnable is requested to start (every spark.kubernetes.executor.apiPollingInterval when ExecutorPodsPollingSnapshotSource is started )","title":" Full Executor Pod State Synchronization"},{"location":"ExecutorPodsSnapshotsStore/#stopping","text":"stop () : Unit Used when: KubernetesClusterSchedulerBackend is requested to stop","title":" Stopping"},{"location":"ExecutorPodsSnapshotsStore/#single-executor-pod-state-update","text":"updatePod ( updatedPod : Pod ) : Unit Used when: ExecutorPodsWatcher is requested to eventReceived","title":" Single Executor Pod State Update"},{"location":"ExecutorPodsSnapshotsStore/#implementations","text":"ExecutorPodsSnapshotsStoreImpl","title":"Implementations"},{"location":"ExecutorPodsSnapshotsStoreImpl/","text":"ExecutorPodsSnapshotsStoreImpl \u00b6 ExecutorPodsSnapshotsStoreImpl is an ExecutorPodsSnapshotsStore . Creating Instance \u00b6 ExecutorPodsSnapshotsStoreImpl takes the following to be created: Java's ScheduledExecutorService ExecutorPodsSnapshotsStoreImpl is created when: KubernetesClusterManager is requested for a SchedulerBackend replaceSnapshot \u00b6 replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit replaceSnapshot is part of the ExecutorPodsSnapshotsStore abstraction. replaceSnapshot replaces the currentSnapshot internal registry with a new ExecutorPodsSnapshot (with the given snapshot and the current time). In the end, updatePod addCurrentSnapshotToSubscribers . updatePod \u00b6 updatePod ( updatedPod : Pod ) : Unit updatePod is part of the ExecutorPodsSnapshotsStore abstraction. updatePod requests the current ExecutorPodsSnapshot to update based on the given updated pod. In the end, updatePod addCurrentSnapshotToSubscribers . Registering Subscriber \u00b6 addSubscriber ( processBatchIntervalMillis : Long ) ( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit addSubscriber is part of the ExecutorPodsSnapshotsStore abstraction. addSubscriber adds a new SnapshotsSubscriber to the subscribers internal registry. addSubscriber requests the ScheduledExecutorService to schedule processing executor pods by the SnapshotsSubscriber every given processBatchIntervalMillis delay (starting immediately). In the end, addSubscriber adds the scheduled action to the pollingTasks internal registry. callSubscriber \u00b6 callSubscriber ( subscriber : SnapshotsSubscriber ) : Unit callSubscriber ...FIXME callSubscriber is used when: ExecutorPodsSnapshotsStoreImpl is requested to addSubscriber and notifySubscribers pollingTasks Registry \u00b6 pollingTasks : CopyOnWriteArrayList [ Future [ _ ]] ExecutorPodsSnapshotsStoreImpl uses pollingTasks internal registry to track the recurring actions scheduled for subscribers . pollingTasks are cancelled when ExecutorPodsSnapshotsStoreImpl is requested to stop . subscribers Registry \u00b6 subscribers : CopyOnWriteArrayList [ SnapshotsSubscriber ] ExecutorPodsSnapshotsStoreImpl uses subscribers internal registry to track subscribers that want to be notified regularly about the current state of executor pods in a cluster. addCurrentSnapshotToSubscribers \u00b6 addCurrentSnapshotToSubscribers () : Unit addCurrentSnapshotToSubscribers requests every SnapshotsSubscriber to addCurrentSnapshot . addCurrentSnapshotToSubscribers is used when: ExecutorPodsSnapshotsStoreImpl is requested to updatePod and replaceSnapshot","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#executorpodssnapshotsstoreimpl","text":"ExecutorPodsSnapshotsStoreImpl is an ExecutorPodsSnapshotsStore .","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#creating-instance","text":"ExecutorPodsSnapshotsStoreImpl takes the following to be created: Java's ScheduledExecutorService ExecutorPodsSnapshotsStoreImpl is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsSnapshotsStoreImpl/#replacesnapshot","text":"replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit replaceSnapshot is part of the ExecutorPodsSnapshotsStore abstraction. replaceSnapshot replaces the currentSnapshot internal registry with a new ExecutorPodsSnapshot (with the given snapshot and the current time). In the end, updatePod addCurrentSnapshotToSubscribers .","title":" replaceSnapshot"},{"location":"ExecutorPodsSnapshotsStoreImpl/#updatepod","text":"updatePod ( updatedPod : Pod ) : Unit updatePod is part of the ExecutorPodsSnapshotsStore abstraction. updatePod requests the current ExecutorPodsSnapshot to update based on the given updated pod. In the end, updatePod addCurrentSnapshotToSubscribers .","title":" updatePod"},{"location":"ExecutorPodsSnapshotsStoreImpl/#registering-subscriber","text":"addSubscriber ( processBatchIntervalMillis : Long ) ( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit addSubscriber is part of the ExecutorPodsSnapshotsStore abstraction. addSubscriber adds a new SnapshotsSubscriber to the subscribers internal registry. addSubscriber requests the ScheduledExecutorService to schedule processing executor pods by the SnapshotsSubscriber every given processBatchIntervalMillis delay (starting immediately). In the end, addSubscriber adds the scheduled action to the pollingTasks internal registry.","title":" Registering Subscriber"},{"location":"ExecutorPodsSnapshotsStoreImpl/#callsubscriber","text":"callSubscriber ( subscriber : SnapshotsSubscriber ) : Unit callSubscriber ...FIXME callSubscriber is used when: ExecutorPodsSnapshotsStoreImpl is requested to addSubscriber and notifySubscribers","title":" callSubscriber"},{"location":"ExecutorPodsSnapshotsStoreImpl/#pollingtasks-registry","text":"pollingTasks : CopyOnWriteArrayList [ Future [ _ ]] ExecutorPodsSnapshotsStoreImpl uses pollingTasks internal registry to track the recurring actions scheduled for subscribers . pollingTasks are cancelled when ExecutorPodsSnapshotsStoreImpl is requested to stop .","title":" pollingTasks Registry"},{"location":"ExecutorPodsSnapshotsStoreImpl/#subscribers-registry","text":"subscribers : CopyOnWriteArrayList [ SnapshotsSubscriber ] ExecutorPodsSnapshotsStoreImpl uses subscribers internal registry to track subscribers that want to be notified regularly about the current state of executor pods in a cluster.","title":" subscribers Registry"},{"location":"ExecutorPodsSnapshotsStoreImpl/#addcurrentsnapshottosubscribers","text":"addCurrentSnapshotToSubscribers () : Unit addCurrentSnapshotToSubscribers requests every SnapshotsSubscriber to addCurrentSnapshot . addCurrentSnapshotToSubscribers is used when: ExecutorPodsSnapshotsStoreImpl is requested to updatePod and replaceSnapshot","title":" addCurrentSnapshotToSubscribers"},{"location":"ExecutorPodsWatchSnapshotSource/","text":"ExecutorPodsWatchSnapshotSource \u00b6 ExecutorPodsWatchSnapshotSource is given KubernetesClient to a Kubernetes API server to watch for status updates of the executor pods of a given Spark application when started (that ExecutorPodsWatcher passes along to the ExecutorPodsSnapshotsStore ). Creating Instance \u00b6 ExecutorPodsWatchSnapshotSource takes the following to be created: ExecutorPodsSnapshotsStore KubernetesClient ExecutorPodsWatchSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend watchConnection \u00b6 watchConnection : Closeable ExecutorPodsWatchSnapshotSource defines watchConnection internal registry to be a \"watch connection\" to a Kubernetes API server to watch any status updates of the executor pods of a given Spark application (using ExecutorPodsWatcher ). ExecutorPodsWatchSnapshotSource uses watchConnection internal registry as an indication of whether it has been started already or not (and throws an IllegalArgumentException when it has). ExecutorPodsWatchSnapshotSource requests the watchConnection to close and null s it when requested to stop . Starting \u00b6 start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor. start requests the KubernetesClient to watch pods with the following labels and values and pass pod updates to ExecutorPodsWatcher . Label Name Value spark-app-selector the given applicationId spark-role executor start is used when: KubernetesClusterSchedulerBackend is requested to start Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL Refer to Logging .","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#executorpodswatchsnapshotsource","text":"ExecutorPodsWatchSnapshotSource is given KubernetesClient to a Kubernetes API server to watch for status updates of the executor pods of a given Spark application when started (that ExecutorPodsWatcher passes along to the ExecutorPodsSnapshotsStore ).","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#creating-instance","text":"ExecutorPodsWatchSnapshotSource takes the following to be created: ExecutorPodsSnapshotsStore KubernetesClient ExecutorPodsWatchSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsWatchSnapshotSource/#watchconnection","text":"watchConnection : Closeable ExecutorPodsWatchSnapshotSource defines watchConnection internal registry to be a \"watch connection\" to a Kubernetes API server to watch any status updates of the executor pods of a given Spark application (using ExecutorPodsWatcher ). ExecutorPodsWatchSnapshotSource uses watchConnection internal registry as an indication of whether it has been started already or not (and throws an IllegalArgumentException when it has). ExecutorPodsWatchSnapshotSource requests the watchConnection to close and null s it when requested to stop .","title":" watchConnection"},{"location":"ExecutorPodsWatchSnapshotSource/#starting","text":"start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor. start requests the KubernetesClient to watch pods with the following labels and values and pass pod updates to ExecutorPodsWatcher . Label Name Value spark-app-selector the given applicationId spark-role executor start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsWatchSnapshotSource/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsWatcher/","text":"ExecutorPodsWatcher \u00b6 ExecutorPodsWatcher is a Kubernetes' Watcher[Pod] to be notified about executor pod updates (and update the ExecutorPodsSnapshotsStore of the parent ExecutorPodsWatchSnapshotSource ). ExecutorPodsWatcher is an internal class of ExecutorPodsWatchSnapshotSource with full access to its internals. Creating Instance \u00b6 ExecutorPodsWatcher takes no arguments to be created. ExecutorPodsWatcher is created when: ExecutorPodsWatchSnapshotSource is requested to start Executor Pod Update \u00b6 eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes Client Watcher abstraction. eventReceived prints out the following DEBUG message to the logs: Received executor pod update for pod named [podName], action [action] eventReceived requests the ExecutorPodsSnapshotsStore (of the parent ExecutorPodsWatchSnapshotSource ) to updatePod . Close Notification \u00b6 onClose ( e : KubernetesClientException ) : Unit onClose is part of the Kubernetes Client Watcher abstraction. onClose prints out the following WARN message to the logs: Kubernetes client has been closed (this is expected if the application is shutting down.) Logging \u00b6 ExecutorPodsWatcher uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger for logging.","title":"ExecutorPodsWatcher"},{"location":"ExecutorPodsWatcher/#executorpodswatcher","text":"ExecutorPodsWatcher is a Kubernetes' Watcher[Pod] to be notified about executor pod updates (and update the ExecutorPodsSnapshotsStore of the parent ExecutorPodsWatchSnapshotSource ). ExecutorPodsWatcher is an internal class of ExecutorPodsWatchSnapshotSource with full access to its internals.","title":"ExecutorPodsWatcher"},{"location":"ExecutorPodsWatcher/#creating-instance","text":"ExecutorPodsWatcher takes no arguments to be created. ExecutorPodsWatcher is created when: ExecutorPodsWatchSnapshotSource is requested to start","title":"Creating Instance"},{"location":"ExecutorPodsWatcher/#executor-pod-update","text":"eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes Client Watcher abstraction. eventReceived prints out the following DEBUG message to the logs: Received executor pod update for pod named [podName], action [action] eventReceived requests the ExecutorPodsSnapshotsStore (of the parent ExecutorPodsWatchSnapshotSource ) to updatePod .","title":" Executor Pod Update"},{"location":"ExecutorPodsWatcher/#close-notification","text":"onClose ( e : KubernetesClientException ) : Unit onClose is part of the Kubernetes Client Watcher abstraction. onClose prints out the following WARN message to the logs: Kubernetes client has been closed (this is expected if the application is shutting down.)","title":" Close Notification"},{"location":"ExecutorPodsWatcher/#logging","text":"ExecutorPodsWatcher uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger for logging.","title":"Logging"},{"location":"K8SSparkSubmitOperation/","text":"K8SSparkSubmitOperation \u00b6 K8SSparkSubmitOperation is an extension of spark-submit ( Apache Spark ) for Spark on Kubernetes to support the operations: kill status K8SSparkSubmitOperation is registered with Apache Spark using META-INF/services/org.apache.spark.deploy.SparkSubmitOperation service file. Killing Submission \u00b6 kill ( submissionId : String , conf : SparkConf ) : Unit kill is part of the SparkSubmitOperation ( Apache Spark ) abstraction. kill prints out the following message to standard error: Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set]. kill creates a KillApplication to execute it (with the input submissionId and SparkConf ). Displaying Submission Status \u00b6 printSubmissionStatus ( submissionId : String , conf : SparkConf ) : Unit printSubmissionStatus is part of the SparkSubmitOperation ( Apache Spark ) abstraction. printSubmissionStatus prints out the following message to standard error: Submitting a request for the status of submission [submissionId] in [spark.master]. printSubmissionStatus creates a ListStatus to execute it (with the input submissionId and SparkConf ). Checking Whether Master URL Supported \u00b6 supports ( master : String ) : Boolean supports is part of the SparkSubmitOperation ( Apache Spark ) abstraction. supports is true when the input master starts with k8s:// prefix. Executing Operation \u00b6 execute ( submissionId : String , sparkConf : SparkConf , op : K8sSubmitOp ) : Unit execute is used for kill and printSubmissionStatus . execute parses the master URL (based on spark.master configuration property). execute uses : (the colon) to split the given submissionId to at most two parts - an optional namespace and a driver pod name. The driver pod name can use a glob pattern as * (the star). execute creates a KubernetesClient (with Submission client type). If the pod name uses the glob pattern (with * ), execute requests the KubernetesClient for the driver pods (pods with the spark-role label with driver value) that it then hands over to the given K8sSubmitOp to executeOnGlob (with the optional namespace). Otherwise, execute requests the given K8sSubmitOp to executeOnPod with the pod and the optional namespace. execute prints out the following message and exits when the given submissionId cannot be split on : to two parts: Submission ID: {[submissionId]} is invalid.","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#k8ssparksubmitoperation","text":"K8SSparkSubmitOperation is an extension of spark-submit ( Apache Spark ) for Spark on Kubernetes to support the operations: kill status K8SSparkSubmitOperation is registered with Apache Spark using META-INF/services/org.apache.spark.deploy.SparkSubmitOperation service file.","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#killing-submission","text":"kill ( submissionId : String , conf : SparkConf ) : Unit kill is part of the SparkSubmitOperation ( Apache Spark ) abstraction. kill prints out the following message to standard error: Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set]. kill creates a KillApplication to execute it (with the input submissionId and SparkConf ).","title":" Killing Submission"},{"location":"K8SSparkSubmitOperation/#displaying-submission-status","text":"printSubmissionStatus ( submissionId : String , conf : SparkConf ) : Unit printSubmissionStatus is part of the SparkSubmitOperation ( Apache Spark ) abstraction. printSubmissionStatus prints out the following message to standard error: Submitting a request for the status of submission [submissionId] in [spark.master]. printSubmissionStatus creates a ListStatus to execute it (with the input submissionId and SparkConf ).","title":" Displaying Submission Status"},{"location":"K8SSparkSubmitOperation/#checking-whether-master-url-supported","text":"supports ( master : String ) : Boolean supports is part of the SparkSubmitOperation ( Apache Spark ) abstraction. supports is true when the input master starts with k8s:// prefix.","title":" Checking Whether Master URL Supported"},{"location":"K8SSparkSubmitOperation/#executing-operation","text":"execute ( submissionId : String , sparkConf : SparkConf , op : K8sSubmitOp ) : Unit execute is used for kill and printSubmissionStatus . execute parses the master URL (based on spark.master configuration property). execute uses : (the colon) to split the given submissionId to at most two parts - an optional namespace and a driver pod name. The driver pod name can use a glob pattern as * (the star). execute creates a KubernetesClient (with Submission client type). If the pod name uses the glob pattern (with * ), execute requests the KubernetesClient for the driver pods (pods with the spark-role label with driver value) that it then hands over to the given K8sSubmitOp to executeOnGlob (with the optional namespace). Otherwise, execute requests the given K8sSubmitOp to executeOnPod with the pod and the optional namespace. execute prints out the following message and exits when the given submissionId cannot be split on : to two parts: Submission ID: {[submissionId]} is invalid.","title":" Executing Operation"},{"location":"KubernetesClientApplication/","text":"KubernetesClientApplication \u00b6 KubernetesClientApplication is a SparkApplication ( Apache Spark ) in Spark on Kubernetes in cluster deploy mode. Creating Instance \u00b6 KubernetesClientApplication takes no arguments to be created. KubernetesClientApplication is created when: SparkSubmit is requested to launch a Spark application (for kubernetes in cluster deploy mode ) Starting Spark Application \u00b6 start ( args : Array [ String ], conf : SparkConf ) : Unit start is part of the SparkApplication ( Apache Spark ) abstraction. start parses the command-line arguments ( args ) and runs . run \u00b6 run ( clientArguments : ClientArguments , sparkConf : SparkConf ) : Unit run generates a custom Spark Application ID of the format: spark-[randomUUID-without-dashes] run creates a KubernetesDriverConf (with the given ClientArguments , SparkConf and the custom Spark Application ID). run removes the k8s:// prefix from the spark.master configuration property (which has already been validated by SparkSubmit itself). run creates a LoggingPodStatusWatcherImpl (with the KubernetesDriverConf ). run creates a KubernetesClient (with the master URL, the namespace , and others). In the end, run creates a Client (with the KubernetesDriverConf , a new KubernetesDriverBuilder , the KubernetesClient , and the LoggingPodStatusWatcherImpl ) and requests it to run .","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#kubernetesclientapplication","text":"KubernetesClientApplication is a SparkApplication ( Apache Spark ) in Spark on Kubernetes in cluster deploy mode.","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#creating-instance","text":"KubernetesClientApplication takes no arguments to be created. KubernetesClientApplication is created when: SparkSubmit is requested to launch a Spark application (for kubernetes in cluster deploy mode )","title":"Creating Instance"},{"location":"KubernetesClientApplication/#starting-spark-application","text":"start ( args : Array [ String ], conf : SparkConf ) : Unit start is part of the SparkApplication ( Apache Spark ) abstraction. start parses the command-line arguments ( args ) and runs .","title":" Starting Spark Application"},{"location":"KubernetesClientApplication/#run","text":"run ( clientArguments : ClientArguments , sparkConf : SparkConf ) : Unit run generates a custom Spark Application ID of the format: spark-[randomUUID-without-dashes] run creates a KubernetesDriverConf (with the given ClientArguments , SparkConf and the custom Spark Application ID). run removes the k8s:// prefix from the spark.master configuration property (which has already been validated by SparkSubmit itself). run creates a LoggingPodStatusWatcherImpl (with the KubernetesDriverConf ). run creates a KubernetesClient (with the master URL, the namespace , and others). In the end, run creates a Client (with the KubernetesDriverConf , a new KubernetesDriverBuilder , the KubernetesClient , and the LoggingPodStatusWatcherImpl ) and requests it to run .","title":" run"},{"location":"KubernetesClientUtils/","text":"KubernetesClientUtils Utility \u00b6 Name of Config Map for Executors \u00b6 configMapNameExecutor : String KubernetesClientUtils defines the name of a config map for executors as follows: spark-exec-[uniqueID]-conf-map spark-exec-[uniqueID] is made under 54 characters to let configMapNameExecutor be at 63 characters maximum. buildSparkConfDirFilesMap \u00b6 buildSparkConfDirFilesMap ( configMapName : String , sparkConf : SparkConf , resolvedPropertiesMap : Map [ String , String ]) : Map [ String , String ] buildSparkConfDirFilesMap ...FIXME buildSparkConfDirFilesMap is used when: BasicExecutorFeatureStep is requested to configure a pod Client is requested to run KubernetesClusterSchedulerBackend is requested to setUpExecutorConfigMap loadSparkConfDirFiles \u00b6 loadSparkConfDirFiles ( conf : SparkConf ) : Map [ String , String ] loadSparkConfDirFiles ...FIXME","title":"KubernetesClientUtils"},{"location":"KubernetesClientUtils/#kubernetesclientutils-utility","text":"","title":"KubernetesClientUtils Utility"},{"location":"KubernetesClientUtils/#name-of-config-map-for-executors","text":"configMapNameExecutor : String KubernetesClientUtils defines the name of a config map for executors as follows: spark-exec-[uniqueID]-conf-map spark-exec-[uniqueID] is made under 54 characters to let configMapNameExecutor be at 63 characters maximum.","title":" Name of Config Map for Executors"},{"location":"KubernetesClientUtils/#buildsparkconfdirfilesmap","text":"buildSparkConfDirFilesMap ( configMapName : String , sparkConf : SparkConf , resolvedPropertiesMap : Map [ String , String ]) : Map [ String , String ] buildSparkConfDirFilesMap ...FIXME buildSparkConfDirFilesMap is used when: BasicExecutorFeatureStep is requested to configure a pod Client is requested to run KubernetesClusterSchedulerBackend is requested to setUpExecutorConfigMap","title":" buildSparkConfDirFilesMap"},{"location":"KubernetesClientUtils/#loadsparkconfdirfiles","text":"loadSparkConfDirFiles ( conf : SparkConf ) : Map [ String , String ] loadSparkConfDirFiles ...FIXME","title":" loadSparkConfDirFiles"},{"location":"KubernetesClusterManager/","text":"KubernetesClusterManager \u00b6 KubernetesClusterManager is an ExternalClusterManager ( Apache Spark ) that can create scheduler components for k8s master URLs: TaskSchedulerImpl KubernetesClusterSchedulerBackend KubernetesClusterManager is registered with Apache Spark using META-INF/services/org.apache.spark.scheduler.ExternalClusterManager service file. Creating Instance \u00b6 KubernetesClusterManager takes no arguments to be created. KubernetesClusterManager is created when: SparkContext is requested for an ExternalClusterManager (when requested for a SchedulerBackend and TaskScheduler ) Creating SchedulerBackend \u00b6 createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend is part of the ExternalClusterManager ( Apache Spark ) abstraction. createSchedulerBackend creates a KubernetesClusterSchedulerBackend . Note createSchedulerBackend assumes that the given TaskScheduler is TaskSchedulerImpl ( Apache Spark ). createSchedulerBackend determines four internal values based on the spark.kubernetes.submitInDriver internal configuration property. spark.kubernetes.submitInDriver Enabled ( true ) Disabled ( false ) authConfPrefix spark.kubernetes.authenticate.driver.mounted spark.kubernetes.authenticate apiServerUri spark.kubernetes.driver.master Master URL with no k8s:// prefix defaultServiceAccountToken /var/run/secrets/kubernetes.io/serviceaccount/token defaultServiceAccountCaCrt /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Unless already defined, createSchedulerBackend sets the spark.kubernetes.executor.podNamePrefix configuration properties based on spark.app.name prefix. createSchedulerBackend creates a KubernetesClient for the Driver client type and the following: spark.kubernetes.namespace configuration property apiServerUri authConfPrefix defaultServiceAccountToken defaultServiceAccountCaCrt With spark.kubernetes.executor.podTemplateFile configuration property enabled, createSchedulerBackend loads the pod spec from the pod template file with the optional spark.kubernetes.executor.podTemplateContainerName configuration property. In the end, createSchedulerBackend creates a KubernetesClusterSchedulerBackend with the following: Java ScheduledExecutorService with kubernetes-executor-maintenance thread name ExecutorPodsSnapshotsStoreImpl with a Java ScheduledExecutorService with kubernetes-executor-snapshots-subscribers thread names and 2 threads ExecutorPodsLifecycleManager ExecutorPodsAllocator ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource with a Java ScheduledExecutorService with kubernetes-executor-pod-polling-sync thread name IllegalArgumentException \u00b6 With spark.kubernetes.submitInDriver enabled, createSchedulerBackend asserts that the name of the driver pod is configured (using spark.kubernetes.driver.pod.name configuration property) or else throws an IllegalArgumentException : If the application is deployed using spark-submit in cluster mode, the driver pod name must be provided. Creating TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler is part of the ExternalClusterManager ( Apache Spark ) abstraction. createTaskScheduler creates a TaskSchedulerImpl ( Apache Spark ). Initializing Scheduling Components \u00b6 initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize is part of the ExternalClusterManager ( Apache Spark ) abstraction. initialize requests the given TaskSchedulerImpl ( Apache Spark ) to initialize with the given SchedulerBackend ( Apache Spark ).","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#kubernetesclustermanager","text":"KubernetesClusterManager is an ExternalClusterManager ( Apache Spark ) that can create scheduler components for k8s master URLs: TaskSchedulerImpl KubernetesClusterSchedulerBackend KubernetesClusterManager is registered with Apache Spark using META-INF/services/org.apache.spark.scheduler.ExternalClusterManager service file.","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#creating-instance","text":"KubernetesClusterManager takes no arguments to be created. KubernetesClusterManager is created when: SparkContext is requested for an ExternalClusterManager (when requested for a SchedulerBackend and TaskScheduler )","title":"Creating Instance"},{"location":"KubernetesClusterManager/#creating-schedulerbackend","text":"createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend is part of the ExternalClusterManager ( Apache Spark ) abstraction. createSchedulerBackend creates a KubernetesClusterSchedulerBackend . Note createSchedulerBackend assumes that the given TaskScheduler is TaskSchedulerImpl ( Apache Spark ). createSchedulerBackend determines four internal values based on the spark.kubernetes.submitInDriver internal configuration property. spark.kubernetes.submitInDriver Enabled ( true ) Disabled ( false ) authConfPrefix spark.kubernetes.authenticate.driver.mounted spark.kubernetes.authenticate apiServerUri spark.kubernetes.driver.master Master URL with no k8s:// prefix defaultServiceAccountToken /var/run/secrets/kubernetes.io/serviceaccount/token defaultServiceAccountCaCrt /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Unless already defined, createSchedulerBackend sets the spark.kubernetes.executor.podNamePrefix configuration properties based on spark.app.name prefix. createSchedulerBackend creates a KubernetesClient for the Driver client type and the following: spark.kubernetes.namespace configuration property apiServerUri authConfPrefix defaultServiceAccountToken defaultServiceAccountCaCrt With spark.kubernetes.executor.podTemplateFile configuration property enabled, createSchedulerBackend loads the pod spec from the pod template file with the optional spark.kubernetes.executor.podTemplateContainerName configuration property. In the end, createSchedulerBackend creates a KubernetesClusterSchedulerBackend with the following: Java ScheduledExecutorService with kubernetes-executor-maintenance thread name ExecutorPodsSnapshotsStoreImpl with a Java ScheduledExecutorService with kubernetes-executor-snapshots-subscribers thread names and 2 threads ExecutorPodsLifecycleManager ExecutorPodsAllocator ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource with a Java ScheduledExecutorService with kubernetes-executor-pod-polling-sync thread name","title":" Creating SchedulerBackend"},{"location":"KubernetesClusterManager/#illegalargumentexception","text":"With spark.kubernetes.submitInDriver enabled, createSchedulerBackend asserts that the name of the driver pod is configured (using spark.kubernetes.driver.pod.name configuration property) or else throws an IllegalArgumentException : If the application is deployed using spark-submit in cluster mode, the driver pod name must be provided.","title":"IllegalArgumentException"},{"location":"KubernetesClusterManager/#creating-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler is part of the ExternalClusterManager ( Apache Spark ) abstraction. createTaskScheduler creates a TaskSchedulerImpl ( Apache Spark ).","title":" Creating TaskScheduler"},{"location":"KubernetesClusterManager/#initializing-scheduling-components","text":"initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize is part of the ExternalClusterManager ( Apache Spark ) abstraction. initialize requests the given TaskSchedulerImpl ( Apache Spark ) to initialize with the given SchedulerBackend ( Apache Spark ).","title":" Initializing Scheduling Components"},{"location":"KubernetesClusterSchedulerBackend/","text":"KubernetesClusterSchedulerBackend \u00b6 KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend ( Apache Spark ) for Spark on Kubernetes . Creating Instance \u00b6 KubernetesClusterSchedulerBackend takes the following to be created: TaskSchedulerImpl ( Apache Spark ) SparkContext ( Apache Spark ) KubernetesClient Java's ScheduledExecutorService ExecutorPodsSnapshotsStore ExecutorPodsAllocator ExecutorPodsLifecycleManager ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource KubernetesClusterSchedulerBackend is created when: KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsLifecycleManager \u00b6 KubernetesClusterSchedulerBackend is given an ExecutorPodsLifecycleManager to be created . KubernetesClusterSchedulerBackend requests the ExecutorPodsLifecycleManager to start (with itself) when started . ExecutorPodsAllocator \u00b6 KubernetesClusterSchedulerBackend is given an ExecutorPodsAllocator to be created . When started , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the number of initial executors and starts it with application Id . When requested for the expected number of executors , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given total number of executors. When requested to isBlacklisted , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to isDeleted with a given executor. Initial Executors \u00b6 initialExecutors : Int KubernetesClusterSchedulerBackend calculates the initial target number of executors when created . initialExecutors is used when KubernetesClusterSchedulerBackend is requested to start and whether or not sufficient resources registered . Application Id \u00b6 applicationId () : String applicationId is part of the SchedulerBackend ( Apache Spark ) abstraction. applicationId is the value of spark.app.id configuration property if defined or the default applicationId . Sufficient Resources Registered \u00b6 sufficientResourcesRegistered () : Boolean sufficientResourcesRegistered is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. sufficientResourcesRegistered holds (is true ) when the totalRegisteredExecutors is at least the ratio of the initial executors . Minimum Resources Available Ratio \u00b6 minRegisteredRatio : Double minRegisteredRatio is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. minRegisteredRatio is 0.8 unless spark.scheduler.minRegisteredResourcesRatio configuration property is defined. Starting SchedulerBackend \u00b6 start () : Unit start is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. start creates a delegation token manager. start requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors . start requests the ExecutorPodsLifecycleManager to start (with this KubernetesClusterSchedulerBackend ). start requests the ExecutorPodsAllocator to start (with the applicationId ) start requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId ) start requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId ) In the end, start setUpExecutorConfigMap . setUpExecutorConfigMap \u00b6 setUpExecutorConfigMap () : Unit setUpExecutorConfigMap takes the Name of Config Map for Executors and buildSparkConfDirFilesMap (with the SparkConf ). setUpExecutorConfigMap buildConfigMap with the labels (and the name of the config map and the configuration files). Name Value spark-app-selector Application Id spark-role executor In the end, setUpExecutorConfigMap requests the KubernetesClient to create a new config map. Creating DriverEndpoint \u00b6 createDriverEndpoint () : DriverEndpoint createDriverEndpoint is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. createDriverEndpoint creates a KubernetesDriverEndpoint . Requesting Executors from Cluster Manager \u00b6 doRequestTotalExecutors ( requestedTotal : Int ) : Future [ Boolean ] doRequestTotalExecutors is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. doRequestTotalExecutors requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given requestedTotal . In the end, doRequestTotalExecutors returns a completed Future with true value. Stopping SchedulerBackend \u00b6 stop () : Unit stop is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. stop ...FIXME","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#kubernetesclusterschedulerbackend","text":"KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend ( Apache Spark ) for Spark on Kubernetes .","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#creating-instance","text":"KubernetesClusterSchedulerBackend takes the following to be created: TaskSchedulerImpl ( Apache Spark ) SparkContext ( Apache Spark ) KubernetesClient Java's ScheduledExecutorService ExecutorPodsSnapshotsStore ExecutorPodsAllocator ExecutorPodsLifecycleManager ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource KubernetesClusterSchedulerBackend is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"KubernetesClusterSchedulerBackend/#executorpodslifecyclemanager","text":"KubernetesClusterSchedulerBackend is given an ExecutorPodsLifecycleManager to be created . KubernetesClusterSchedulerBackend requests the ExecutorPodsLifecycleManager to start (with itself) when started .","title":" ExecutorPodsLifecycleManager"},{"location":"KubernetesClusterSchedulerBackend/#executorpodsallocator","text":"KubernetesClusterSchedulerBackend is given an ExecutorPodsAllocator to be created . When started , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the number of initial executors and starts it with application Id . When requested for the expected number of executors , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given total number of executors. When requested to isBlacklisted , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to isDeleted with a given executor.","title":" ExecutorPodsAllocator"},{"location":"KubernetesClusterSchedulerBackend/#initial-executors","text":"initialExecutors : Int KubernetesClusterSchedulerBackend calculates the initial target number of executors when created . initialExecutors is used when KubernetesClusterSchedulerBackend is requested to start and whether or not sufficient resources registered .","title":" Initial Executors"},{"location":"KubernetesClusterSchedulerBackend/#application-id","text":"applicationId () : String applicationId is part of the SchedulerBackend ( Apache Spark ) abstraction. applicationId is the value of spark.app.id configuration property if defined or the default applicationId .","title":" Application Id"},{"location":"KubernetesClusterSchedulerBackend/#sufficient-resources-registered","text":"sufficientResourcesRegistered () : Boolean sufficientResourcesRegistered is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. sufficientResourcesRegistered holds (is true ) when the totalRegisteredExecutors is at least the ratio of the initial executors .","title":" Sufficient Resources Registered"},{"location":"KubernetesClusterSchedulerBackend/#minimum-resources-available-ratio","text":"minRegisteredRatio : Double minRegisteredRatio is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. minRegisteredRatio is 0.8 unless spark.scheduler.minRegisteredResourcesRatio configuration property is defined.","title":" Minimum Resources Available Ratio"},{"location":"KubernetesClusterSchedulerBackend/#starting-schedulerbackend","text":"start () : Unit start is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. start creates a delegation token manager. start requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors . start requests the ExecutorPodsLifecycleManager to start (with this KubernetesClusterSchedulerBackend ). start requests the ExecutorPodsAllocator to start (with the applicationId ) start requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId ) start requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId ) In the end, start setUpExecutorConfigMap .","title":" Starting SchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#setupexecutorconfigmap","text":"setUpExecutorConfigMap () : Unit setUpExecutorConfigMap takes the Name of Config Map for Executors and buildSparkConfDirFilesMap (with the SparkConf ). setUpExecutorConfigMap buildConfigMap with the labels (and the name of the config map and the configuration files). Name Value spark-app-selector Application Id spark-role executor In the end, setUpExecutorConfigMap requests the KubernetesClient to create a new config map.","title":" setUpExecutorConfigMap"},{"location":"KubernetesClusterSchedulerBackend/#creating-driverendpoint","text":"createDriverEndpoint () : DriverEndpoint createDriverEndpoint is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. createDriverEndpoint creates a KubernetesDriverEndpoint .","title":" Creating DriverEndpoint"},{"location":"KubernetesClusterSchedulerBackend/#requesting-executors-from-cluster-manager","text":"doRequestTotalExecutors ( requestedTotal : Int ) : Future [ Boolean ] doRequestTotalExecutors is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. doRequestTotalExecutors requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given requestedTotal . In the end, doRequestTotalExecutors returns a completed Future with true value.","title":" Requesting Executors from Cluster Manager"},{"location":"KubernetesClusterSchedulerBackend/#stopping-schedulerbackend","text":"stop () : Unit stop is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. stop ...FIXME","title":" Stopping SchedulerBackend"},{"location":"KubernetesConf/","text":"KubernetesConf \u00b6 KubernetesConf is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors ). Contract \u00b6 annotations \u00b6 annotations : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod environment \u00b6 environment : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod labels \u00b6 labels : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod DriverServiceFeatureStep is requested to getAdditionalKubernetesResources resourceNamePrefix \u00b6 resourceNamePrefix : String Prefix of resource names secretEnvNamesToKeyRefs \u00b6 secretEnvNamesToKeyRefs : Map [ String , String ] Used when: EnvSecretsFeatureStep is requested to configurePod secretNamesToMountPaths \u00b6 secretNamesToMountPaths : Map [ String , String ] Used when: MountSecretsFeatureStep is requested to configurePod Volume Specs \u00b6 volumes : Seq [ KubernetesVolumeSpec ] KubernetesVolumeSpec s with the following: volumeName mountPath mountSubPath mountReadOnly volumeConf Used when: MountVolumesFeatureStep is requested to configure a pod Implementations \u00b6 KubernetesDriverConf KubernetesExecutorConf Creating Instance \u00b6 KubernetesConf takes the following to be created: SparkConf Abstract Class KubernetesConf is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs . Namespace \u00b6 namespace : String namespace is the value of spark.kubernetes.namespace configuration property. namespace is used when: DriverServiceFeatureStep is requested to getAdditionalPodSystemProperties Client is requested to run KubernetesClientApplication is requested to start imagePullPolicy \u00b6 imagePullPolicy : String imagePullPolicy is the value of spark.kubernetes.container.image.pullPolicy configuration property. imagePullPolicy is used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod Creating KubernetesDriverConf \u00b6 createDriverConf ( sparkConf : SparkConf , appId : String , mainAppResource : MainAppResource , mainClass : String , appArgs : Array [ String ]) : KubernetesDriverConf Note The goal of createDriverConf is to validate executor volumes before creating a KubernetesDriverConf . createDriverConf parse volumes for executors (with spark.kubernetes.executor.volumes prefix). Note createDriverConf parses executor volumes in order to verify configuration before the driver pod is created. In the end, createDriverConf creates a KubernetesDriverConf . createDriverConf is used when: KubernetesClientApplication is requested to start Creating KubernetesExecutorConf \u00b6 createExecutorConf ( sparkConf : SparkConf , executorId : String , appId : String , driverPod : Option [ Pod ]) : KubernetesExecutorConf createExecutorConf ( does nothing more but ) creates a KubernetesExecutorConf for the given input arguments. createExecutorConf is used when: ExecutorPodsAllocator is requested to onNewSnapshots (and requests missing executors from Kubernetes) AppName-Based Unique Resource Name Prefix \u00b6 getResourceNamePrefix ( appName : String ) : String getResourceNamePrefix ...FIXME getResourceNamePrefix is used when: KubernetesDriverConf is requested for the resourceNamePrefix KubernetesExecutorConf is requested for the resourceNamePrefix KubernetesClusterManager is requested to createSchedulerBackend","title":"KubernetesConf"},{"location":"KubernetesConf/#kubernetesconf","text":"KubernetesConf is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors ).","title":"KubernetesConf"},{"location":"KubernetesConf/#contract","text":"","title":"Contract"},{"location":"KubernetesConf/#annotations","text":"annotations : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" annotations"},{"location":"KubernetesConf/#environment","text":"environment : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" environment"},{"location":"KubernetesConf/#labels","text":"labels : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod DriverServiceFeatureStep is requested to getAdditionalKubernetesResources","title":" labels"},{"location":"KubernetesConf/#resourcenameprefix","text":"resourceNamePrefix : String Prefix of resource names","title":" resourceNamePrefix"},{"location":"KubernetesConf/#secretenvnamestokeyrefs","text":"secretEnvNamesToKeyRefs : Map [ String , String ] Used when: EnvSecretsFeatureStep is requested to configurePod","title":" secretEnvNamesToKeyRefs"},{"location":"KubernetesConf/#secretnamestomountpaths","text":"secretNamesToMountPaths : Map [ String , String ] Used when: MountSecretsFeatureStep is requested to configurePod","title":" secretNamesToMountPaths"},{"location":"KubernetesConf/#volume-specs","text":"volumes : Seq [ KubernetesVolumeSpec ] KubernetesVolumeSpec s with the following: volumeName mountPath mountSubPath mountReadOnly volumeConf Used when: MountVolumesFeatureStep is requested to configure a pod","title":" Volume Specs"},{"location":"KubernetesConf/#implementations","text":"KubernetesDriverConf KubernetesExecutorConf","title":"Implementations"},{"location":"KubernetesConf/#creating-instance","text":"KubernetesConf takes the following to be created: SparkConf Abstract Class KubernetesConf is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs .","title":"Creating Instance"},{"location":"KubernetesConf/#namespace","text":"namespace : String namespace is the value of spark.kubernetes.namespace configuration property. namespace is used when: DriverServiceFeatureStep is requested to getAdditionalPodSystemProperties Client is requested to run KubernetesClientApplication is requested to start","title":" Namespace"},{"location":"KubernetesConf/#imagepullpolicy","text":"imagePullPolicy : String imagePullPolicy is the value of spark.kubernetes.container.image.pullPolicy configuration property. imagePullPolicy is used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod","title":" imagePullPolicy"},{"location":"KubernetesConf/#creating-kubernetesdriverconf","text":"createDriverConf ( sparkConf : SparkConf , appId : String , mainAppResource : MainAppResource , mainClass : String , appArgs : Array [ String ]) : KubernetesDriverConf Note The goal of createDriverConf is to validate executor volumes before creating a KubernetesDriverConf . createDriverConf parse volumes for executors (with spark.kubernetes.executor.volumes prefix). Note createDriverConf parses executor volumes in order to verify configuration before the driver pod is created. In the end, createDriverConf creates a KubernetesDriverConf . createDriverConf is used when: KubernetesClientApplication is requested to start","title":" Creating KubernetesDriverConf"},{"location":"KubernetesConf/#creating-kubernetesexecutorconf","text":"createExecutorConf ( sparkConf : SparkConf , executorId : String , appId : String , driverPod : Option [ Pod ]) : KubernetesExecutorConf createExecutorConf ( does nothing more but ) creates a KubernetesExecutorConf for the given input arguments. createExecutorConf is used when: ExecutorPodsAllocator is requested to onNewSnapshots (and requests missing executors from Kubernetes)","title":" Creating KubernetesExecutorConf"},{"location":"KubernetesConf/#appname-based-unique-resource-name-prefix","text":"getResourceNamePrefix ( appName : String ) : String getResourceNamePrefix ...FIXME getResourceNamePrefix is used when: KubernetesDriverConf is requested for the resourceNamePrefix KubernetesExecutorConf is requested for the resourceNamePrefix KubernetesClusterManager is requested to createSchedulerBackend","title":" AppName-Based Unique Resource Name Prefix"},{"location":"KubernetesDriverBuilder/","text":"KubernetesDriverBuilder \u00b6 KubernetesDriverBuilder is used to build a specification of a driver pod (for a Spark application deployed in cluster deploy mode). Creating Instance \u00b6 KubernetesDriverBuilder takes no arguments to be created. KubernetesDriverBuilder is created when: KubernetesClientApplication is requested to start KubernetesDriverSpec \u00b6 KubernetesDriverSpec is the following: SparkPod Driver Resources System Properties Building Pod Spec for Driver \u00b6 buildFromFeatures ( conf : KubernetesDriverConf , client : KubernetesClient ) : KubernetesDriverSpec buildFromFeatures creates an initial driver pod specification. With spark.kubernetes.driver.podTemplateFile configuration property defined, buildFromFeatures loads it (with the given KubernetesClient and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification. buildFromFeatures builds a KubernetesDriverSpec (with the initial driver pod specification). In the end, buildFromFeatures configures the driver pod specification (with additional system properties and additional resources ) through a series of the feature steps: BasicDriverFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep DriverCommandFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep PodTemplateConfigMapStep LocalDirsFeatureStep buildFromFeatures is used when: Client is requested to run","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#kubernetesdriverbuilder","text":"KubernetesDriverBuilder is used to build a specification of a driver pod (for a Spark application deployed in cluster deploy mode).","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#creating-instance","text":"KubernetesDriverBuilder takes no arguments to be created. KubernetesDriverBuilder is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"KubernetesDriverBuilder/#kubernetesdriverspec","text":"KubernetesDriverSpec is the following: SparkPod Driver Resources System Properties","title":" KubernetesDriverSpec"},{"location":"KubernetesDriverBuilder/#building-pod-spec-for-driver","text":"buildFromFeatures ( conf : KubernetesDriverConf , client : KubernetesClient ) : KubernetesDriverSpec buildFromFeatures creates an initial driver pod specification. With spark.kubernetes.driver.podTemplateFile configuration property defined, buildFromFeatures loads it (with the given KubernetesClient and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification. buildFromFeatures builds a KubernetesDriverSpec (with the initial driver pod specification). In the end, buildFromFeatures configures the driver pod specification (with additional system properties and additional resources ) through a series of the feature steps: BasicDriverFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep DriverCommandFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep PodTemplateConfigMapStep LocalDirsFeatureStep buildFromFeatures is used when: Client is requested to run","title":" Building Pod Spec for Driver"},{"location":"KubernetesDriverConf/","text":"KubernetesDriverConf \u00b6 KubernetesDriverConf is a KubernetesConf . Creating Instance \u00b6 KubernetesDriverConf takes the following to be created: SparkConf Application ID MainAppResource Name of the Main Class Application Arguments KubernetesDriverConf is created when: KubernetesClientApplication is requested to start (via KubernetesConf utility ) Volume Specs \u00b6 volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the driver (with the spark.kubernetes.driver.volumes. prefix) from the SparkConf .","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#kubernetesdriverconf","text":"KubernetesDriverConf is a KubernetesConf .","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#creating-instance","text":"KubernetesDriverConf takes the following to be created: SparkConf Application ID MainAppResource Name of the Main Class Application Arguments KubernetesDriverConf is created when: KubernetesClientApplication is requested to start (via KubernetesConf utility )","title":"Creating Instance"},{"location":"KubernetesDriverConf/#volume-specs","text":"volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the driver (with the spark.kubernetes.driver.volumes. prefix) from the SparkConf .","title":" Volume Specs"},{"location":"KubernetesDriverEndpoint/","text":"KubernetesDriverEndpoint \u00b6 KubernetesDriverEndpoint is a DriverEndpoint ( Apache Spark ). Intercepting Executor Lost Event \u00b6 onDisconnected ( rpcAddress : RpcAddress ) : Unit onDisconnected is part of the RpcEndpoint ( Apache Spark ) abstraction. onDisconnected disables the executor known by the RpcAddress (found in the Executors by RpcAddress Registry registry).","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#kubernetesdriverendpoint","text":"KubernetesDriverEndpoint is a DriverEndpoint ( Apache Spark ).","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#intercepting-executor-lost-event","text":"onDisconnected ( rpcAddress : RpcAddress ) : Unit onDisconnected is part of the RpcEndpoint ( Apache Spark ) abstraction. onDisconnected disables the executor known by the RpcAddress (found in the Executors by RpcAddress Registry registry).","title":" Intercepting Executor Lost Event"},{"location":"KubernetesExecutorBuilder/","text":"KubernetesExecutorBuilder \u00b6 KubernetesExecutorBuilder is used to build a pod spec of executors (when ExecutorPodsAllocator is requested to handle executor pods snapshots and finds executors to be requested from Kubernetes). Creating Instance \u00b6 KubernetesExecutorBuilder takes no arguments to be created ( and could really be a Scala utility object ). KubernetesExecutorBuilder is created when: KubernetesClusterManager is requested to create a SchedulerBackend (and creates a ExecutorPodsAllocator ) Building Pod Spec for Executors \u00b6 buildFromFeatures ( conf : KubernetesExecutorConf , secMgr : SecurityManager , client : KubernetesClient ) : SparkPod When defined, buildFromFeatures loads the pod spec from the pod template file (based on the spark.kubernetes.executor.podTemplateFile and spark.kubernetes.executor.podTemplateContainerName configuration properties). Otherwise, buildFromFeatures starts from an initial empty pod specification. In the end, buildFromFeatures configures the executor pod specification through a series of the feature steps: BasicExecutorFeatureStep ExecutorKubernetesCredentialsFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep LocalDirsFeatureStep buildFromFeatures is used when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes)","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorBuilder/#kubernetesexecutorbuilder","text":"KubernetesExecutorBuilder is used to build a pod spec of executors (when ExecutorPodsAllocator is requested to handle executor pods snapshots and finds executors to be requested from Kubernetes).","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorBuilder/#creating-instance","text":"KubernetesExecutorBuilder takes no arguments to be created ( and could really be a Scala utility object ). KubernetesExecutorBuilder is created when: KubernetesClusterManager is requested to create a SchedulerBackend (and creates a ExecutorPodsAllocator )","title":"Creating Instance"},{"location":"KubernetesExecutorBuilder/#building-pod-spec-for-executors","text":"buildFromFeatures ( conf : KubernetesExecutorConf , secMgr : SecurityManager , client : KubernetesClient ) : SparkPod When defined, buildFromFeatures loads the pod spec from the pod template file (based on the spark.kubernetes.executor.podTemplateFile and spark.kubernetes.executor.podTemplateContainerName configuration properties). Otherwise, buildFromFeatures starts from an initial empty pod specification. In the end, buildFromFeatures configures the executor pod specification through a series of the feature steps: BasicExecutorFeatureStep ExecutorKubernetesCredentialsFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep LocalDirsFeatureStep buildFromFeatures is used when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes)","title":" Building Pod Spec for Executors"},{"location":"KubernetesExecutorConf/","text":"KubernetesExecutorConf \u00b6 KubernetesExecutorConf is a KubernetesConf (for KubernetesExecutorBuilder to build an executor pod ). Creating Instance \u00b6 KubernetesExecutorConf takes the following to be created: SparkConf Application ID Executor ID Driver Pod KubernetesExecutorConf is created when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes via KubernetesConf utility ) Volume Specs \u00b6 volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the executor pod (with the spark.kubernetes.executor.volumes. prefix) from the SparkConf .","title":"KubernetesExecutorConf"},{"location":"KubernetesExecutorConf/#kubernetesexecutorconf","text":"KubernetesExecutorConf is a KubernetesConf (for KubernetesExecutorBuilder to build an executor pod ).","title":"KubernetesExecutorConf"},{"location":"KubernetesExecutorConf/#creating-instance","text":"KubernetesExecutorConf takes the following to be created: SparkConf Application ID Executor ID Driver Pod KubernetesExecutorConf is created when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes via KubernetesConf utility )","title":"Creating Instance"},{"location":"KubernetesExecutorConf/#volume-specs","text":"volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the executor pod (with the spark.kubernetes.executor.volumes. prefix) from the SparkConf .","title":" Volume Specs"},{"location":"KubernetesFeatureConfigStep/","text":"KubernetesFeatureConfigStep \u00b6 KubernetesFeatureConfigStep is an abstraction of Kubernetes pod features for drivers and executors. Contract \u00b6 Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod Used when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec Additional Kubernetes Resources \u00b6 getAdditionalKubernetesResources () : Seq [ HasMetadata ] Additional Kubernetes resources (that are going to created when Client is requested to run ) Default: empty Used when: KubernetesDriverBuilder is requested for a driver pod spec Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] Additional system properties of a driver pod (that are going to be part of spark.properties as a ConfigMap ) Default: empty Used when: KubernetesDriverBuilder is requested for a driver pod spec Implementations \u00b6 BasicDriverFeatureStep BasicExecutorFeatureStep DriverCommandFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep EnvSecretsFeatureStep ExecutorKubernetesCredentialsFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep LocalDirsFeatureStep MountSecretsFeatureStep MountVolumesFeatureStep PodTemplateConfigMapStep","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#kubernetesfeatureconfigstep","text":"KubernetesFeatureConfigStep is an abstraction of Kubernetes pod features for drivers and executors.","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#contract","text":"","title":"Contract"},{"location":"KubernetesFeatureConfigStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod Used when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec","title":" Configuring Pod"},{"location":"KubernetesFeatureConfigStep/#additional-kubernetes-resources","text":"getAdditionalKubernetesResources () : Seq [ HasMetadata ] Additional Kubernetes resources (that are going to created when Client is requested to run ) Default: empty Used when: KubernetesDriverBuilder is requested for a driver pod spec","title":" Additional Kubernetes Resources"},{"location":"KubernetesFeatureConfigStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] Additional system properties of a driver pod (that are going to be part of spark.properties as a ConfigMap ) Default: empty Used when: KubernetesDriverBuilder is requested for a driver pod spec","title":" Additional System Properties"},{"location":"KubernetesFeatureConfigStep/#implementations","text":"BasicDriverFeatureStep BasicExecutorFeatureStep DriverCommandFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep EnvSecretsFeatureStep ExecutorKubernetesCredentialsFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep LocalDirsFeatureStep MountSecretsFeatureStep MountVolumesFeatureStep PodTemplateConfigMapStep","title":"Implementations"},{"location":"KubernetesUtils/","text":"KubernetesUtils Utility \u00b6 Parsing Master URL \u00b6 parseMasterUrl ( url : String ) : String parseMasterUrl takes off the k8s:// prefix from the given url. parseMasterUrl is used when: K8SSparkSubmitOperation is requested to execute an operation KubernetesClientApplication is requested to start KubernetesClusterManager is requested for a SchedulerBackend buildPodWithServiceAccount \u00b6 buildPodWithServiceAccount ( serviceAccount : Option [ String ], pod : SparkPod ) : Option [ Pod ] buildPodWithServiceAccount creates a new pod spec with the service account and service account name based on the given service account (if defined). Otherwise, buildPodWithServiceAccount returns None . buildPodWithServiceAccount is used when: DriverKubernetesCredentialsFeatureStep is requested to configure a pod ExecutorKubernetesCredentialsFeatureStep is requested to configure a pod Loading Pod Spec from Template File \u00b6 loadPodFromTemplate ( kubernetesClient : KubernetesClient , templateFile : File , containerName : Option [ String ]) : SparkPod loadPodFromTemplate requests the given KubernetesClient to load a pod spec from the input template file. loadPodFromTemplate selects the Spark container (from the pod spec and the input container name). In case of an Exception , loadPodFromTemplate prints out the following ERROR message to the logs: Encountered exception while attempting to load initial pod spec from file loadPodFromTemplate (re)throws a SparkException : Could not load pod from template file. loadPodFromTemplate is used when: KubernetesClusterManager is requested for a SchedulerBackend KubernetesDriverBuilder is requested for a pod spec for a driver KubernetesExecutorBuilder is requested for a pod spec for executors selectSparkContainer \u00b6 selectSparkContainer ( pod : Pod , containerName : Option [ String ]) : SparkPod selectSparkContainer creates a SparkPod based on the containers in the given Pod and the containerName . selectSparkContainer takes the container specs from the the given Pod spec and tries to find the one with the containerName or takes the first defined. selectSparkContainer includes the other containers in the pod spec. selectSparkContainer prints out the following WARN message to the logs when no container could be found by the given name: specified container [name] not found on the pod template, falling back to taking the first container Uploading Local Files to Hadoop DFS \u00b6 uploadAndTransformFileUris ( fileUris : Iterable [ String ], conf : Option [ SparkConf ] = None ) : Iterable [ String ] uploadAndTransformFileUris uploads local files in the given fileUris to Hadoop DFS (based on spark.kubernetes.file.upload.path configuration property). In the end, uploadAndTransformFileUris returns the target URIs. uploadAndTransformFileUris is used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties uploadFileUri \u00b6 uploadFileUri ( uri : String , conf : Option [ SparkConf ] = None ) : String uploadFileUri resolves the given uri to a well-formed file URI. uploadFileUri creates a new Hadoop Configuration and resolves the spark.kubernetes.file.upload.path configuration property to a Hadoop FileSystem . uploadFileUri creates ( mkdirs ) the Hadoop DFS path to upload the file of the format: [spark.kubernetes.file.upload.path]/[spark-upload-[randomUUID]] uploadFileUri prints out the following INFO message to the logs: Uploading file: [path] to dest: [targetUri]... In the end, uploadFileUri uploads the file to the target location (using Hadoop DFS's FileSystem.copyFromLocalFile ) and returns the target URI. SparkExceptions \u00b6 uploadFileUri throws a SparkException when: Uploading the uri fails: Uploading file [path] failed... spark.kubernetes.file.upload.path configuration property is not defined: Please specify spark.kubernetes.file.upload.path property. SparkConf is not defined: Spark configuration is missing... renameMainAppResource \u00b6 renameMainAppResource ( resource : String , conf : SparkConf ) : String renameMainAppResource is converted to spark-internal internal name when the given resource is local and resolvable . Otherwise, renameMainAppResource returns the given resource as-is. renameMainAppResource is used when: DriverCommandFeatureStep is requested for the base driver container (for a JavaMainAppResource application) isLocalAndResolvable \u00b6 isLocalAndResolvable ( resource : String ) : Boolean isLocalAndResolvable is true when the given resource is: Not internal Uses either file or no URI scheme (after converting to a well-formed URI) isLocalAndResolvable is used when: KubernetesUtils is requested to renameMainAppResource BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties isLocalDependency \u00b6 isLocalDependency ( uri : URI ) : Boolean An input URI is a local dependency when the scheme is null (undefined) or file . Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.KubernetesUtils logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.KubernetesUtils=ALL Refer to Logging .","title":"KubernetesUtils"},{"location":"KubernetesUtils/#kubernetesutils-utility","text":"","title":"KubernetesUtils Utility"},{"location":"KubernetesUtils/#parsing-master-url","text":"parseMasterUrl ( url : String ) : String parseMasterUrl takes off the k8s:// prefix from the given url. parseMasterUrl is used when: K8SSparkSubmitOperation is requested to execute an operation KubernetesClientApplication is requested to start KubernetesClusterManager is requested for a SchedulerBackend","title":" Parsing Master URL"},{"location":"KubernetesUtils/#buildpodwithserviceaccount","text":"buildPodWithServiceAccount ( serviceAccount : Option [ String ], pod : SparkPod ) : Option [ Pod ] buildPodWithServiceAccount creates a new pod spec with the service account and service account name based on the given service account (if defined). Otherwise, buildPodWithServiceAccount returns None . buildPodWithServiceAccount is used when: DriverKubernetesCredentialsFeatureStep is requested to configure a pod ExecutorKubernetesCredentialsFeatureStep is requested to configure a pod","title":" buildPodWithServiceAccount"},{"location":"KubernetesUtils/#loading-pod-spec-from-template-file","text":"loadPodFromTemplate ( kubernetesClient : KubernetesClient , templateFile : File , containerName : Option [ String ]) : SparkPod loadPodFromTemplate requests the given KubernetesClient to load a pod spec from the input template file. loadPodFromTemplate selects the Spark container (from the pod spec and the input container name). In case of an Exception , loadPodFromTemplate prints out the following ERROR message to the logs: Encountered exception while attempting to load initial pod spec from file loadPodFromTemplate (re)throws a SparkException : Could not load pod from template file. loadPodFromTemplate is used when: KubernetesClusterManager is requested for a SchedulerBackend KubernetesDriverBuilder is requested for a pod spec for a driver KubernetesExecutorBuilder is requested for a pod spec for executors","title":" Loading Pod Spec from Template File"},{"location":"KubernetesUtils/#selectsparkcontainer","text":"selectSparkContainer ( pod : Pod , containerName : Option [ String ]) : SparkPod selectSparkContainer creates a SparkPod based on the containers in the given Pod and the containerName . selectSparkContainer takes the container specs from the the given Pod spec and tries to find the one with the containerName or takes the first defined. selectSparkContainer includes the other containers in the pod spec. selectSparkContainer prints out the following WARN message to the logs when no container could be found by the given name: specified container [name] not found on the pod template, falling back to taking the first container","title":" selectSparkContainer"},{"location":"KubernetesUtils/#uploading-local-files-to-hadoop-dfs","text":"uploadAndTransformFileUris ( fileUris : Iterable [ String ], conf : Option [ SparkConf ] = None ) : Iterable [ String ] uploadAndTransformFileUris uploads local files in the given fileUris to Hadoop DFS (based on spark.kubernetes.file.upload.path configuration property). In the end, uploadAndTransformFileUris returns the target URIs. uploadAndTransformFileUris is used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties","title":" Uploading Local Files to Hadoop DFS"},{"location":"KubernetesUtils/#uploadfileuri","text":"uploadFileUri ( uri : String , conf : Option [ SparkConf ] = None ) : String uploadFileUri resolves the given uri to a well-formed file URI. uploadFileUri creates a new Hadoop Configuration and resolves the spark.kubernetes.file.upload.path configuration property to a Hadoop FileSystem . uploadFileUri creates ( mkdirs ) the Hadoop DFS path to upload the file of the format: [spark.kubernetes.file.upload.path]/[spark-upload-[randomUUID]] uploadFileUri prints out the following INFO message to the logs: Uploading file: [path] to dest: [targetUri]... In the end, uploadFileUri uploads the file to the target location (using Hadoop DFS's FileSystem.copyFromLocalFile ) and returns the target URI.","title":" uploadFileUri"},{"location":"KubernetesUtils/#sparkexceptions","text":"uploadFileUri throws a SparkException when: Uploading the uri fails: Uploading file [path] failed... spark.kubernetes.file.upload.path configuration property is not defined: Please specify spark.kubernetes.file.upload.path property. SparkConf is not defined: Spark configuration is missing...","title":" SparkExceptions"},{"location":"KubernetesUtils/#renamemainappresource","text":"renameMainAppResource ( resource : String , conf : SparkConf ) : String renameMainAppResource is converted to spark-internal internal name when the given resource is local and resolvable . Otherwise, renameMainAppResource returns the given resource as-is. renameMainAppResource is used when: DriverCommandFeatureStep is requested for the base driver container (for a JavaMainAppResource application)","title":" renameMainAppResource"},{"location":"KubernetesUtils/#islocalandresolvable","text":"isLocalAndResolvable ( resource : String ) : Boolean isLocalAndResolvable is true when the given resource is: Not internal Uses either file or no URI scheme (after converting to a well-formed URI) isLocalAndResolvable is used when: KubernetesUtils is requested to renameMainAppResource BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties","title":" isLocalAndResolvable"},{"location":"KubernetesUtils/#islocaldependency","text":"isLocalDependency ( uri : URI ) : Boolean An input URI is a local dependency when the scheme is null (undefined) or file .","title":" isLocalDependency"},{"location":"KubernetesUtils/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.KubernetesUtils logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.KubernetesUtils=ALL Refer to Logging .","title":"Logging"},{"location":"KubernetesVolumeUtils/","text":"KubernetesVolumeUtils \u00b6 parseVolumesWithPrefix \u00b6 parseVolumesWithPrefix ( sparkConf : SparkConf , prefix : String ) : Seq [ KubernetesVolumeSpec ] parseVolumesWithPrefix requests the SparkConf to get all properties with the given prefix . parseVolumesWithPrefix extracts volume types and names (from the keys of the properties) and for every pair creates a KubernetesVolumeSpec with the following: volumeName mountPath based on [volumeType].[volumeName].mount.path key (in the properties) mountSubPath based on [volumeType].[volumeName].mount.subPath key (in the properties) if available or defaults to an empty path mountReadOnly based on [volumeType].[volumeName].mount.readOnly key (in the properties) if available or false volumeConf with a KubernetesVolumeSpecificConf based on the properties and the volumeType and volumeName of the volume parseVolumesWithPrefix is used when: KubernetesDriverConf is requested for volumes KubernetesExecutorConf is requested for volumes KubernetesConf utility is used to create a KubernetesDriverConf Extracting Volume Types and Names \u00b6 getVolumeTypesAndNames ( properties : Map [ String , String ]) : Set [( String , String )] getVolumeTypesAndNames splits the keys (in the given properties key-value collection) by . to a pair of a volume type and a name. Extracting Volume Configuration \u00b6 parseVolumeSpecificConf ( options : Map [ String , String ], volumeType : String , volumeName : String ) : KubernetesVolumeSpecificConf parseVolumeSpecificConf creates a KubernetesVolumeSpecificConf based on the given volumeType . volumeType Keys hostPath [volumeType].[volumeName].options.path persistentVolumeClaim [volumeType].[volumeName].options.claimName emptyDir [volumeType].[volumeName].options.medium [volumeType].[volumeName].options.sizeLimit parseVolumeSpecificConf throws an IllegalArgumentException for unsupported volumeType : Kubernetes Volume type `[volumeType]` is not supported","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#kubernetesvolumeutils","text":"","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#parsevolumeswithprefix","text":"parseVolumesWithPrefix ( sparkConf : SparkConf , prefix : String ) : Seq [ KubernetesVolumeSpec ] parseVolumesWithPrefix requests the SparkConf to get all properties with the given prefix . parseVolumesWithPrefix extracts volume types and names (from the keys of the properties) and for every pair creates a KubernetesVolumeSpec with the following: volumeName mountPath based on [volumeType].[volumeName].mount.path key (in the properties) mountSubPath based on [volumeType].[volumeName].mount.subPath key (in the properties) if available or defaults to an empty path mountReadOnly based on [volumeType].[volumeName].mount.readOnly key (in the properties) if available or false volumeConf with a KubernetesVolumeSpecificConf based on the properties and the volumeType and volumeName of the volume parseVolumesWithPrefix is used when: KubernetesDriverConf is requested for volumes KubernetesExecutorConf is requested for volumes KubernetesConf utility is used to create a KubernetesDriverConf","title":" parseVolumesWithPrefix"},{"location":"KubernetesVolumeUtils/#extracting-volume-types-and-names","text":"getVolumeTypesAndNames ( properties : Map [ String , String ]) : Set [( String , String )] getVolumeTypesAndNames splits the keys (in the given properties key-value collection) by . to a pair of a volume type and a name.","title":" Extracting Volume Types and Names"},{"location":"KubernetesVolumeUtils/#extracting-volume-configuration","text":"parseVolumeSpecificConf ( options : Map [ String , String ], volumeType : String , volumeName : String ) : KubernetesVolumeSpecificConf parseVolumeSpecificConf creates a KubernetesVolumeSpecificConf based on the given volumeType . volumeType Keys hostPath [volumeType].[volumeName].options.path persistentVolumeClaim [volumeType].[volumeName].options.claimName emptyDir [volumeType].[volumeName].options.medium [volumeType].[volumeName].options.sizeLimit parseVolumeSpecificConf throws an IllegalArgumentException for unsupported volumeType : Kubernetes Volume type `[volumeType]` is not supported","title":" Extracting Volume Configuration"},{"location":"LocalDirsFeatureStep/","text":"LocalDirsFeatureStep \u00b6 LocalDirsFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 LocalDirsFeatureStep takes the following to be created: KubernetesConf Default Local Directory (default: /var/data/spark-[randomUUID] ) LocalDirsFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod KubernetesExecutorBuilder is requested to build an executor pod spark.kubernetes.local.dirs.tmpfs \u00b6 LocalDirsFeatureStep uses spark.kubernetes.local.dirs.tmpfs configuration property when configuring a pod . Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod finds mount paths of the volume mounts with spark-local-dir- prefix name of the input SparkPod ( localDirs ). If there are no local directory mount paths, configurePod ...FIXME configurePod adds the local directory volumes to a new pod specification (there could be none). configurePod defines SPARK_LOCAL_DIRS environment variable as a comma-separated local directories and adds the local directory volume mounts to a new container specification (there could be none). In the end, configurePod creates a new SparkPod with the new pod and container.","title":"LocalDirsFeatureStep"},{"location":"LocalDirsFeatureStep/#localdirsfeaturestep","text":"LocalDirsFeatureStep is a KubernetesFeatureConfigStep .","title":"LocalDirsFeatureStep"},{"location":"LocalDirsFeatureStep/#creating-instance","text":"LocalDirsFeatureStep takes the following to be created: KubernetesConf Default Local Directory (default: /var/data/spark-[randomUUID] ) LocalDirsFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod KubernetesExecutorBuilder is requested to build an executor pod","title":"Creating Instance"},{"location":"LocalDirsFeatureStep/#sparkkuberneteslocaldirstmpfs","text":"LocalDirsFeatureStep uses spark.kubernetes.local.dirs.tmpfs configuration property when configuring a pod .","title":" spark.kubernetes.local.dirs.tmpfs"},{"location":"LocalDirsFeatureStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod finds mount paths of the volume mounts with spark-local-dir- prefix name of the input SparkPod ( localDirs ). If there are no local directory mount paths, configurePod ...FIXME configurePod adds the local directory volumes to a new pod specification (there could be none). configurePod defines SPARK_LOCAL_DIRS environment variable as a comma-separated local directories and adds the local directory volume mounts to a new container specification (there could be none). In the end, configurePod creates a new SparkPod with the new pod and container.","title":" Configuring Pod"},{"location":"LoggingPodStatusWatcher/","text":"LoggingPodStatusWatcher \u00b6 LoggingPodStatusWatcher is an extension of Kubernetes' Watcher[Pod] for pod watchers that can watchOrStop . LoggingPodStatusWatcher is used to create a Client when a KubernetesClientApplication is requested to start . Contract \u00b6 watchOrStop \u00b6 watchOrStop ( submissionId : String ) : Unit Used when: Client is requested to run Implementations \u00b6 LoggingPodStatusWatcherImpl","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#loggingpodstatuswatcher","text":"LoggingPodStatusWatcher is an extension of Kubernetes' Watcher[Pod] for pod watchers that can watchOrStop . LoggingPodStatusWatcher is used to create a Client when a KubernetesClientApplication is requested to start .","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#contract","text":"","title":"Contract"},{"location":"LoggingPodStatusWatcher/#watchorstop","text":"watchOrStop ( submissionId : String ) : Unit Used when: Client is requested to run","title":" watchOrStop"},{"location":"LoggingPodStatusWatcher/#implementations","text":"LoggingPodStatusWatcherImpl","title":"Implementations"},{"location":"LoggingPodStatusWatcherImpl/","text":"LoggingPodStatusWatcherImpl \u00b6 LoggingPodStatusWatcherImpl is a LoggingPodStatusWatcher that monitors and logs the application status . Creating Instance \u00b6 LoggingPodStatusWatcherImpl takes the following to be created: KubernetesDriverConf LoggingPodStatusWatcherImpl is created when: KubernetesClientApplication is requested to start watchOrStop \u00b6 watchOrStop ( sId : String ) : Unit watchOrStop is part of the LoggingPodStatusWatcher abstraction. watchOrStop uses spark.kubernetes.submission.waitAppCompletion configuration property to control whether to wait for the Spark application to complete ( true ) or merely print out the following INFO message to the logs: Deployed Spark application [appName] with submission ID [sId] into Kubernetes While waiting for the Spark application to complete, watchOrStop prints out the following INFO message to the logs: Waiting for application [appName] with submission ID [sId] to finish... Until podCompleted flag is true , watchOrStop waits spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) Once podCompleted flag is true , watchOrStop prints out the following INFO messages to the logs: Container final statuses: [containersDescription] Application [appName] with submission ID [sId] finished When no pod is available, watchOrStop prints out the following INFO message to the logs: No containers were found in the driver pod. eventReceived \u00b6 eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes' Watcher abstraction. eventReceived brances off based on the given Action : For DELETED or ERROR actions, eventReceived closeWatch For any other actions, logLongStatus followed by closeWatch if hasCompleted . logLongStatus \u00b6 logLongStatus () : Unit logLongStatus prints out the following INFO message to the logs: State changed, new state: [formatPodState|unknown] hasCompleted \u00b6 hasCompleted () : Boolean hasCompleted is true when the phase is Succeeded or Failed . hasCompleted is used when: LoggingPodStatusWatcherImpl is requested to eventReceived (when an action is neither DELETED nor ERROR ) podCompleted Flag \u00b6 LoggingPodStatusWatcherImpl turns podCompleted off when created . Until podCompleted is on, LoggingPodStatusWatcherImpl waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) podCompleted turns podCompleted on when closeWatch . closeWatch \u00b6 closeWatch () : Unit closeWatch turns podCompleted on. closeWatch is used when: LoggingPodStatusWatcherImpl is requested to eventReceived and onClose Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL Refer to Logging .","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#loggingpodstatuswatcherimpl","text":"LoggingPodStatusWatcherImpl is a LoggingPodStatusWatcher that monitors and logs the application status .","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#creating-instance","text":"LoggingPodStatusWatcherImpl takes the following to be created: KubernetesDriverConf LoggingPodStatusWatcherImpl is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"LoggingPodStatusWatcherImpl/#watchorstop","text":"watchOrStop ( sId : String ) : Unit watchOrStop is part of the LoggingPodStatusWatcher abstraction. watchOrStop uses spark.kubernetes.submission.waitAppCompletion configuration property to control whether to wait for the Spark application to complete ( true ) or merely print out the following INFO message to the logs: Deployed Spark application [appName] with submission ID [sId] into Kubernetes While waiting for the Spark application to complete, watchOrStop prints out the following INFO message to the logs: Waiting for application [appName] with submission ID [sId] to finish... Until podCompleted flag is true , watchOrStop waits spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) Once podCompleted flag is true , watchOrStop prints out the following INFO messages to the logs: Container final statuses: [containersDescription] Application [appName] with submission ID [sId] finished When no pod is available, watchOrStop prints out the following INFO message to the logs: No containers were found in the driver pod.","title":" watchOrStop"},{"location":"LoggingPodStatusWatcherImpl/#eventreceived","text":"eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes' Watcher abstraction. eventReceived brances off based on the given Action : For DELETED or ERROR actions, eventReceived closeWatch For any other actions, logLongStatus followed by closeWatch if hasCompleted .","title":" eventReceived"},{"location":"LoggingPodStatusWatcherImpl/#loglongstatus","text":"logLongStatus () : Unit logLongStatus prints out the following INFO message to the logs: State changed, new state: [formatPodState|unknown]","title":" logLongStatus"},{"location":"LoggingPodStatusWatcherImpl/#hascompleted","text":"hasCompleted () : Boolean hasCompleted is true when the phase is Succeeded or Failed . hasCompleted is used when: LoggingPodStatusWatcherImpl is requested to eventReceived (when an action is neither DELETED nor ERROR )","title":" hasCompleted"},{"location":"LoggingPodStatusWatcherImpl/#podcompleted-flag","text":"LoggingPodStatusWatcherImpl turns podCompleted off when created . Until podCompleted is on, LoggingPodStatusWatcherImpl waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) podCompleted turns podCompleted on when closeWatch .","title":" podCompleted Flag"},{"location":"LoggingPodStatusWatcherImpl/#closewatch","text":"closeWatch () : Unit closeWatch turns podCompleted on. closeWatch is used when: LoggingPodStatusWatcherImpl is requested to eventReceived and onClose","title":" closeWatch"},{"location":"LoggingPodStatusWatcherImpl/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL Refer to Logging .","title":"Logging"},{"location":"MountSecretsFeatureStep/","text":"MountSecretsFeatureStep \u00b6 MountSecretsFeatureStep is...FIXME","title":"MountSecretsFeatureStep"},{"location":"MountSecretsFeatureStep/#mountsecretsfeaturestep","text":"MountSecretsFeatureStep is...FIXME","title":"MountSecretsFeatureStep"},{"location":"MountVolumesFeatureStep/","text":"MountVolumesFeatureStep \u00b6 MountVolumesFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 MountVolumesFeatureStep takes the following to be created: KubernetesConf MountVolumesFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod constructs the volumes and volume mounts from the volumes (of the KubernetesConf ) and creates a new SparkPod : Adds the volumes to the pod specification Adds the volume mounts to the container specification constructVolumes \u00b6 constructVolumes ( volumeSpecs : Iterable [ KubernetesVolumeSpec ]) : Iterable [( VolumeMount , Volume )] constructVolumes creates Kubernetes VolumeMount s and Volume s based on the given KubernetesVolumeSpec specs. VolumeMount s are built based on the following: mountPath mountReadOnly mountSubPath volumeName Volume s are build based on the type of the volume: HostPath PersistentVolumeClaim EmptyDir In the end, Volume s and VolumeMount s are wired together using volumeName .","title":"MountVolumesFeatureStep"},{"location":"MountVolumesFeatureStep/#mountvolumesfeaturestep","text":"MountVolumesFeatureStep is a KubernetesFeatureConfigStep .","title":"MountVolumesFeatureStep"},{"location":"MountVolumesFeatureStep/#creating-instance","text":"MountVolumesFeatureStep takes the following to be created: KubernetesConf MountVolumesFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec","title":"Creating Instance"},{"location":"MountVolumesFeatureStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod constructs the volumes and volume mounts from the volumes (of the KubernetesConf ) and creates a new SparkPod : Adds the volumes to the pod specification Adds the volume mounts to the container specification","title":" Configuring Driver Pod"},{"location":"MountVolumesFeatureStep/#constructvolumes","text":"constructVolumes ( volumeSpecs : Iterable [ KubernetesVolumeSpec ]) : Iterable [( VolumeMount , Volume )] constructVolumes creates Kubernetes VolumeMount s and Volume s based on the given KubernetesVolumeSpec specs. VolumeMount s are built based on the following: mountPath mountReadOnly mountSubPath volumeName Volume s are build based on the type of the volume: HostPath PersistentVolumeClaim EmptyDir In the end, Volume s and VolumeMount s are wired together using volumeName .","title":" constructVolumes"},{"location":"PodTemplateConfigMapStep/","text":"PodTemplateConfigMapStep \u00b6 PodTemplateConfigMapStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 PodTemplateConfigMapStep takes the following to be created: KubernetesConf PodTemplateConfigMapStep is created when: KubernetesDriverBuilder is requested for a driver pod spec hasTemplate Flag \u00b6 PodTemplateConfigMapStep uses hasTemplate flag based on the spark.kubernetes.executor.podTemplateFile configuration property to control whether or not to configure a pod , and offer Additional Pod System Properties and Additional Kubernetes Resources . Additional Kubernetes Resources \u00b6 getAdditionalKubernetesResources () : Seq [ HasMetadata ] getAdditionalKubernetesResources is part of the KubernetesFeatureConfigStep abstraction. getAdditionalKubernetesResources ...FIXME Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties ...FIXME Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":"PodTemplateConfigMapStep"},{"location":"PodTemplateConfigMapStep/#podtemplateconfigmapstep","text":"PodTemplateConfigMapStep is a KubernetesFeatureConfigStep .","title":"PodTemplateConfigMapStep"},{"location":"PodTemplateConfigMapStep/#creating-instance","text":"PodTemplateConfigMapStep takes the following to be created: KubernetesConf PodTemplateConfigMapStep is created when: KubernetesDriverBuilder is requested for a driver pod spec","title":"Creating Instance"},{"location":"PodTemplateConfigMapStep/#hastemplate-flag","text":"PodTemplateConfigMapStep uses hasTemplate flag based on the spark.kubernetes.executor.podTemplateFile configuration property to control whether or not to configure a pod , and offer Additional Pod System Properties and Additional Kubernetes Resources .","title":" hasTemplate Flag"},{"location":"PodTemplateConfigMapStep/#additional-kubernetes-resources","text":"getAdditionalKubernetesResources () : Seq [ HasMetadata ] getAdditionalKubernetesResources is part of the KubernetesFeatureConfigStep abstraction. getAdditionalKubernetesResources ...FIXME","title":" Additional Kubernetes Resources"},{"location":"PodTemplateConfigMapStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties ...FIXME","title":" Additional System Properties"},{"location":"PodTemplateConfigMapStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Driver Pod"},{"location":"PollRunnable/","text":"PollRunnable \u00b6 PollRunnable is a Java Runnable that ExecutorPodsPollingSnapshotSource schedules for full executor pod state snapshots from Kubernetes every spark.kubernetes.executor.apiPollingInterval in the Spark application . PollRunnable is an internal class of ExecutorPodsPollingSnapshotSource with full access to its internals. Creating Instance \u00b6 PollRunnable takes the following to be created: Application Id PollRunnable is created when: ExecutorPodsPollingSnapshotSource is requested to start Starting Thread \u00b6 run () : Unit run prints out the following DEBUG message to the logs: Resynchronizing full executor pod state from Kubernetes. run requests the KubernetesClient for Spark executor pods with the following labels and values. Label Name Value spark-app-selector application Id spark-role executor spark-exec-inactive any value but true In the end, run requests the ExecutorPodsSnapshotsStore to replace the snapshot . Logging \u00b6 PollRunnable uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.","title":"PollRunnable"},{"location":"PollRunnable/#pollrunnable","text":"PollRunnable is a Java Runnable that ExecutorPodsPollingSnapshotSource schedules for full executor pod state snapshots from Kubernetes every spark.kubernetes.executor.apiPollingInterval in the Spark application . PollRunnable is an internal class of ExecutorPodsPollingSnapshotSource with full access to its internals.","title":"PollRunnable"},{"location":"PollRunnable/#creating-instance","text":"PollRunnable takes the following to be created: Application Id PollRunnable is created when: ExecutorPodsPollingSnapshotSource is requested to start","title":"Creating Instance"},{"location":"PollRunnable/#starting-thread","text":"run () : Unit run prints out the following DEBUG message to the logs: Resynchronizing full executor pod state from Kubernetes. run requests the KubernetesClient for Spark executor pods with the following labels and values. Label Name Value spark-app-selector application Id spark-role executor spark-exec-inactive any value but true In the end, run requests the ExecutorPodsSnapshotsStore to replace the snapshot .","title":" Starting Thread"},{"location":"PollRunnable/#logging","text":"PollRunnable uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.","title":"Logging"},{"location":"SparkKubernetesClientFactory/","text":"SparkKubernetesClientFactory Utility \u00b6 SparkKubernetesClientFactory is a Spark-opinionated builder for Kubernetes clients . Creating KubernetesClient \u00b6 createKubernetesClient ( master : String , namespace : Option [ String ], kubernetesAuthConfPrefix : String , clientType : ClientType.Value , sparkConf : SparkConf , defaultServiceAccountToken : Option [ File ], defaultServiceAccountCaCert : Option [ File ]) : KubernetesClient createKubernetesClient utility takes the OAuth token-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .oauthTokenFile (or defaults to the input defaultServiceAccountToken ) kubernetesAuthConfPrefix .oauthToken createKubernetesClient takes the spark.kubernetes.context configuraiton property ( kubeContext ). createKubernetesClient takes the certificate-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .caCertFile (or defaults to the input defaultServiceAccountCaCert ) kubernetesAuthConfPrefix .clientKeyFile kubernetesAuthConfPrefix .clientCertFile createKubernetesClient prints out the following INFO message to the logs: Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file createKubernetesClient builds a Kubernetes Config (based on the configuration properties). createKubernetesClient builds an OkHttpClient with a custom kubernetes-dispatcher dispatcher. In the end, createKubernetesClient creates a Kubernetes DefaultKubernetesClient (with the OkHttpClient and Config ). createKubernetesClient is used when: K8SSparkSubmitOperation is requested to execute KubernetesClientApplication is requested to start KubernetesClusterManager is requested to create a SchedulerBackend Exceptions \u00b6 createKubernetesClient throws an IllegalArgumentException when an OAuth token is specified through a file and a value: Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf]. Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.SparkKubernetesClientFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL Refer to Logging .","title":"SparkKubernetesClientFactory"},{"location":"SparkKubernetesClientFactory/#sparkkubernetesclientfactory-utility","text":"SparkKubernetesClientFactory is a Spark-opinionated builder for Kubernetes clients .","title":"SparkKubernetesClientFactory Utility"},{"location":"SparkKubernetesClientFactory/#creating-kubernetesclient","text":"createKubernetesClient ( master : String , namespace : Option [ String ], kubernetesAuthConfPrefix : String , clientType : ClientType.Value , sparkConf : SparkConf , defaultServiceAccountToken : Option [ File ], defaultServiceAccountCaCert : Option [ File ]) : KubernetesClient createKubernetesClient utility takes the OAuth token-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .oauthTokenFile (or defaults to the input defaultServiceAccountToken ) kubernetesAuthConfPrefix .oauthToken createKubernetesClient takes the spark.kubernetes.context configuraiton property ( kubeContext ). createKubernetesClient takes the certificate-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .caCertFile (or defaults to the input defaultServiceAccountCaCert ) kubernetesAuthConfPrefix .clientKeyFile kubernetesAuthConfPrefix .clientCertFile createKubernetesClient prints out the following INFO message to the logs: Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file createKubernetesClient builds a Kubernetes Config (based on the configuration properties). createKubernetesClient builds an OkHttpClient with a custom kubernetes-dispatcher dispatcher. In the end, createKubernetesClient creates a Kubernetes DefaultKubernetesClient (with the OkHttpClient and Config ). createKubernetesClient is used when: K8SSparkSubmitOperation is requested to execute KubernetesClientApplication is requested to start KubernetesClusterManager is requested to create a SchedulerBackend","title":" Creating KubernetesClient"},{"location":"SparkKubernetesClientFactory/#exceptions","text":"createKubernetesClient throws an IllegalArgumentException when an OAuth token is specified through a file and a value: Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf].","title":"Exceptions"},{"location":"SparkKubernetesClientFactory/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.SparkKubernetesClientFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL Refer to Logging .","title":"Logging"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 spark.kubernetes.allocation.batch.delay \u00b6 Time (in millis) to wait between each round of executor allocation Default: 1s Used when: ExecutorPodsAllocator is created spark.kubernetes.appKillPodDeletionGracePeriod \u00b6 Grace Period that is the time (in seconds) to wait for a graceful deletion of Spark pods when spark-submit --kill Default: (undefined) Used when: K8SSparkSubmitOperation is requested to kill spark.kubernetes.allocation.batch.size \u00b6 Minimum number of executor pods to allocate at once in each round of executor allocation Default: 5 Used when: ExecutorPodsAllocator is created spark.kubernetes.allocation.executor.timeout \u00b6 Time (in millis) to wait before a pending executor is considered timed out Default: 600s Used when: ExecutorPodsAllocator is requested to handle executor pods snapshots spark.kubernetes.authenticate \u00b6 FIXME spark.kubernetes.authenticate.driver.mounted \u00b6 FIXME spark.kubernetes.authenticate.executor.serviceAccountName \u00b6 Service account for executor pods Default: (undefined) Used when: ExecutorKubernetesCredentialsFeatureStep is requested to configure a pod spark.kubernetes.configMap.maxSize \u00b6 Max size limit ( long ) for a config map. Configurable as per https://etcd.io/docs/v3.4.0/dev-guide/limit/ on k8s server end. Default: 1572864 ( 1.5 MiB ) Used when: KubernetesClientUtils utility is used to loadSparkConfDirFiles spark.kubernetes.container.image \u00b6 Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined) Default: (undefined) spark.kubernetes.container.image.pullPolicy \u00b6 Kubernetes image pull policy: Always Never IfNotPresent Default: IfNotPresent Used when: KubernetesConf is requested for imagePullPolicy spark.kubernetes.context \u00b6 The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the KUBECONFIG environment variable or by defaulting to .kube/config under your home directory. If not specified then your current context is used. You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options. Default: (undefined) Used when: SparkKubernetesClientFactory is requested to create a KubernetesClient spark.kubernetes.driver.container.image \u00b6 Container image for drivers Default: spark.kubernetes.container.image Used when: BasicDriverFeatureStep is requested for a driverContainerImage spark.kubernetes.driver.master \u00b6 The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc spark.kubernetes.driver.pod.name \u00b6 Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed in cluster deploy mode Used when: BasicDriverFeatureStep is requested for the driverPodName (and additional system properties of a driver pod ) ExecutorPodsAllocator is requested for the kubernetesDriverPodName spark.kubernetes.driver.podTemplateContainerName \u00b6 Name of the driver container in a pod template Default: (undefined) Used when: KubernetesDriverBuilder is requested for a driver pod specification spark.kubernetes.driver.podTemplateFile \u00b6 Pod template file for drivers (in cluster deploy mode) Default: (undefined) Used when: KubernetesDriverBuilder is requested for a driver pod specification spark.kubernetes.driver.request.cores \u00b6 Specify the cpu request for the driver pod Default: (undefined) Used when: BasicDriverFeatureStep is requested to configure a pod spark.kubernetes.executor.apiPollingInterval \u00b6 Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors. Default: 30s Used when: ExecutorPodsPollingSnapshotSource is requested to start spark.kubernetes.executor.checkAllContainers \u00b6 Controls whether or not to check the status of all containers in a running executor pod when reporting executor status Default: false Used when: KubernetesClusterManager is requested for a SchedulerBackend spark.kubernetes.executor.container.image \u00b6 Container image for executors Default: spark.kubernetes.container.image Used when: BasicExecutorFeatureStep is requested for a driverContainerImage spark.kubernetes.executor.deleteOnTermination \u00b6 Controls whether or not to delete executor pods after they have finished (successfully or not) Default: true Used when: ExecutorPodsAllocator is requested to handle executor pods snapshots ExecutorPodsLifecycleManager is requested to handle executor pods snapshots KubernetesClusterSchedulerBackend is requested to stop spark.kubernetes.executor.eventProcessingInterval \u00b6 Interval (in millis) between successive inspection of executor events sent from the Kubernetes API Default: 1s Used when: ExecutorPodsLifecycleManager is requested to start and register a new subscriber spark.kubernetes.executor.missingPodDetectDelta \u00b6 Time (in millis) to wait before an executor is removed due to the executor's pod being missed in the Kubernetes API server's polled list of pods Default: 30s Used when: ExecutorPodsLifecycleManager is requested to handle executor pods snapshots spark.kubernetes.executor.podNamePrefix \u00b6 (internal) Prefix of the executor pod names Default: (undefined) Unless defined, it is set explicitly when KubernetesClusterManager is requested to create a SchedulerBackend Used when: KubernetesExecutorConf is requested for the resourceNamePrefix spark.kubernetes.executor.podTemplateContainerName \u00b6 Name of the container for executors in a pod template Default: (undefined) Used when: KubernetesClusterManager is requested for a SchedulerBackend KubernetesExecutorBuilder is requested for a pod spec for executors spark.kubernetes.executor.podTemplateFile \u00b6 Pod template file for executors Default: (undefined) Used when: KubernetesClusterManager is requested to create a SchedulerBackend KubernetesExecutorBuilder is requested to build an executor pod PodTemplateConfigMapStep is created and requested to configurePod , getAdditionalPodSystemProperties , getAdditionalKubernetesResources spark.kubernetes.executor.request.cores \u00b6 Specifies the cpu quantity request for executor pods (to be more Kubernetes-oriented when requesting resources for executor pods than Spark scheduler's approach based on spark.executor.cores ). Default: (undefined) Used when: BasicExecutorFeatureStep is requested to configure an executor pod spark.kubernetes.executor.scheduler.name \u00b6 Name of the scheduler for executor pods (a pod's spec.schedulerName ) Default: (undefined) Used when: BasicExecutorFeatureStep is requested to configure an executor pod spark.kubernetes.file.upload.path \u00b6 Hadoop DFS-compatible file system path where files from the local file system will be uploded to in cluster deploy mode. The subdirectories (one per Spark application) with the local files are of the format spark-upload-[uuid] . Default: (undefined) Used when: KubernetesUtils is requested to uploadFileUri spark.kubernetes.local.dirs.tmpfs \u00b6 If true , emptyDir volumes created to back SPARK_LOCAL_DIRS will have their medium set to Memory so that they will be created as tmpfs (i.e. RAM) backed volumes. This may improve performance but scratch space usage will count towards your pods memory limit so you may wish to request more memory. Default: false Used when: LocalDirsFeatureStep is requested to configure a pod spark.kubernetes.memoryOverheadFactor \u00b6 Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs Must be a double between (0, 1.0) Default: 0.1 Used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod spark.kubernetes.namespace \u00b6 Namespace for running the driver and executor pods Default: default Used when: KubernetesConf is requested for namespace KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsAllocator is created (and initializes namespace ) spark.kubernetes.report.interval \u00b6 Interval between reports of the current app status in cluster mode Default: 1s Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop spark.kubernetes.submission.waitAppCompletion \u00b6 In cluster deploy mode, whether to wait for the application to finish before exiting the launcher process. Default: true Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop spark.kubernetes.submitInDriver \u00b6 (internal) Whether executing in cluster deploy mode Default: false spark.kubernetes.submitInDriver is true in BasicDriverFeatureStep . Used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties KubernetesClusterManager is requested for a SchedulerBackend","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkkubernetesallocationbatchdelay","text":"Time (in millis) to wait between each round of executor allocation Default: 1s Used when: ExecutorPodsAllocator is created","title":" spark.kubernetes.allocation.batch.delay"},{"location":"configuration-properties/#sparkkubernetesappkillpoddeletiongraceperiod","text":"Grace Period that is the time (in seconds) to wait for a graceful deletion of Spark pods when spark-submit --kill Default: (undefined) Used when: K8SSparkSubmitOperation is requested to kill","title":" spark.kubernetes.appKillPodDeletionGracePeriod"},{"location":"configuration-properties/#sparkkubernetesallocationbatchsize","text":"Minimum number of executor pods to allocate at once in each round of executor allocation Default: 5 Used when: ExecutorPodsAllocator is created","title":" spark.kubernetes.allocation.batch.size"},{"location":"configuration-properties/#sparkkubernetesallocationexecutortimeout","text":"Time (in millis) to wait before a pending executor is considered timed out Default: 600s Used when: ExecutorPodsAllocator is requested to handle executor pods snapshots","title":" spark.kubernetes.allocation.executor.timeout"},{"location":"configuration-properties/#sparkkubernetesauthenticate","text":"FIXME","title":" spark.kubernetes.authenticate"},{"location":"configuration-properties/#sparkkubernetesauthenticatedrivermounted","text":"FIXME","title":" spark.kubernetes.authenticate.driver.mounted"},{"location":"configuration-properties/#sparkkubernetesauthenticateexecutorserviceaccountname","text":"Service account for executor pods Default: (undefined) Used when: ExecutorKubernetesCredentialsFeatureStep is requested to configure a pod","title":" spark.kubernetes.authenticate.executor.serviceAccountName"},{"location":"configuration-properties/#sparkkubernetesconfigmapmaxsize","text":"Max size limit ( long ) for a config map. Configurable as per https://etcd.io/docs/v3.4.0/dev-guide/limit/ on k8s server end. Default: 1572864 ( 1.5 MiB ) Used when: KubernetesClientUtils utility is used to loadSparkConfDirFiles","title":" spark.kubernetes.configMap.maxSize"},{"location":"configuration-properties/#sparkkubernetescontainerimage","text":"Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined) Default: (undefined)","title":" spark.kubernetes.container.image"},{"location":"configuration-properties/#sparkkubernetescontainerimagepullpolicy","text":"Kubernetes image pull policy: Always Never IfNotPresent Default: IfNotPresent Used when: KubernetesConf is requested for imagePullPolicy","title":" spark.kubernetes.container.image.pullPolicy"},{"location":"configuration-properties/#sparkkubernetescontext","text":"The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the KUBECONFIG environment variable or by defaulting to .kube/config under your home directory. If not specified then your current context is used. You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options. Default: (undefined) Used when: SparkKubernetesClientFactory is requested to create a KubernetesClient","title":" spark.kubernetes.context"},{"location":"configuration-properties/#sparkkubernetesdrivercontainerimage","text":"Container image for drivers Default: spark.kubernetes.container.image Used when: BasicDriverFeatureStep is requested for a driverContainerImage","title":" spark.kubernetes.driver.container.image"},{"location":"configuration-properties/#sparkkubernetesdrivermaster","text":"The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc","title":" spark.kubernetes.driver.master"},{"location":"configuration-properties/#sparkkubernetesdriverpodname","text":"Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed in cluster deploy mode Used when: BasicDriverFeatureStep is requested for the driverPodName (and additional system properties of a driver pod ) ExecutorPodsAllocator is requested for the kubernetesDriverPodName","title":" spark.kubernetes.driver.pod.name"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatecontainername","text":"Name of the driver container in a pod template Default: (undefined) Used when: KubernetesDriverBuilder is requested for a driver pod specification","title":" spark.kubernetes.driver.podTemplateContainerName"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatefile","text":"Pod template file for drivers (in cluster deploy mode) Default: (undefined) Used when: KubernetesDriverBuilder is requested for a driver pod specification","title":" spark.kubernetes.driver.podTemplateFile"},{"location":"configuration-properties/#sparkkubernetesdriverrequestcores","text":"Specify the cpu request for the driver pod Default: (undefined) Used when: BasicDriverFeatureStep is requested to configure a pod","title":" spark.kubernetes.driver.request.cores"},{"location":"configuration-properties/#sparkkubernetesexecutorapipollinginterval","text":"Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors. Default: 30s Used when: ExecutorPodsPollingSnapshotSource is requested to start","title":" spark.kubernetes.executor.apiPollingInterval"},{"location":"configuration-properties/#sparkkubernetesexecutorcheckallcontainers","text":"Controls whether or not to check the status of all containers in a running executor pod when reporting executor status Default: false Used when: KubernetesClusterManager is requested for a SchedulerBackend","title":" spark.kubernetes.executor.checkAllContainers"},{"location":"configuration-properties/#sparkkubernetesexecutorcontainerimage","text":"Container image for executors Default: spark.kubernetes.container.image Used when: BasicExecutorFeatureStep is requested for a driverContainerImage","title":" spark.kubernetes.executor.container.image"},{"location":"configuration-properties/#sparkkubernetesexecutordeleteontermination","text":"Controls whether or not to delete executor pods after they have finished (successfully or not) Default: true Used when: ExecutorPodsAllocator is requested to handle executor pods snapshots ExecutorPodsLifecycleManager is requested to handle executor pods snapshots KubernetesClusterSchedulerBackend is requested to stop","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"configuration-properties/#sparkkubernetesexecutoreventprocessinginterval","text":"Interval (in millis) between successive inspection of executor events sent from the Kubernetes API Default: 1s Used when: ExecutorPodsLifecycleManager is requested to start and register a new subscriber","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"configuration-properties/#sparkkubernetesexecutormissingpoddetectdelta","text":"Time (in millis) to wait before an executor is removed due to the executor's pod being missed in the Kubernetes API server's polled list of pods Default: 30s Used when: ExecutorPodsLifecycleManager is requested to handle executor pods snapshots","title":" spark.kubernetes.executor.missingPodDetectDelta"},{"location":"configuration-properties/#sparkkubernetesexecutorpodnameprefix","text":"(internal) Prefix of the executor pod names Default: (undefined) Unless defined, it is set explicitly when KubernetesClusterManager is requested to create a SchedulerBackend Used when: KubernetesExecutorConf is requested for the resourceNamePrefix","title":" spark.kubernetes.executor.podNamePrefix"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatecontainername","text":"Name of the container for executors in a pod template Default: (undefined) Used when: KubernetesClusterManager is requested for a SchedulerBackend KubernetesExecutorBuilder is requested for a pod spec for executors","title":" spark.kubernetes.executor.podTemplateContainerName"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatefile","text":"Pod template file for executors Default: (undefined) Used when: KubernetesClusterManager is requested to create a SchedulerBackend KubernetesExecutorBuilder is requested to build an executor pod PodTemplateConfigMapStep is created and requested to configurePod , getAdditionalPodSystemProperties , getAdditionalKubernetesResources","title":" spark.kubernetes.executor.podTemplateFile"},{"location":"configuration-properties/#sparkkubernetesexecutorrequestcores","text":"Specifies the cpu quantity request for executor pods (to be more Kubernetes-oriented when requesting resources for executor pods than Spark scheduler's approach based on spark.executor.cores ). Default: (undefined) Used when: BasicExecutorFeatureStep is requested to configure an executor pod","title":" spark.kubernetes.executor.request.cores"},{"location":"configuration-properties/#sparkkubernetesexecutorschedulername","text":"Name of the scheduler for executor pods (a pod's spec.schedulerName ) Default: (undefined) Used when: BasicExecutorFeatureStep is requested to configure an executor pod","title":" spark.kubernetes.executor.scheduler.name"},{"location":"configuration-properties/#sparkkubernetesfileuploadpath","text":"Hadoop DFS-compatible file system path where files from the local file system will be uploded to in cluster deploy mode. The subdirectories (one per Spark application) with the local files are of the format spark-upload-[uuid] . Default: (undefined) Used when: KubernetesUtils is requested to uploadFileUri","title":" spark.kubernetes.file.upload.path"},{"location":"configuration-properties/#sparkkuberneteslocaldirstmpfs","text":"If true , emptyDir volumes created to back SPARK_LOCAL_DIRS will have their medium set to Memory so that they will be created as tmpfs (i.e. RAM) backed volumes. This may improve performance but scratch space usage will count towards your pods memory limit so you may wish to request more memory. Default: false Used when: LocalDirsFeatureStep is requested to configure a pod","title":" spark.kubernetes.local.dirs.tmpfs"},{"location":"configuration-properties/#sparkkubernetesmemoryoverheadfactor","text":"Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs Must be a double between (0, 1.0) Default: 0.1 Used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod","title":" spark.kubernetes.memoryOverheadFactor"},{"location":"configuration-properties/#sparkkubernetesnamespace","text":"Namespace for running the driver and executor pods Default: default Used when: KubernetesConf is requested for namespace KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsAllocator is created (and initializes namespace )","title":" spark.kubernetes.namespace"},{"location":"configuration-properties/#sparkkubernetesreportinterval","text":"Interval between reports of the current app status in cluster mode Default: 1s Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop","title":" spark.kubernetes.report.interval"},{"location":"configuration-properties/#sparkkubernetessubmissionwaitappcompletion","text":"In cluster deploy mode, whether to wait for the application to finish before exiting the launcher process. Default: true Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop","title":" spark.kubernetes.submission.waitAppCompletion"},{"location":"configuration-properties/#sparkkubernetessubmitindriver","text":"(internal) Whether executing in cluster deploy mode Default: false spark.kubernetes.submitInDriver is true in BasicDriverFeatureStep . Used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties KubernetesClusterManager is requested for a SchedulerBackend","title":" spark.kubernetes.submitInDriver"},{"location":"overview/","text":"Spark on Kubernetes \u00b6 Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Apache Spark supports Kubernetes resource manager using KubernetesClusterManager (and KubernetesClusterSchedulerBackend ) with k8s:// -prefixed master URLs (that point at Kubernetes API servers ). Spark on Kubernetes uses TaskSchedulerImpl ( Apache Spark ) for task scheduling. Kubernetes GA in Spark 3.1.1 \u00b6 As per SPARK-33005 Kubernetes GA Preparation , Spark 3.1.1 comes with many improvements for Kubernetes support and is expected to get General Availability (GA) marker \ud83c\udf89 Note There will never be 3.1.0 . Inactive Executor Pods \u00b6 Spark on Kubernetes defines spark-exec-inactive label to mark executor pods as inactive after they have finished (successfully or not) but spark.kubernetes.executor.deleteOnTermination configuration property is false (when ExecutorPodsLifecycleManager is requested to handle executor pods snapshots ). This label is used to skip executor pods when PollRunnable is requested to fetch status of all executor pods in a Spark application from Kubernetes API server . Cluster Deploy Mode \u00b6 Spark on Kubernetes uses KubernetesClientApplication in cluster deploy mode (as the SparkApplication ( Apache Spark ) to run). Note Use spark-submit --deploy-mode , spark.submit.deployMode or DEPLOY_MODE environment variable to specify the deploy mode of a Spark application. Volumes \u00b6 Volumes and volume mounts are configured using spark.kubernetes.[type].volumes. -prefixed configuration properties with type being driver or executor (for the driver and executor pods, respectively). KubernetesVolumeUtils utility is used to extract volume configuration based on the volume type: Volume Type Configuration Property emptyDir [volumesPrefix].[volumeType].[volumeName].options.medium [volumesPrefix].[volumeType].[volumeName].options.sizeLimit hostPath [volumesPrefix].[volumeType].[volumeName].options.path persistentVolumeClaim [volumesPrefix].[volumeType].[volumeName].options.claimName Executor volumes ( spark.kubernetes.executor.volumes. -prefixed configuration properties) are parsed right when KubernetesConf utility is used for a KubernetesDriverConf (and a driver pod created). That makes executor volumes required when driver volumes are defined. Static File Resources \u00b6 File resources are resources with file or no URI scheme (that are then considered file -based indirectly). In Spark applications, file resources can be the primary resource (application jar, Python or R files) as well as files referenced by spark.jars and spark.files configuration properties (or their --jars and --files options of spark-submit , respectively). When deployed in cluster mode, Spark on Kubernetes uploads file resources of a Spark application to a Hadoop DFS-compatible file system defined by the required spark.kubernetes.file.upload.path configuration property. Local URI Scheme \u00b6 A special case of static file resources are local resources that are resources with local URI scheme. They are considered already available on every Spark node (and are not added to a Spark file server for distribution when SparkContext is requested to add such file ). In Spark on Kubernetes, local resources are used for primary application resource that are already included in a container image. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Executor Pods State Synchronization \u00b6 Spark on Kubernetes uses ExecutorPodsPollingSnapshotSource for polling Kubernetes API server for executor pods of a Spark application every polling interval (based on spark.kubernetes.executor.apiPollingInterval configuration property). ExecutorPodsPollingSnapshotSource is given an ExecutorPodsSnapshotsStore that is requested to replace a snapshot regularly. ExecutorPodsSnapshotsStore keeps track of executor pods state snapshots and allows subscribers to be regularly updated (e.g. ExecutorPodsAllocator and ExecutorPodsLifecycleManager ). Dynamic Allocation of Executors \u00b6 Spark on Kubernetes supports Dynamic Allocation of Executors using ExecutorPodsAllocator . The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark . Internal Resource Marker \u00b6 Spark on Kubernetes uses spark-internal special name in cluster deploy mode for internal application resources (that are supposed to be part of an image). Given renameMainAppResource , DriverCommandFeatureStep will re-write local file -scheme-based primary application resources to spark-internal special name when requested for the base driver container (for a JavaMainAppResource application). Demo \u00b6 This demo is a follow-up to Demo: Running Spark Application on minikube . Run it first. Note --deploy-mode cluster and the application jar is \"locally resolvable\" (i.e. uses file: scheme indirectly). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --verbose \\ ~/dev/meetups/spark-meetup/spark-docker-example/target/scala-2.12/spark-docker-example_2.12-0.1.0.jar $ kubectl get po -l spark-role=driver NAME READY STATUS RESTARTS AGE spark-docker-example-dfd7d076e7099718-driver 0/1 Error 0 7m25s Note spark-internal in the below output. $ kubectl describe po spark-docker-example-dfd7d076e7099718-driver ... Containers: spark-kubernetes-driver: ... Args: driver --properties-file /opt/spark/conf/spark.properties --class meetup.SparkApp spark-internal ... Resources \u00b6 Official documentation Spark on Kubernetes by Scott Haines (video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert","title":"Spark on Kubernetes"},{"location":"overview/#spark-on-kubernetes","text":"Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Apache Spark supports Kubernetes resource manager using KubernetesClusterManager (and KubernetesClusterSchedulerBackend ) with k8s:// -prefixed master URLs (that point at Kubernetes API servers ). Spark on Kubernetes uses TaskSchedulerImpl ( Apache Spark ) for task scheduling.","title":"Spark on Kubernetes"},{"location":"overview/#kubernetes-ga-in-spark-311","text":"As per SPARK-33005 Kubernetes GA Preparation , Spark 3.1.1 comes with many improvements for Kubernetes support and is expected to get General Availability (GA) marker \ud83c\udf89 Note There will never be 3.1.0 .","title":"Kubernetes GA in Spark 3.1.1"},{"location":"overview/#inactive-executor-pods","text":"Spark on Kubernetes defines spark-exec-inactive label to mark executor pods as inactive after they have finished (successfully or not) but spark.kubernetes.executor.deleteOnTermination configuration property is false (when ExecutorPodsLifecycleManager is requested to handle executor pods snapshots ). This label is used to skip executor pods when PollRunnable is requested to fetch status of all executor pods in a Spark application from Kubernetes API server .","title":" Inactive Executor Pods"},{"location":"overview/#cluster-deploy-mode","text":"Spark on Kubernetes uses KubernetesClientApplication in cluster deploy mode (as the SparkApplication ( Apache Spark ) to run). Note Use spark-submit --deploy-mode , spark.submit.deployMode or DEPLOY_MODE environment variable to specify the deploy mode of a Spark application.","title":"Cluster Deploy Mode"},{"location":"overview/#volumes","text":"Volumes and volume mounts are configured using spark.kubernetes.[type].volumes. -prefixed configuration properties with type being driver or executor (for the driver and executor pods, respectively). KubernetesVolumeUtils utility is used to extract volume configuration based on the volume type: Volume Type Configuration Property emptyDir [volumesPrefix].[volumeType].[volumeName].options.medium [volumesPrefix].[volumeType].[volumeName].options.sizeLimit hostPath [volumesPrefix].[volumeType].[volumeName].options.path persistentVolumeClaim [volumesPrefix].[volumeType].[volumeName].options.claimName Executor volumes ( spark.kubernetes.executor.volumes. -prefixed configuration properties) are parsed right when KubernetesConf utility is used for a KubernetesDriverConf (and a driver pod created). That makes executor volumes required when driver volumes are defined.","title":"Volumes"},{"location":"overview/#static-file-resources","text":"File resources are resources with file or no URI scheme (that are then considered file -based indirectly). In Spark applications, file resources can be the primary resource (application jar, Python or R files) as well as files referenced by spark.jars and spark.files configuration properties (or their --jars and --files options of spark-submit , respectively). When deployed in cluster mode, Spark on Kubernetes uploads file resources of a Spark application to a Hadoop DFS-compatible file system defined by the required spark.kubernetes.file.upload.path configuration property.","title":"Static File Resources"},{"location":"overview/#local-uri-scheme","text":"A special case of static file resources are local resources that are resources with local URI scheme. They are considered already available on every Spark node (and are not added to a Spark file server for distribution when SparkContext is requested to add such file ). In Spark on Kubernetes, local resources are used for primary application resource that are already included in a container image. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar","title":"Local URI Scheme"},{"location":"overview/#executor-pods-state-synchronization","text":"Spark on Kubernetes uses ExecutorPodsPollingSnapshotSource for polling Kubernetes API server for executor pods of a Spark application every polling interval (based on spark.kubernetes.executor.apiPollingInterval configuration property). ExecutorPodsPollingSnapshotSource is given an ExecutorPodsSnapshotsStore that is requested to replace a snapshot regularly. ExecutorPodsSnapshotsStore keeps track of executor pods state snapshots and allows subscribers to be regularly updated (e.g. ExecutorPodsAllocator and ExecutorPodsLifecycleManager ).","title":"Executor Pods State Synchronization"},{"location":"overview/#dynamic-allocation-of-executors","text":"Spark on Kubernetes supports Dynamic Allocation of Executors using ExecutorPodsAllocator . The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark .","title":"Dynamic Allocation of Executors"},{"location":"overview/#internal-resource-marker","text":"Spark on Kubernetes uses spark-internal special name in cluster deploy mode for internal application resources (that are supposed to be part of an image). Given renameMainAppResource , DriverCommandFeatureStep will re-write local file -scheme-based primary application resources to spark-internal special name when requested for the base driver container (for a JavaMainAppResource application).","title":" Internal Resource Marker"},{"location":"overview/#demo","text":"This demo is a follow-up to Demo: Running Spark Application on minikube . Run it first. Note --deploy-mode cluster and the application jar is \"locally resolvable\" (i.e. uses file: scheme indirectly). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --verbose \\ ~/dev/meetups/spark-meetup/spark-docker-example/target/scala-2.12/spark-docker-example_2.12-0.1.0.jar $ kubectl get po -l spark-role=driver NAME READY STATUS RESTARTS AGE spark-docker-example-dfd7d076e7099718-driver 0/1 Error 0 7m25s Note spark-internal in the below output. $ kubectl describe po spark-docker-example-dfd7d076e7099718-driver ... Containers: spark-kubernetes-driver: ... Args: driver --properties-file /opt/spark/conf/spark.properties --class meetup.SparkApp spark-internal ...","title":" Demo"},{"location":"overview/#resources","text":"Official documentation Spark on Kubernetes by Scott Haines (video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert","title":"Resources"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: spark-shell on minikube Running Spark Application on minikube Spark and Local Filesystem in minikube Running Spark Examples on minikube Running Spark Examples on Google Kubernetes Engine Deploying Spark Application to Google Kubernetes Engine Running Spark Structured Streaming on minikube","title":"Welcome"},{"location":"demo/#demos","text":"The following demos are available: spark-shell on minikube Running Spark Application on minikube Spark and Local Filesystem in minikube Running Spark Examples on minikube Running Spark Examples on Google Kubernetes Engine Deploying Spark Application to Google Kubernetes Engine Running Spark Structured Streaming on minikube","title":"Demos"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/","text":"Demo: Deploying Spark Application to Google Kubernetes Engine \u00b6 This demo shows the steps to deploy a Spark application to a Google Kubernetes Engine (GKE) cluster. Before you begin \u00b6 Make sure to review the other demos (esp. Demo: Running Spark Examples on Google Kubernetes Engine ) to get some experience with Spark on Kubernetes and Google Kubernetes Engine. Build Spark Application Image \u00b6 sbt clean docker:publishLocal List the images using docker images . $ docker images \\ --filter=reference='$GCP_CR/*:*' \\ --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example 0.1.0 eu.gcr.io/spark-on-kubernetes-2021/spark v3.0.1 Pushing Image to Container Registry \u00b6 Upload the image to a registry so that your GKE cluster can download and run the container image (as described in Pushing the Docker image to Container Registry ). gcloud auth configure-docker $ sbt docker:publish ... [info] Built image eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example with tags [0.1.0] [info] The push refers to repository [eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example] ... [info] Published image eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example:0.1.0 View the images in the repository. $ gcloud container images list --repository $GCP_CR NAME eu.gcr.io/spark-on-kubernetes-2021/spark eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example Creating Google Kubernetes Engine Cluster \u00b6 Create a GKE cluster to run the Spark application. export CLUSTER_NAME=spark-demo-cluster gcloud container clusters create $CLUSTER_NAME \\ --cluster-version=1.17.15-gke.800 \\ --machine-type=c2-standard-4 Deploying Spark Application to GKE \u00b6 Let's deploy the Docker image of the Spark application to the GKE cluster. Important Create the required Kubernetes resources to run Spark applications as described in Demo: Running Spark Examples on Google Kubernetes Engine cd $SPARK_HOME export K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') export DEMO_POD_NAME=spark-demo-gke export CONTAINER_IMAGE=$GCP_CR/spark-docker-example:0.1.0 ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $DEMO_POD_NAME \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=$CONTAINER_IMAGE \\ --conf spark.kubernetes.driver.pod.name=$DEMO_POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Watch the pods in another terminal. k get po -w Accessing Logs \u00b6 Access the logs of the driver. k logs -f $DEMO_POD_NAME Cleaning up \u00b6 Delete the GKE cluster. gcloud container clusters delete $CLUSTER_NAME --quiet Delete the images. gcloud container images delete $GCP_CR/spark:v3.0.1 --force-delete-tags --quiet gcloud container images delete $GCP_CR/spark-docker-example:0.1.0 --force-delete-tags --quiet","title":"Deploying Spark Application to Google Kubernetes Engine"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#demo-deploying-spark-application-to-google-kubernetes-engine","text":"This demo shows the steps to deploy a Spark application to a Google Kubernetes Engine (GKE) cluster.","title":"Demo: Deploying Spark Application to Google Kubernetes Engine"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#before-you-begin","text":"Make sure to review the other demos (esp. Demo: Running Spark Examples on Google Kubernetes Engine ) to get some experience with Spark on Kubernetes and Google Kubernetes Engine.","title":"Before you begin"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#build-spark-application-image","text":"sbt clean docker:publishLocal List the images using docker images . $ docker images \\ --filter=reference='$GCP_CR/*:*' \\ --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example 0.1.0 eu.gcr.io/spark-on-kubernetes-2021/spark v3.0.1","title":"Build Spark Application Image"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#pushing-image-to-container-registry","text":"Upload the image to a registry so that your GKE cluster can download and run the container image (as described in Pushing the Docker image to Container Registry ). gcloud auth configure-docker $ sbt docker:publish ... [info] Built image eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example with tags [0.1.0] [info] The push refers to repository [eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example] ... [info] Published image eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example:0.1.0 View the images in the repository. $ gcloud container images list --repository $GCP_CR NAME eu.gcr.io/spark-on-kubernetes-2021/spark eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example","title":"Pushing Image to Container Registry"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#creating-google-kubernetes-engine-cluster","text":"Create a GKE cluster to run the Spark application. export CLUSTER_NAME=spark-demo-cluster gcloud container clusters create $CLUSTER_NAME \\ --cluster-version=1.17.15-gke.800 \\ --machine-type=c2-standard-4","title":"Creating Google Kubernetes Engine Cluster"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#deploying-spark-application-to-gke","text":"Let's deploy the Docker image of the Spark application to the GKE cluster. Important Create the required Kubernetes resources to run Spark applications as described in Demo: Running Spark Examples on Google Kubernetes Engine cd $SPARK_HOME export K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') export DEMO_POD_NAME=spark-demo-gke export CONTAINER_IMAGE=$GCP_CR/spark-docker-example:0.1.0 ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $DEMO_POD_NAME \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=$CONTAINER_IMAGE \\ --conf spark.kubernetes.driver.pod.name=$DEMO_POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Watch the pods in another terminal. k get po -w","title":"Deploying Spark Application to GKE"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#accessing-logs","text":"Access the logs of the driver. k logs -f $DEMO_POD_NAME","title":"Accessing Logs"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#cleaning-up","text":"Delete the GKE cluster. gcloud container clusters delete $CLUSTER_NAME --quiet Delete the images. gcloud container images delete $GCP_CR/spark:v3.0.1 --force-delete-tags --quiet gcloud container images delete $GCP_CR/spark-docker-example:0.1.0 --force-delete-tags --quiet","title":"Cleaning up"},{"location":"demo/running-spark-application-on-minikube/","text":"Demo: Running Spark Application on minikube \u00b6 This demo shows how to deploy a Spark application to Kubernetes (using minikube ). Tip Start with Demo: spark-shell on minikube . Start Cluster \u00b6 Unless already started, start minikube. minikube start Build Spark Application Image \u00b6 Make sure you've got a Spark image available in minikube's Docker registry. Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List the Spark image. docker images spark REPOSITORY TAG IMAGE ID CREATED SIZE spark v3.1.1-rc1 e64950545e8f About an hour ago 509MB Use this image in your Spark application: FROM spark:v3.1.1-rc1 In your Spark application project execute the command to build and push a Docker image to minikube's Docker repository. sbt clean meetup-spark-app/docker:publishLocal List the images and make sure that the image of your Spark application project is available. docker images 'meetup*' REPOSITORY TAG IMAGE ID CREATED SIZE meetup-spark-app 0.1.0 5b9b7132a36b 44 seconds ago 520MB meetup-app-deps 0.1.0 ace517e0dd60 2 minutes ago 515MB docker image inspect \u00b6 Use docker image inspect command to display detailed information on the Spark application image. docker image inspect meetup-spark-app:0.1.0 docker image history \u00b6 Use docker image history command to show the history of the Spark application image. docker image history meetup-spark-app:0.1.0 Create Kubernetes Resources \u00b6 Create required Kubernetes resources to run a Spark application. Spark official documentation Learn more from the Spark official documentation . A namespace is optional, but the service account and the cluster role binding with proper permissions would lead to the following exception message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. Declaratively \u00b6 Use the following rbac.yml file. apiVersion: v1 kind: Namespace metadata: name: spark-demo --- apiVersion: v1 kind: ServiceAccount metadata: name: spark namespace: spark-demo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-role namespace: spark-demo subjects: - kind: ServiceAccount name: spark namespace: spark-demo roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io --- Create the resources in the Kubernetes cluster. k create -f rbac.yml Tip With declarative approach (using rbac.yml ) cleaning up becomes as simple as k delete -f rbac.yml . Imperatively \u00b6 k create ns spark-demo k create serviceaccount spark -n spark-demo k create clusterrolebinding spark-role \\ --clusterrole edit \\ --serviceaccount spark-demo:spark \\ -n spark-demo Submit Spark Application to minikube \u00b6 cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') export POD_NAME=meetup-spark-app export IMAGE_NAME=$POD_NAME:0.1.0 Please note the configuration properties (some not really necessary but make the demo easier to guide you through, e.g. spark.kubernetes.driver.pod.name ). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=$IMAGE_NAME \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/jars/meetup.meetup-spark-app-0.1.0.jar If all went fine you should soon see termination reason: Completed message. 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Succeeded) 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Container final statuses: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed Accessing web UI \u00b6 k port-forward $POD_NAME 4040:4040 Accessing Logs \u00b6 Access the logs of the driver. k logs -f $POD_NAME Reviewing Spark Application Configuration (ConfigMap) \u00b6 k get cm k describe cm [driverPod]-conf-map Describe the driver pod and review volumes ( .spec.volumes ) and volume mounts ( .spec.containers[].volumeMounts ). k describe po $POD_NAME $ k get po $POD_NAME -o=jsonpath='{.spec.volumes}' | jq [ { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-f76bf776ec818be5-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-24krm\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-24krm\" } } ] $ k get po $POD_NAME -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/var/data/spark-b5d0a070-ff9a-41a3-91aa-82059ceba5b0\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-24krm\", \"readOnly\": true } ] Spark Application Management \u00b6 K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') $ ./bin/spark-submit --status \"spark-demo:spark-docker-example-*\" --master k8s://$K8S_SERVER ... Application status (driver): pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed Listing Services \u00b6 $ k get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE spark-docker-example-3de43976e3a46fcf-driver-svc ClusterIP None <none> 7078/TCP,7079/TCP,4040/TCP 101s Stopping Cluster \u00b6 minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all","title":"Running Spark Application on minikube"},{"location":"demo/running-spark-application-on-minikube/#demo-running-spark-application-on-minikube","text":"This demo shows how to deploy a Spark application to Kubernetes (using minikube ). Tip Start with Demo: spark-shell on minikube .","title":"Demo: Running Spark Application on minikube"},{"location":"demo/running-spark-application-on-minikube/#start-cluster","text":"Unless already started, start minikube. minikube start","title":"Start Cluster"},{"location":"demo/running-spark-application-on-minikube/#build-spark-application-image","text":"Make sure you've got a Spark image available in minikube's Docker registry. Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List the Spark image. docker images spark REPOSITORY TAG IMAGE ID CREATED SIZE spark v3.1.1-rc1 e64950545e8f About an hour ago 509MB Use this image in your Spark application: FROM spark:v3.1.1-rc1 In your Spark application project execute the command to build and push a Docker image to minikube's Docker repository. sbt clean meetup-spark-app/docker:publishLocal List the images and make sure that the image of your Spark application project is available. docker images 'meetup*' REPOSITORY TAG IMAGE ID CREATED SIZE meetup-spark-app 0.1.0 5b9b7132a36b 44 seconds ago 520MB meetup-app-deps 0.1.0 ace517e0dd60 2 minutes ago 515MB","title":"Build Spark Application Image"},{"location":"demo/running-spark-application-on-minikube/#docker-image-inspect","text":"Use docker image inspect command to display detailed information on the Spark application image. docker image inspect meetup-spark-app:0.1.0","title":"docker image inspect"},{"location":"demo/running-spark-application-on-minikube/#docker-image-history","text":"Use docker image history command to show the history of the Spark application image. docker image history meetup-spark-app:0.1.0","title":"docker image history"},{"location":"demo/running-spark-application-on-minikube/#create-kubernetes-resources","text":"Create required Kubernetes resources to run a Spark application. Spark official documentation Learn more from the Spark official documentation . A namespace is optional, but the service account and the cluster role binding with proper permissions would lead to the following exception message: Forbidden!Configured service account doesn't have access. Service account may have been revoked.","title":"Create Kubernetes Resources"},{"location":"demo/running-spark-application-on-minikube/#declaratively","text":"Use the following rbac.yml file. apiVersion: v1 kind: Namespace metadata: name: spark-demo --- apiVersion: v1 kind: ServiceAccount metadata: name: spark namespace: spark-demo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-role namespace: spark-demo subjects: - kind: ServiceAccount name: spark namespace: spark-demo roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io --- Create the resources in the Kubernetes cluster. k create -f rbac.yml Tip With declarative approach (using rbac.yml ) cleaning up becomes as simple as k delete -f rbac.yml .","title":"Declaratively"},{"location":"demo/running-spark-application-on-minikube/#imperatively","text":"k create ns spark-demo k create serviceaccount spark -n spark-demo k create clusterrolebinding spark-role \\ --clusterrole edit \\ --serviceaccount spark-demo:spark \\ -n spark-demo","title":"Imperatively"},{"location":"demo/running-spark-application-on-minikube/#submit-spark-application-to-minikube","text":"cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') export POD_NAME=meetup-spark-app export IMAGE_NAME=$POD_NAME:0.1.0 Please note the configuration properties (some not really necessary but make the demo easier to guide you through, e.g. spark.kubernetes.driver.pod.name ). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=$IMAGE_NAME \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/jars/meetup.meetup-spark-app-0.1.0.jar If all went fine you should soon see termination reason: Completed message. 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Succeeded) 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Container final statuses: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed","title":"Submit Spark Application to minikube"},{"location":"demo/running-spark-application-on-minikube/#accessing-web-ui","text":"k port-forward $POD_NAME 4040:4040","title":"Accessing web UI"},{"location":"demo/running-spark-application-on-minikube/#accessing-logs","text":"Access the logs of the driver. k logs -f $POD_NAME","title":"Accessing Logs"},{"location":"demo/running-spark-application-on-minikube/#reviewing-spark-application-configuration-configmap","text":"k get cm k describe cm [driverPod]-conf-map Describe the driver pod and review volumes ( .spec.volumes ) and volume mounts ( .spec.containers[].volumeMounts ). k describe po $POD_NAME $ k get po $POD_NAME -o=jsonpath='{.spec.volumes}' | jq [ { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-f76bf776ec818be5-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-24krm\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-24krm\" } } ] $ k get po $POD_NAME -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/var/data/spark-b5d0a070-ff9a-41a3-91aa-82059ceba5b0\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-24krm\", \"readOnly\": true } ]","title":"Reviewing Spark Application Configuration (ConfigMap)"},{"location":"demo/running-spark-application-on-minikube/#spark-application-management","text":"K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') $ ./bin/spark-submit --status \"spark-demo:spark-docker-example-*\" --master k8s://$K8S_SERVER ... Application status (driver): pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed","title":"Spark Application Management"},{"location":"demo/running-spark-application-on-minikube/#listing-services","text":"$ k get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE spark-docker-example-3de43976e3a46fcf-driver-svc ClusterIP None <none> 7078/TCP,7079/TCP,4040/TCP 101s","title":"Listing Services"},{"location":"demo/running-spark-application-on-minikube/#stopping-cluster","text":"minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all","title":"Stopping Cluster"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/","text":"Demo: Running Spark Examples on Google Kubernetes Engine \u00b6 This demo shows how to run the official Spark Examples on a Kubernetes cluster on Google Kubernetes Engine (GKE) . This demo focuses on the ubiquitous SparkPi example, but should let you run the other sample Spark applications too. ./bin/run-example SparkPi 10 Before you begin \u00b6 Make sure to enable the Kubernetes Engine API (as described in Deploying a containerized web application ). Review Demo: Running Spark Examples on minikube to build a basic understanding of the process of deploying Spark applications to a local Kubernetes cluster using minikube. Building Spark Container Image \u00b6 export PROJECT_ID=$(gcloud info --format='value(config.project)') export GCP_CR=eu.gcr.io/${PROJECT_ID} Build and push a Apache Spark base image to Container Registry on Google Cloud Platform. cd $SPARK_HOME ./bin/docker-image-tool.sh \\ -r $GCP_CR \\ -t v3.1.1-rc1 \\ build List the images using docker images command (and some other fancy options). docker images \"$GCP_CR/*\" \\ --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG eu.gcr.io/spark-on-kubernetes-2021/spark v3.1.1-rc1 Push Spark Image to Container Registry \u00b6 Push the container image to the Container Registry so that a GKE cluster can run it in a pod (as described in Pushing the Docker image to Container Registry ). gcloud auth configure-docker ./bin/docker-image-tool.sh \\ -r $GCP_CR \\ -t v3.1.1-rc1 \\ push List Images \u00b6 Use gcloud container images list to list the Spark image in the repository. gcloud container images list --repository $GCP_CR NAME eu.gcr.io/spark-on-kubernetes-2021/spark List Tags \u00b6 Use gcloud container images list-tags to list tags and digests for the specified image. gcloud container images list-tags $GCP_CR/spark DIGEST TAGS TIMESTAMP 9a50d1435bbe v3.1.1-rc1 2021-01-26T13:02:11 Describe Spark Image \u00b6 Use gcloud container images describe to list information about the Spark image. gcloud container images describe $GCP_CR/spark:v3.1.1-rc1 image_summary: digest: sha256:9a50d1435bbe81dd3a23d3e43c244a0bfc37e14fb3754b68431cbf8510360b84 fully_qualified_digest: eu.gcr.io/spark-on-kubernetes-2021/spark@sha256:9a50d1435bbe81dd3a23d3e43c244a0bfc37e14fb3754b68431cbf8510360b84 registry: eu.gcr.io repository: spark-on-kubernetes-2021/spark Create Kubernetes Cluster \u00b6 export CLUSTER_NAME=spark-examples-cluster The default version of Kubernetes varies per Google Cloud zone and is often older than the latest stable release. A cluster version can be changed using --cluster-version option. Use gcloud container get-server-config command to check which Kubernetes versions are available and default in your zone. gcloud container get-server-config Tip Use latest version alias to use the highest supported Kubernetes version currently available on GKE in the cluster's zone or region. gcloud container clusters create $CLUSTER_NAME \\ --cluster-version=latest Wait a few minutes before the GKE cluster is ready. In the end, you should see a summary of the cluster. kubeconfig entry generated for spark-examples-cluster. NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS spark-examples-cluster europe-west3-b 1.18.14-gke.1200 35.198.74.105 e2-medium 1.18.14-gke.1200 3 RUNNING Review the configuration of the GKE cluster. k config view Review the cluster's VM instances. gcloud compute instances list Running SparkPi on GKE \u00b6 Note What follows is a more succinct version of Demo: Running Spark Application on minikube . Create Kubernetes Resources \u00b6 Use the following yaml configuration file ( rbac.yml ) to create required resources. apiVersion: v1 kind: Namespace metadata: name: spark-demo --- apiVersion: v1 kind: ServiceAccount metadata: name: spark namespace: spark-demo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-role namespace: spark-demo subjects: - kind: ServiceAccount name: spark namespace: spark-demo roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io --- Use k create to create the Kubernetes resources. k create -f rbac.yml Run SparkPi \u00b6 cd $SPARK_HOME export K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') export POD_NAME=spark-examples-pod export SPARK_IMAGE=$GCP_CR/spark:v3.1.1-rc1 Important For the time being we're going to use spark-submit not run-example . See Demo: Running Spark Examples on minikube for more information. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.driver.request.cores=400m \\ --conf spark.kubernetes.executor.request.cores=100m \\ --conf spark.kubernetes.container.image=$SPARK_IMAGE \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 Note spark.kubernetes.*.request.cores configuration properties were required due to the default machine type of a GKE cluster is too small CPU-wise. You may consider another machine type for a GKE cluster (e.g. c2-standard-4 ). Open another terminal and watch the pods being created and terminated. Don't forget about the spark-demo namespace. k get po -n spark-demo -w In the end, review the logs. k logs -n spark-demo $POD_NAME Cleaning up \u00b6 Delete the GKE cluster. gcloud container clusters delete $CLUSTER_NAME --quiet Delete the images. gcloud container images delete $SPARK_IMAGE --force-delete-tags --quiet","title":"Running Spark Examples on Google Kubernetes Engine"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#demo-running-spark-examples-on-google-kubernetes-engine","text":"This demo shows how to run the official Spark Examples on a Kubernetes cluster on Google Kubernetes Engine (GKE) . This demo focuses on the ubiquitous SparkPi example, but should let you run the other sample Spark applications too. ./bin/run-example SparkPi 10","title":"Demo: Running Spark Examples on Google Kubernetes Engine"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#before-you-begin","text":"Make sure to enable the Kubernetes Engine API (as described in Deploying a containerized web application ). Review Demo: Running Spark Examples on minikube to build a basic understanding of the process of deploying Spark applications to a local Kubernetes cluster using minikube.","title":"Before you begin"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#building-spark-container-image","text":"export PROJECT_ID=$(gcloud info --format='value(config.project)') export GCP_CR=eu.gcr.io/${PROJECT_ID} Build and push a Apache Spark base image to Container Registry on Google Cloud Platform. cd $SPARK_HOME ./bin/docker-image-tool.sh \\ -r $GCP_CR \\ -t v3.1.1-rc1 \\ build List the images using docker images command (and some other fancy options). docker images \"$GCP_CR/*\" \\ --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG eu.gcr.io/spark-on-kubernetes-2021/spark v3.1.1-rc1","title":"Building Spark Container Image"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#push-spark-image-to-container-registry","text":"Push the container image to the Container Registry so that a GKE cluster can run it in a pod (as described in Pushing the Docker image to Container Registry ). gcloud auth configure-docker ./bin/docker-image-tool.sh \\ -r $GCP_CR \\ -t v3.1.1-rc1 \\ push","title":"Push Spark Image to Container Registry"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#list-images","text":"Use gcloud container images list to list the Spark image in the repository. gcloud container images list --repository $GCP_CR NAME eu.gcr.io/spark-on-kubernetes-2021/spark","title":"List Images"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#list-tags","text":"Use gcloud container images list-tags to list tags and digests for the specified image. gcloud container images list-tags $GCP_CR/spark DIGEST TAGS TIMESTAMP 9a50d1435bbe v3.1.1-rc1 2021-01-26T13:02:11","title":"List Tags"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#describe-spark-image","text":"Use gcloud container images describe to list information about the Spark image. gcloud container images describe $GCP_CR/spark:v3.1.1-rc1 image_summary: digest: sha256:9a50d1435bbe81dd3a23d3e43c244a0bfc37e14fb3754b68431cbf8510360b84 fully_qualified_digest: eu.gcr.io/spark-on-kubernetes-2021/spark@sha256:9a50d1435bbe81dd3a23d3e43c244a0bfc37e14fb3754b68431cbf8510360b84 registry: eu.gcr.io repository: spark-on-kubernetes-2021/spark","title":"Describe Spark Image"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#create-kubernetes-cluster","text":"export CLUSTER_NAME=spark-examples-cluster The default version of Kubernetes varies per Google Cloud zone and is often older than the latest stable release. A cluster version can be changed using --cluster-version option. Use gcloud container get-server-config command to check which Kubernetes versions are available and default in your zone. gcloud container get-server-config Tip Use latest version alias to use the highest supported Kubernetes version currently available on GKE in the cluster's zone or region. gcloud container clusters create $CLUSTER_NAME \\ --cluster-version=latest Wait a few minutes before the GKE cluster is ready. In the end, you should see a summary of the cluster. kubeconfig entry generated for spark-examples-cluster. NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS spark-examples-cluster europe-west3-b 1.18.14-gke.1200 35.198.74.105 e2-medium 1.18.14-gke.1200 3 RUNNING Review the configuration of the GKE cluster. k config view Review the cluster's VM instances. gcloud compute instances list","title":"Create Kubernetes Cluster"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#running-sparkpi-on-gke","text":"Note What follows is a more succinct version of Demo: Running Spark Application on minikube .","title":"Running SparkPi on GKE"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#create-kubernetes-resources","text":"Use the following yaml configuration file ( rbac.yml ) to create required resources. apiVersion: v1 kind: Namespace metadata: name: spark-demo --- apiVersion: v1 kind: ServiceAccount metadata: name: spark namespace: spark-demo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-role namespace: spark-demo subjects: - kind: ServiceAccount name: spark namespace: spark-demo roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io --- Use k create to create the Kubernetes resources. k create -f rbac.yml","title":"Create Kubernetes Resources"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#run-sparkpi","text":"cd $SPARK_HOME export K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') export POD_NAME=spark-examples-pod export SPARK_IMAGE=$GCP_CR/spark:v3.1.1-rc1 Important For the time being we're going to use spark-submit not run-example . See Demo: Running Spark Examples on minikube for more information. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.driver.request.cores=400m \\ --conf spark.kubernetes.executor.request.cores=100m \\ --conf spark.kubernetes.container.image=$SPARK_IMAGE \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 Note spark.kubernetes.*.request.cores configuration properties were required due to the default machine type of a GKE cluster is too small CPU-wise. You may consider another machine type for a GKE cluster (e.g. c2-standard-4 ). Open another terminal and watch the pods being created and terminated. Don't forget about the spark-demo namespace. k get po -n spark-demo -w In the end, review the logs. k logs -n spark-demo $POD_NAME","title":"Run SparkPi"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#cleaning-up","text":"Delete the GKE cluster. gcloud container clusters delete $CLUSTER_NAME --quiet Delete the images. gcloud container images delete $SPARK_IMAGE --force-delete-tags --quiet","title":"Cleaning up"},{"location":"demo/running-spark-examples-on-minikube/","text":"Demo: Running Spark Examples on minikube \u00b6 This demo shows how to run the Spark example applications on minikube to advance from spark-shell on minikube to a more serious deployment (yet with no Spark development). This demo lets you explore deploying a Spark application (e.g. SparkPi ) to Kubernetes in cluster deploy mode. Before you begin \u00b6 Start up minikube with necessary Kubernetes resources. Follow the steps in Demo: spark-shell on minikube and Demo: Running Spark Application on minikube : Start minikube Build Spark Image Create Kubernetes Resources Running SparkPi on minikube \u00b6 cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') Let's use an environment variable for the name of the pod to be more \"stable\" and predictable. It should make viewing logs and restarting Spark examples easier. Just change the environment variable or delete the pod and off you go! export POD_NAME=a1 WIP: No use of run-example without spark.kubernetes.file.upload.path Using the run-example shell script to run the Spark examples will not work unless you define spark.kubernetes.file.upload.path configuration property. The reason is that run-example uses the spark-examples jar file that is found locally so spark-submit (that is used under the covers) has to upload the locally-available resource file to be available in a cluster. We'll get to it later. Consider it a work in progress. ./bin/run-example \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --jars local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ SparkPi 10 ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 In another terminal, use k get po -w to watch the pods of the driver and executors. k get po -w Review the logs of the driver (as long as the driver pod is up and running). k logs $POD_NAME For repeatable SparkPi executions, delete the driver pod using k delete pod command. k delete po $POD_NAME spark.kubernetes.executor.deleteOnTermination Configuration Property Use spark.kubernetes.executor.deleteOnTermination configuration property to keep executor pods available once a Spark application is finished (e.g. for examination). Using Pod Templates \u00b6 spark.kubernetes.driver.podTemplateFile configuration property allows to define a template file for driver pods (e.g. for multi-container pods). spark.kubernetes.executor.podTemplateFile Configuration Property Use spark.kubernetes.executor.podTemplateFile configuration property for the template file of executor pods. Pod Template \u00b6 The following is a very basic pod template file. It is incorrect Spark-wise though (as it does not really allow submitting Spark applications) but does allow playing with pod templates. spec: containers: - name: spark image: busybox command: ['sh', '-c', 'echo \"Hello, Spark on Kubernetes!\"'] ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --conf spark.kubernetes.driver.podTemplateFile=pod-template.yml \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 k logs $POD_NAME Hello, Spark on Kubernetes! Container Resources \u00b6 In cluster deploy mode, Spark on Kubernetes may use extra Non-Heap Memory Overhead in memory requirements of the driver pod (based on spark.kubernetes.memoryOverheadFactor configuration property). k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq $ k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq { \"limits\": { \"memory\": \"1408Mi\" }, \"requests\": { \"cpu\": \"1\", \"memory\": \"1408Mi\" } } Note that this extra memory requirements could be part of a pod template. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --driver-memory 1g \\ --conf spark.kubernetes.memoryOverheadFactor=0.5 \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq $ k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq { \"limits\": { \"memory\": \"1536Mi\" }, \"requests\": { \"cpu\": \"1\", \"memory\": \"1536Mi\" } }","title":"Running Spark Examples on minikube"},{"location":"demo/running-spark-examples-on-minikube/#demo-running-spark-examples-on-minikube","text":"This demo shows how to run the Spark example applications on minikube to advance from spark-shell on minikube to a more serious deployment (yet with no Spark development). This demo lets you explore deploying a Spark application (e.g. SparkPi ) to Kubernetes in cluster deploy mode.","title":"Demo: Running Spark Examples on minikube"},{"location":"demo/running-spark-examples-on-minikube/#before-you-begin","text":"Start up minikube with necessary Kubernetes resources. Follow the steps in Demo: spark-shell on minikube and Demo: Running Spark Application on minikube : Start minikube Build Spark Image Create Kubernetes Resources","title":"Before you begin"},{"location":"demo/running-spark-examples-on-minikube/#running-sparkpi-on-minikube","text":"cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') Let's use an environment variable for the name of the pod to be more \"stable\" and predictable. It should make viewing logs and restarting Spark examples easier. Just change the environment variable or delete the pod and off you go! export POD_NAME=a1 WIP: No use of run-example without spark.kubernetes.file.upload.path Using the run-example shell script to run the Spark examples will not work unless you define spark.kubernetes.file.upload.path configuration property. The reason is that run-example uses the spark-examples jar file that is found locally so spark-submit (that is used under the covers) has to upload the locally-available resource file to be available in a cluster. We'll get to it later. Consider it a work in progress. ./bin/run-example \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --jars local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ SparkPi 10 ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 In another terminal, use k get po -w to watch the pods of the driver and executors. k get po -w Review the logs of the driver (as long as the driver pod is up and running). k logs $POD_NAME For repeatable SparkPi executions, delete the driver pod using k delete pod command. k delete po $POD_NAME spark.kubernetes.executor.deleteOnTermination Configuration Property Use spark.kubernetes.executor.deleteOnTermination configuration property to keep executor pods available once a Spark application is finished (e.g. for examination).","title":"Running SparkPi on minikube"},{"location":"demo/running-spark-examples-on-minikube/#using-pod-templates","text":"spark.kubernetes.driver.podTemplateFile configuration property allows to define a template file for driver pods (e.g. for multi-container pods). spark.kubernetes.executor.podTemplateFile Configuration Property Use spark.kubernetes.executor.podTemplateFile configuration property for the template file of executor pods.","title":"Using Pod Templates"},{"location":"demo/running-spark-examples-on-minikube/#pod-template","text":"The following is a very basic pod template file. It is incorrect Spark-wise though (as it does not really allow submitting Spark applications) but does allow playing with pod templates. spec: containers: - name: spark image: busybox command: ['sh', '-c', 'echo \"Hello, Spark on Kubernetes!\"'] ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --conf spark.kubernetes.driver.podTemplateFile=pod-template.yml \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 k logs $POD_NAME Hello, Spark on Kubernetes!","title":"Pod Template"},{"location":"demo/running-spark-examples-on-minikube/#container-resources","text":"In cluster deploy mode, Spark on Kubernetes may use extra Non-Heap Memory Overhead in memory requirements of the driver pod (based on spark.kubernetes.memoryOverheadFactor configuration property). k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq $ k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq { \"limits\": { \"memory\": \"1408Mi\" }, \"requests\": { \"cpu\": \"1\", \"memory\": \"1408Mi\" } } Note that this extra memory requirements could be part of a pod template. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --driver-memory 1g \\ --conf spark.kubernetes.memoryOverheadFactor=0.5 \\ --name $POD_NAME \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10 k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq $ k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq { \"limits\": { \"memory\": \"1536Mi\" }, \"requests\": { \"cpu\": \"1\", \"memory\": \"1536Mi\" } }","title":"Container Resources"},{"location":"demo/running-spark-structured-streaming-on-minikube/","text":"Demo: Running Spark Structured Streaming on minikube \u00b6 This demo shows how to run a Spark Structured Streaming application on minikube: spark.kubernetes.submission.waitAppCompletion configuration property spark-submit --status and --kill Before you begin \u00b6 It is assumed that you are familiar with the basics of Spark on Kubernetes and the other demos . Start Cluster \u00b6 Start minikube. minikube start Build Spark Application Image \u00b6 Make sure you've got a Spark image available in minikube's Docker registry. Learn the steps in Demo: spark-shell on minikube . Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List the Spark image. Make sure it matches the version of Spark you want to work with. docker images spark REPOSITORY TAG IMAGE ID CREATED SIZE spark v3.1.1-rc1 e64950545e8f About an hour ago 509MB Publish the image of the Spark Structured Streaming application. It is project-dependent, and the project uses sbt with sbt-native-packager plugin. sbt clean docker:publishLocal List the images and make sure that the image of your Spark application project is available. docker images spark-streams-demo REPOSITORY TAG IMAGE ID CREATED SIZE spark-streams-demo 0.1.0 20145c134ca9 4 minutes ago 515MB Submit Spark Application to minikube \u00b6 cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') Make sure that the Kubernetes resources (e.g. a namespace and a service account) are available in the cluster. Learn more in Demo: Running Spark Application on minikube . k create -f rbac.yml The name of the pod is going to be based on the name of the container image for demo purposes. Pick what works for you. export POD_NAME=spark-streams-demo export IMAGE_NAME=$POD_NAME:0.1.0 You may optionally delete all pods (since we use a fixed name for the demo). k delete po --all One of the differences between streaming and batch Spark applications is that the Spark Structured Streaming application is supposed to never stop. That's why the demo uses spark.kubernetes.submission.waitAppCompletion configuration property. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class meetup.SparkStreamsApp \\ --conf spark.kubernetes.container.image=$IMAGE_NAME \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.submission.waitAppCompletion=false \\ --verbose \\ local:///opt/spark/jars/meetup.spark-streams-demo-0.1.0.jar In the end, you should be given a so-called submission ID that you're going to use with spark-submit tool (via K8SSparkSubmitOperation extension). INFO LoggingPodStatusWatcherImpl: Deployed Spark application spark-streams-demo with submission ID spark-demo:spark-streams-demo into Kubernetes Take a note of it as that is how you are going to monitor the application using Spark's spark-submit --status (and possibly kill it with spark-submit --kill ). export SUBMISSION_ID=spark-demo:spark-streams-demo Once submitted, observe pods in another terminal. Make sure you use spark-demo namespace. k get po -w Request Status of Spark Application \u00b6 Use spark-submit --status SUBMISSION_ID to requests the status of the Spark driver in cluster deploy mode. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --status $SUBMISSION_ID You should see something similar to the following: Submitting a request for the status of submission spark-demo:spark-streams-demo in k8s://https://127.0.0.1:55004. 21/01/18 12:16:27 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file Application status (driver): pod name: spark-streams-demo namespace: spark-demo labels: spark-app-selector -> spark-46ca76cc77c242509f27af3c506eb1f5, spark-role -> driver pod uid: 034ed206-5804-4e9d-ab68-ec56a7678b65 creation time: 2021-01-18T11:09:46Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-888gj node name: minikube start time: 2021-01-18T11:09:46Z phase: Running container status: container name: spark-kubernetes-driver container image: spark-streams-demo:0.1.0 container state: running container started at: 2021-01-18T11:09:47Z Kill Spark Application \u00b6 In the end, you can spark-submit --kill the Spark Structured Streaming application. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --kill $SUBMISSION_ID You should see something similar to the following: Submitting a request to kill submission spark-demo:spark-streams-demo in k8s://https://127.0.0.1:55004. Grace period in secs: not set. Clean Up \u00b6 Clean up the cluster as described in Demo: spark-shell on minikube . That's it. Congratulations!","title":"Running Spark Structured Streaming on minikube"},{"location":"demo/running-spark-structured-streaming-on-minikube/#demo-running-spark-structured-streaming-on-minikube","text":"This demo shows how to run a Spark Structured Streaming application on minikube: spark.kubernetes.submission.waitAppCompletion configuration property spark-submit --status and --kill","title":"Demo: Running Spark Structured Streaming on minikube"},{"location":"demo/running-spark-structured-streaming-on-minikube/#before-you-begin","text":"It is assumed that you are familiar with the basics of Spark on Kubernetes and the other demos .","title":"Before you begin"},{"location":"demo/running-spark-structured-streaming-on-minikube/#start-cluster","text":"Start minikube. minikube start","title":"Start Cluster"},{"location":"demo/running-spark-structured-streaming-on-minikube/#build-spark-application-image","text":"Make sure you've got a Spark image available in minikube's Docker registry. Learn the steps in Demo: spark-shell on minikube . Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List the Spark image. Make sure it matches the version of Spark you want to work with. docker images spark REPOSITORY TAG IMAGE ID CREATED SIZE spark v3.1.1-rc1 e64950545e8f About an hour ago 509MB Publish the image of the Spark Structured Streaming application. It is project-dependent, and the project uses sbt with sbt-native-packager plugin. sbt clean docker:publishLocal List the images and make sure that the image of your Spark application project is available. docker images spark-streams-demo REPOSITORY TAG IMAGE ID CREATED SIZE spark-streams-demo 0.1.0 20145c134ca9 4 minutes ago 515MB","title":"Build Spark Application Image"},{"location":"demo/running-spark-structured-streaming-on-minikube/#submit-spark-application-to-minikube","text":"cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') Make sure that the Kubernetes resources (e.g. a namespace and a service account) are available in the cluster. Learn more in Demo: Running Spark Application on minikube . k create -f rbac.yml The name of the pod is going to be based on the name of the container image for demo purposes. Pick what works for you. export POD_NAME=spark-streams-demo export IMAGE_NAME=$POD_NAME:0.1.0 You may optionally delete all pods (since we use a fixed name for the demo). k delete po --all One of the differences between streaming and batch Spark applications is that the Spark Structured Streaming application is supposed to never stop. That's why the demo uses spark.kubernetes.submission.waitAppCompletion configuration property. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name $POD_NAME \\ --class meetup.SparkStreamsApp \\ --conf spark.kubernetes.container.image=$IMAGE_NAME \\ --conf spark.kubernetes.driver.pod.name=$POD_NAME \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.submission.waitAppCompletion=false \\ --verbose \\ local:///opt/spark/jars/meetup.spark-streams-demo-0.1.0.jar In the end, you should be given a so-called submission ID that you're going to use with spark-submit tool (via K8SSparkSubmitOperation extension). INFO LoggingPodStatusWatcherImpl: Deployed Spark application spark-streams-demo with submission ID spark-demo:spark-streams-demo into Kubernetes Take a note of it as that is how you are going to monitor the application using Spark's spark-submit --status (and possibly kill it with spark-submit --kill ). export SUBMISSION_ID=spark-demo:spark-streams-demo Once submitted, observe pods in another terminal. Make sure you use spark-demo namespace. k get po -w","title":"Submit Spark Application to minikube"},{"location":"demo/running-spark-structured-streaming-on-minikube/#request-status-of-spark-application","text":"Use spark-submit --status SUBMISSION_ID to requests the status of the Spark driver in cluster deploy mode. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --status $SUBMISSION_ID You should see something similar to the following: Submitting a request for the status of submission spark-demo:spark-streams-demo in k8s://https://127.0.0.1:55004. 21/01/18 12:16:27 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file Application status (driver): pod name: spark-streams-demo namespace: spark-demo labels: spark-app-selector -> spark-46ca76cc77c242509f27af3c506eb1f5, spark-role -> driver pod uid: 034ed206-5804-4e9d-ab68-ec56a7678b65 creation time: 2021-01-18T11:09:46Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-888gj node name: minikube start time: 2021-01-18T11:09:46Z phase: Running container status: container name: spark-kubernetes-driver container image: spark-streams-demo:0.1.0 container state: running container started at: 2021-01-18T11:09:47Z","title":"Request Status of Spark Application"},{"location":"demo/running-spark-structured-streaming-on-minikube/#kill-spark-application","text":"In the end, you can spark-submit --kill the Spark Structured Streaming application. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --kill $SUBMISSION_ID You should see something similar to the following: Submitting a request to kill submission spark-demo:spark-streams-demo in k8s://https://127.0.0.1:55004. Grace period in secs: not set.","title":"Kill Spark Application"},{"location":"demo/running-spark-structured-streaming-on-minikube/#clean-up","text":"Clean up the cluster as described in Demo: spark-shell on minikube . That's it. Congratulations!","title":"Clean Up"},{"location":"demo/spark-and-local-filesystem-in-minikube/","text":"Demo: Spark and Local Filesystem in minikube \u00b6 The motivation of the demo is to set up a Spark application deployed to minikube to access files on a local filesystem. Tip Start with Demo: Running Spark Application on minikube . The demo uses spark-submit --files and spark.kubernetes.file.upload.path configuration property to upload a static file to a directory that is then mounted to Spark application pods. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Mounting Filesystems \u00b6 Let's start off by mounting a host directory ( /tmp/spark-k8s ) to minikube. Quoting Mounting filesystems of minikube's official documentation: To mount a directory from the host into the guest using the mount subcommand $ minikube mount /tmp/spark-k8s:/tmp/spark-k8s \ud83d\udcc1 Mounting host path /tmp/spark-k8s into VM as /tmp/spark-k8s ... \u25aa Mount type: \u25aa User ID: docker \u25aa Group ID: docker \u25aa Version: 9p2000.L \u25aa Message Size: 262144 \u25aa Permissions: 755 (-rwxr-xr-x) \u25aa Options: map[] \u25aa Bind Address: 127.0.0.1:53125 \ud83d\ude80 Userspace file server: ufs starting \u2705 Successfully mounted /tmp/spark-k8s to /tmp/spark-k8s \ud83d\udccc NOTE: This process must stay alive for the mount to be accessible ... Using Kubernetes Volumes \u00b6 Quoting Using Kubernetes Volumes of Apache Spark's official documentation: users can mount the following types of Kubernetes volumes into the driver and executor pods: hostPath: mounts a file or directory from the host node\u2019s filesystem into a pod. emptyDir: an initially empty volume created when a pod is assigned to a node. persistentVolumeClaim: used to mount a PersistentVolume into a pod. hostPath \u00b6 Let's use Kubernetes' hostPath that requires spark.kubernetes.*.volumes -prefixed configuration properties and the name of the volume (under the volumes field in the pod specification): --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s The demo uses configuration properties to set up a hostPath volume type with spark-k8s name and /tmp/spark-k8s path on the host (for the driver and executors separately). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Reviewing Volumes \u00b6 k describe po spark-demo-minikube $ k get po spark-demo-minikube -o=jsonpath='{.spec.volumes}' | jq [ { \"hostPath\": { \"path\": \"/tmp/spark-k8s\", \"type\": \"\" }, \"name\": \"spark-k8s\" }, { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-85c0cb76f277cdbe-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-zd6jt\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-zd6jt\" } } ] $ k get po spark-demo-minikube -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/tmp/spark-k8s\", \"name\": \"spark-k8s\" }, { \"mountPath\": \"/var/data/spark-87d1ba7c-819a-4274-82ca-98c1e135c136\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-zd6jt\", \"readOnly\": true } ] Log into the driver pod container and review /tmp/spark-k8s directory. There should be at least one spark-upload directory. $ k exec -it spark-demo-minikube -- bash $ ls -ltr /tmp/spark-k8s total 5 ... drwxr-xr-x 1 1000 999 96 Jan 11 17:21 spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08 $ cat /tmp/spark-k8s/spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08/test.me Hello World","title":"Spark and Local Filesystem in minikube"},{"location":"demo/spark-and-local-filesystem-in-minikube/#demo-spark-and-local-filesystem-in-minikube","text":"The motivation of the demo is to set up a Spark application deployed to minikube to access files on a local filesystem. Tip Start with Demo: Running Spark Application on minikube . The demo uses spark-submit --files and spark.kubernetes.file.upload.path configuration property to upload a static file to a directory that is then mounted to Spark application pods. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar","title":"Demo: Spark and Local Filesystem in minikube"},{"location":"demo/spark-and-local-filesystem-in-minikube/#mounting-filesystems","text":"Let's start off by mounting a host directory ( /tmp/spark-k8s ) to minikube. Quoting Mounting filesystems of minikube's official documentation: To mount a directory from the host into the guest using the mount subcommand $ minikube mount /tmp/spark-k8s:/tmp/spark-k8s \ud83d\udcc1 Mounting host path /tmp/spark-k8s into VM as /tmp/spark-k8s ... \u25aa Mount type: \u25aa User ID: docker \u25aa Group ID: docker \u25aa Version: 9p2000.L \u25aa Message Size: 262144 \u25aa Permissions: 755 (-rwxr-xr-x) \u25aa Options: map[] \u25aa Bind Address: 127.0.0.1:53125 \ud83d\ude80 Userspace file server: ufs starting \u2705 Successfully mounted /tmp/spark-k8s to /tmp/spark-k8s \ud83d\udccc NOTE: This process must stay alive for the mount to be accessible ...","title":"Mounting Filesystems"},{"location":"demo/spark-and-local-filesystem-in-minikube/#using-kubernetes-volumes","text":"Quoting Using Kubernetes Volumes of Apache Spark's official documentation: users can mount the following types of Kubernetes volumes into the driver and executor pods: hostPath: mounts a file or directory from the host node\u2019s filesystem into a pod. emptyDir: an initially empty volume created when a pod is assigned to a node. persistentVolumeClaim: used to mount a PersistentVolume into a pod.","title":"Using Kubernetes Volumes"},{"location":"demo/spark-and-local-filesystem-in-minikube/#hostpath","text":"Let's use Kubernetes' hostPath that requires spark.kubernetes.*.volumes -prefixed configuration properties and the name of the volume (under the volumes field in the pod specification): --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s The demo uses configuration properties to set up a hostPath volume type with spark-k8s name and /tmp/spark-k8s path on the host (for the driver and executors separately). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar","title":"hostPath"},{"location":"demo/spark-and-local-filesystem-in-minikube/#reviewing-volumes","text":"k describe po spark-demo-minikube $ k get po spark-demo-minikube -o=jsonpath='{.spec.volumes}' | jq [ { \"hostPath\": { \"path\": \"/tmp/spark-k8s\", \"type\": \"\" }, \"name\": \"spark-k8s\" }, { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-85c0cb76f277cdbe-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-zd6jt\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-zd6jt\" } } ] $ k get po spark-demo-minikube -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/tmp/spark-k8s\", \"name\": \"spark-k8s\" }, { \"mountPath\": \"/var/data/spark-87d1ba7c-819a-4274-82ca-98c1e135c136\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-zd6jt\", \"readOnly\": true } ] Log into the driver pod container and review /tmp/spark-k8s directory. There should be at least one spark-upload directory. $ k exec -it spark-demo-minikube -- bash $ ls -ltr /tmp/spark-k8s total 5 ... drwxr-xr-x 1 1000 999 96 Jan 11 17:21 spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08 $ cat /tmp/spark-k8s/spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08/test.me Hello World","title":"Reviewing Volumes"},{"location":"demo/spark-shell-on-minikube/","text":"Demo: spark-shell on minikube \u00b6 This demo shows how to run spark-shell on minikube that touts itself as: minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Note k is an alias of kubectl . Start minikube \u00b6 Quoting Prerequisites in the official documentation of Apache Spark: We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor. Let's start minikube with the recommended resources. minikube start --cpus 4 --memory 8192 Review Cluster Info \u00b6 Cluster Info \u00b6 k cluster-info k config view Pods \u00b6 List available pods. There should be none except Kubernetes system pods (and that's the reason for -A to include all pods, including system's). k get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-vqm2f 1/1 Running 0 2m3s kube-system etcd-minikube 1/1 Running 0 2m18s kube-system kube-apiserver-minikube 1/1 Running 0 2m18s kube-system kube-controller-manager-minikube 1/1 Running 0 2m18s kube-system kube-proxy-ms4wd 1/1 Running 0 2m4s kube-system kube-scheduler-minikube 1/1 Running 0 2m18s kube-system storage-provisioner 1/1 Running 0 2m18s Container Images \u00b6 List available Docker images in minikube's Docker registry. Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List available container images. docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.20.2 43154ddb57a8 12 days ago 118MB k8s.gcr.io/kube-controller-manager v1.20.2 a27166429d98 12 days ago 116MB k8s.gcr.io/kube-apiserver v1.20.2 a8c2fdb8bf76 12 days ago 122MB k8s.gcr.io/kube-scheduler v1.20.2 ed2c44fbdd78 12 days ago 46.4MB kubernetesui/dashboard v2.1.0 9a07b5b4bfac 6 weeks ago 226MB gcr.io/k8s-minikube/storage-provisioner v4 85069258b98a 7 weeks ago 29.7MB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 5 months ago 253MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 7 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 10 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 11 months ago 683kB Kubernetes Dashboard \u00b6 minikube dashboard Build Spark Image \u00b6 Quoting Submitting Applications to Kubernetes in the official documentation of Apache Spark: Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose, or customized to match an individual application\u2019s needs. It can be found in the kubernetes/dockerfiles/ directory. In a separate terminal... cd $SPARK_HOME Tip Review kubernetes/dockerfiles/spark (in your Spark installation) or resource-managers/kubernetes/docker (in the Spark source code). docker-image-tool \u00b6 Build and publish the Spark image. Note -m option to point the shell script to use minikube's Docker daemon. ./bin/docker-image-tool.sh \\ -m \\ -b java_image_tag=11-jre-slim \\ -t v3.1.1-rc1 \\ build Note As of Spark 3.1.1, java_image_tag argument is assumed 11-jre-slim (and so -b java_image_tag=11-jre-slim could've been skipped in the above command). docker images \u00b6 Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List the Spark image. docker images spark REPOSITORY TAG IMAGE ID CREATED SIZE spark v3.1.1-rc1 ad49befe3d09 19 seconds ago 509MB docker image inspect \u00b6 Use docker image inspect command to display detailed information on the Spark image. docker image inspect spark:v3.1.1-rc1 Create Namespace \u00b6 This step is optional, but gives a better exposure to the Kubernetes features supported by Apache Spark and is highly recommended. Tip Learn more in Creating a new namespace . k create ns spark-demo Set spark-demo as the default namespace using kubens tool. kubens spark-demo Spark Logging \u00b6 Enable ALL logging level for Kubernetes-related loggers to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s=ALL log4j.logger.org.apache.spark.scheduler.cluster.k8s=ALL Refer to Logging . Launch spark-shell \u00b6 cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') ./bin/spark-shell \\ --master k8s://$K8S_SERVER \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --verbose Soon you should see the Spark prompt similar to the following: Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.10) Type in expressions to have them evaluated. Type :help for more information. scala> spark.version res0: String = 3.1.1 scala> sc.master res1: String = k8s://https://127.0.0.1:55020 web UIs \u00b6 Open web UI of the Spark application at http://localhost:4040/ . Review the pods in the Kubernetes UI . Make sure to use spark-demo namespace. Scale Executors \u00b6 Just for some more fun, in spark-shell , request two more executors and observe the logs. sc.requestTotalExecutors(numExecutors = 4, localityAwareTasks = 0, hostToLocalTaskCount = Map.empty) sc.killExecutors(Seq(\"1\", \"3\")) Review the number of executors at http://localhost:4040/executors/ and in the Kubernetes UI. Clean Up \u00b6 minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all That's it. Congratulations!","title":"spark-shell on minikube"},{"location":"demo/spark-shell-on-minikube/#demo-spark-shell-on-minikube","text":"This demo shows how to run spark-shell on minikube that touts itself as: minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Note k is an alias of kubectl .","title":"Demo: spark-shell on minikube"},{"location":"demo/spark-shell-on-minikube/#start-minikube","text":"Quoting Prerequisites in the official documentation of Apache Spark: We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor. Let's start minikube with the recommended resources. minikube start --cpus 4 --memory 8192","title":"Start minikube"},{"location":"demo/spark-shell-on-minikube/#review-cluster-info","text":"","title":"Review Cluster Info"},{"location":"demo/spark-shell-on-minikube/#cluster-info","text":"k cluster-info k config view","title":"Cluster Info"},{"location":"demo/spark-shell-on-minikube/#pods","text":"List available pods. There should be none except Kubernetes system pods (and that's the reason for -A to include all pods, including system's). k get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-vqm2f 1/1 Running 0 2m3s kube-system etcd-minikube 1/1 Running 0 2m18s kube-system kube-apiserver-minikube 1/1 Running 0 2m18s kube-system kube-controller-manager-minikube 1/1 Running 0 2m18s kube-system kube-proxy-ms4wd 1/1 Running 0 2m4s kube-system kube-scheduler-minikube 1/1 Running 0 2m18s kube-system storage-provisioner 1/1 Running 0 2m18s","title":"Pods"},{"location":"demo/spark-shell-on-minikube/#container-images","text":"List available Docker images in minikube's Docker registry. Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List available container images. docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.20.2 43154ddb57a8 12 days ago 118MB k8s.gcr.io/kube-controller-manager v1.20.2 a27166429d98 12 days ago 116MB k8s.gcr.io/kube-apiserver v1.20.2 a8c2fdb8bf76 12 days ago 122MB k8s.gcr.io/kube-scheduler v1.20.2 ed2c44fbdd78 12 days ago 46.4MB kubernetesui/dashboard v2.1.0 9a07b5b4bfac 6 weeks ago 226MB gcr.io/k8s-minikube/storage-provisioner v4 85069258b98a 7 weeks ago 29.7MB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 5 months ago 253MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 7 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 10 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 11 months ago 683kB","title":"Container Images"},{"location":"demo/spark-shell-on-minikube/#kubernetes-dashboard","text":"minikube dashboard","title":"Kubernetes Dashboard"},{"location":"demo/spark-shell-on-minikube/#build-spark-image","text":"Quoting Submitting Applications to Kubernetes in the official documentation of Apache Spark: Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose, or customized to match an individual application\u2019s needs. It can be found in the kubernetes/dockerfiles/ directory. In a separate terminal... cd $SPARK_HOME Tip Review kubernetes/dockerfiles/spark (in your Spark installation) or resource-managers/kubernetes/docker (in the Spark source code).","title":"Build Spark Image"},{"location":"demo/spark-shell-on-minikube/#docker-image-tool","text":"Build and publish the Spark image. Note -m option to point the shell script to use minikube's Docker daemon. ./bin/docker-image-tool.sh \\ -m \\ -b java_image_tag=11-jre-slim \\ -t v3.1.1-rc1 \\ build Note As of Spark 3.1.1, java_image_tag argument is assumed 11-jre-slim (and so -b java_image_tag=11-jre-slim could've been skipped in the above command).","title":"docker-image-tool"},{"location":"demo/spark-shell-on-minikube/#docker-images","text":"Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) List the Spark image. docker images spark REPOSITORY TAG IMAGE ID CREATED SIZE spark v3.1.1-rc1 ad49befe3d09 19 seconds ago 509MB","title":"docker images"},{"location":"demo/spark-shell-on-minikube/#docker-image-inspect","text":"Use docker image inspect command to display detailed information on the Spark image. docker image inspect spark:v3.1.1-rc1","title":"docker image inspect"},{"location":"demo/spark-shell-on-minikube/#create-namespace","text":"This step is optional, but gives a better exposure to the Kubernetes features supported by Apache Spark and is highly recommended. Tip Learn more in Creating a new namespace . k create ns spark-demo Set spark-demo as the default namespace using kubens tool. kubens spark-demo","title":"Create Namespace"},{"location":"demo/spark-shell-on-minikube/#spark-logging","text":"Enable ALL logging level for Kubernetes-related loggers to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s=ALL log4j.logger.org.apache.spark.scheduler.cluster.k8s=ALL Refer to Logging .","title":"Spark Logging"},{"location":"demo/spark-shell-on-minikube/#launch-spark-shell","text":"cd $SPARK_HOME K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}') ./bin/spark-shell \\ --master k8s://$K8S_SERVER \\ --conf spark.kubernetes.container.image=spark:v3.1.1-rc1 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --verbose Soon you should see the Spark prompt similar to the following: Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 3.1.1 /_/ Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.10) Type in expressions to have them evaluated. Type :help for more information. scala> spark.version res0: String = 3.1.1 scala> sc.master res1: String = k8s://https://127.0.0.1:55020","title":"Launch spark-shell"},{"location":"demo/spark-shell-on-minikube/#web-uis","text":"Open web UI of the Spark application at http://localhost:4040/ . Review the pods in the Kubernetes UI . Make sure to use spark-demo namespace.","title":"web UIs"},{"location":"demo/spark-shell-on-minikube/#scale-executors","text":"Just for some more fun, in spark-shell , request two more executors and observe the logs. sc.requestTotalExecutors(numExecutors = 4, localityAwareTasks = 0, hostToLocalTaskCount = Map.empty) sc.killExecutors(Seq(\"1\", \"3\")) Review the number of executors at http://localhost:4040/executors/ and in the Kubernetes UI.","title":"Scale Executors"},{"location":"demo/spark-shell-on-minikube/#clean-up","text":"minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all That's it. Congratulations!","title":"Clean Up"}]}