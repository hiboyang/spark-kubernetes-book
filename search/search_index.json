{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":true},"docs":[{"location":"","text":"<p>Welcome to The Internals of Spark on Kubernetes online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>  \"The Internals Of\" series<p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark on Kubernetes \ud83d\udd25</p>","title":"The Internals of Spark on Kubernetes (Apache Spark 3.2.0)"},{"location":"BasicDriverFeatureStep/","text":"<p><code>BasicDriverFeatureStep</code> is a KubernetesFeatureConfigStep.</p>","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#creating-instance","text":"<p><code>BasicDriverFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesDriverConf  <p><code>BasicDriverFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for the driver pod spec</li> </ul>","title":"Creating Instance"},{"location":"BasicDriverFeatureStep/#name-of-driver-pod","text":"","title":"Name of Driver Pod <p><code>BasicDriverFeatureStep</code> uses the spark.kubernetes.driver.pod.name configuration property (if defined) or the resourceNamePrefix (of the given KubernetesDriverConf) as the name of the driver pod:</p> <pre><code>[resourceNamePrefix]-driver\n</code></pre>"},{"location":"BasicDriverFeatureStep/#driver-memory","text":"","title":"Driver Memory <p><code>BasicDriverFeatureStep</code> calculates the driver memory for the driver pod based on the Base Driver Memory with the extra Non-Heap Memory Overhead.</p> <p>Driver memory is the quantity (in <code>Mi</code>s) of the driver memory resource (the request and limit).</p> Demo: Running Spark Examples on minikube<p>Use <code>kubectl get po [driverPod]</code> to review the memory resource spec. Learn more in Demo: Running Spark Examples on minikube.</p>"},{"location":"BasicDriverFeatureStep/#base-driver-memory","text":"","title":"Base Driver Memory <p><code>BasicDriverFeatureStep</code> uses <code>spark.driver.memory</code> configuration property (default: <code>1g</code>) for the base driver memory.</p>"},{"location":"BasicDriverFeatureStep/#memory-overhead-factor","text":"","title":"Memory Overhead Factor <p><code>BasicDriverFeatureStep</code> uses different memory overhead factors based on the MainAppResource (of the given KubernetesDriverConf):</p> <ul> <li> <p>For JVM applications (Java, Scala, etc.), it is spark.kubernetes.memoryOverheadFactor configuration property</p> </li> <li> <p>For non-JVM applications (Python and R applications), it is spark.kubernetes.memoryOverheadFactor configuration property (if defined) or <code>0.4</code></p> </li> </ul> <p>The memory overhead factor is used for the following:</p> <ul> <li>Additional System Properties</li> <li>Memory Overhead</li> </ul>"},{"location":"BasicDriverFeatureStep/#non-heap-memory-overhead","text":"","title":"Non-Heap Memory Overhead <p><code>BasicDriverFeatureStep</code> defines the memory overhead for a non-heap memory to be allocated per driver in cluster mode (in <code>MiB</code>s) based on the following:</p> <ul> <li><code>spark.driver.memoryOverhead</code> configuration property (if defined)</li> <li>Maximum of the Memory Overhead Factor multiplied by the Base Driver Memory and <code>384</code></li> </ul>"},{"location":"BasicDriverFeatureStep/#additional-system-properties","text":"","title":"Additional System Properties <pre><code>getAdditionalPodSystemProperties(): Map[String, String]\n</code></pre> <p><code>getAdditionalPodSystemProperties</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>getAdditionalPodSystemProperties</code> sets the following additional properties:</p>    Name Value     spark.kubernetes.submitInDriver <code>true</code>   spark.kubernetes.driver.pod.name driverPodName   spark.kubernetes.memoryOverheadFactor overheadFactor   <code>spark.app.id</code> appId (of the KubernetesDriverConf)    <p>In the end, <code>getAdditionalPodSystemProperties</code> uploads local files (in <code>spark.jars</code> and <code>spark.files</code> configuration properties) to a Hadoop-compatible file system and adds their target Hadoop paths to the additional properties.</p>  <p>spark.kubernetes.file.upload.path Configuration Property</p> <p>The Hadoop DFS path to upload local files to is defined using spark.kubernetes.file.upload.path configuration property.</p>"},{"location":"BasicDriverFeatureStep/#driver-container-image-name","text":"","title":"Driver Container Image Name <p><code>BasicDriverFeatureStep</code> uses spark.kubernetes.driver.container.image for the name of the container image for drivers.</p> <p>The name must be defined or <code>BasicDriverFeatureStep</code> throws an <code>SparkException</code>:</p> <pre><code>Must specify the driver container image\n</code></pre> <p><code>driverContainerImage</code>\u00a0is used when requested for configurePod.</p>"},{"location":"BasicDriverFeatureStep/#configuring-driver-pod","text":"","title":"Configuring Driver Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code>...FIXME</p>"},{"location":"BasicExecutorFeatureStep/","text":"<p><code>BasicExecutorFeatureStep</code> is a KubernetesFeatureConfigStep.</p>","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#creating-instance","text":"<p><code>BasicExecutorFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesExecutorConf <li> <code>SecurityManager</code> (Apache Spark)  <p><code>BasicExecutorFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>","title":"Creating Instance"},{"location":"BasicExecutorFeatureStep/#executor-container-image-name","text":"","title":"Executor Container Image Name <p><code>BasicExecutorFeatureStep</code> asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a <code>SparkException</code>:</p> <pre><code>Must specify the executor container image\n</code></pre>"},{"location":"BasicExecutorFeatureStep/#configuring-pod","text":"","title":"Configuring Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code>...FIXME</p>"},{"location":"Client/","text":"<p><code>Client</code> submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status).</p>","title":"Client"},{"location":"Client/#creating-instance","text":"<p><code>Client</code> takes the following to be created:</p> <ul> <li> KubernetesDriverConf <li> KubernetesDriverBuilder <li> Kubernetes' <code>KubernetesClient</code> <li> LoggingPodStatusWatcher  <p><code>Client</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"Client/#running-driver-pod","text":"","title":"Running Driver Pod <pre><code>run(): Unit\n</code></pre> <p><code>run</code> requests the KubernetesDriverBuilder to build a KubernetesDriverSpec.</p> <p><code>run</code> requests the KubernetesDriverConf for the resourceNamePrefix to be used for the name of the driver's config map:</p> <pre><code>[resourceNamePrefix]-driver-conf-map\n</code></pre> <p><code>run</code> builds a ConfigMap (with the name and the system properties of the <code>KubernetesDriverSpec</code>).</p> <p><code>run</code> creates a driver container (based on the <code>KubernetesDriverSpec</code>) and adds the following:</p> <ul> <li><code>SPARK_CONF_DIR</code> env var as <code>/opt/spark/conf</code></li> <li><code>spark-conf-volume</code> volume mount as <code>/opt/spark/conf</code></li> </ul> <p><code>run</code> creates a driver pod (based on the <code>KubernetesDriverSpec</code>) with the driver container and a new <code>spark-conf-volume</code> volume for the <code>ConfigMap</code>.</p> <p><code>run</code> requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher) and, when available, attaches the <code>ConfigMap</code>.</p> <p><code>run</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>"},{"location":"Client/#adddriverownerreference","text":"","title":"addDriverOwnerReference <pre><code>addDriverOwnerReference(\n  driverPod: Pod,\n  resources: Seq[HasMetadata]): Unit\n</code></pre> <p><code>addDriverOwnerReference</code>...FIXME</p>"},{"location":"Client/#building-configmap","text":"","title":"Building ConfigMap <pre><code>buildConfigMap(\n  configMapName: String,\n  conf: Map[String, String]): ConfigMap\n</code></pre> <p><code>buildConfigMap</code> builds a Kubernetes <code>ConfigMap</code> with the given <code>conf</code> key-value pairs stored as spark.properties data and the given <code>configMapName</code> name.</p> <p>The stored data uses an extra comment:</p> <pre><code>Java properties built from Kubernetes config map with name: [configMapName]\n</code></pre> Kubernetes Documentation<p>Learn more about ConfigMaps in the official Kubernetes Documentation.</p>  Demo: Spark and Local Filesystem in minikube<p>Learn more about ConfigMaps and volumes in Demo: Spark and Local Filesystem in minikube.</p>"},{"location":"ClientArguments/","text":"<p><code>ClientArguments</code> represents a KubernetesClientApplication to start.</p>","title":"ClientArguments"},{"location":"ClientArguments/#creating-instance","text":"<p><code>ClientArguments</code> takes the following to be created:</p> <ul> <li> <code>MainAppResource</code> <li> Name of the main class of a Spark application to run <li> Driver Arguments  <p><code>ClientArguments</code> is created\u00a0(via fromCommandLineArgs utility) when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"ClientArguments/#fromcommandlineargs-utility","text":"","title":"fromCommandLineArgs Utility <pre><code>fromCommandLineArgs(\n  args: Array[String]): ClientArguments\n</code></pre> <p><code>fromCommandLineArgs</code> slices the input <code>args</code> into key-value pairs and creates a ClientArguments as follows:</p> <ul> <li><code>--primary-java-resource</code>, <code>--primary-py-file</code> or <code>--primary-r-file</code> keys are used for the MainAppResource</li> <li><code>--main-class</code> for the name of the main class</li> <li><code>--arg</code> for the driver arguments</li> </ul>  <p>Important</p> <p>The main class must be specified via <code>--main-class</code> or <code>fromCommandLineArgs</code> throws an <code>IllegalArgumentException</code>.</p>  <p><code>fromCommandLineArgs</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>"},{"location":"DriverCommandFeatureStep/","text":"<p><code>DriverCommandFeatureStep</code> is a KubernetesFeatureConfigStep.</p>","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#creating-instance","text":"<p><code>DriverCommandFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesDriverConf  <p><code>DriverCommandFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested to build a KubernetesDriverSpec from features</li> </ul>","title":"Creating Instance"},{"location":"DriverCommandFeatureStep/#configuring-pod","text":"","title":"Configuring Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code> branches off based on the MainAppResource (of the KubernetesDriverConf):</p> <ul> <li> <p>For <code>JavaMainAppResource</code>, <code>configurePod</code> configures a pod for Java with the primary resource (if defined) or uses <code>spark-internal</code> special value</p> </li> <li> <p>For <code>PythonMainAppResource</code>, <code>configurePod</code> configures a pod for Python with the primary resource</p> </li> <li> <p>For <code>RMainAppResource</code>, <code>configurePod</code> configures a pod for R with the primary resource</p> </li> </ul>"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-java-application","text":"","title":"Configuring Pod for Java Application <pre><code>configureForJava(\n  pod: SparkPod,\n  res: String): SparkPod\n</code></pre> <p><code>configureForJava</code> builds the base driver container for the given <code>SparkPod</code> and the primary resource.</p> <p>In the end, <code>configureForJava</code> creates another <code>SparkPod</code> (for the pod of the given <code>SparkPod</code>) and the driver container.</p>"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-python-application","text":"","title":"Configuring Pod for Python Application <pre><code>configureForPython(\n  pod: SparkPod,\n  res: String): SparkPod\n</code></pre> <p><code>configureForPython</code>...FIXME</p>"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-r-application","text":"","title":"Configuring Pod for R Application <pre><code>configureForR(\n  pod: SparkPod,\n  res: String): SparkPod\n</code></pre> <p><code>configureForR</code>...FIXME</p>"},{"location":"DriverCommandFeatureStep/#base-driver-containerbuilder","text":"","title":"Base Driver ContainerBuilder <pre><code>baseDriverContainer(\n  pod: SparkPod,\n  resource: String): ContainerBuilder\n</code></pre> <p><code>baseDriverContainer</code> renames the given primary <code>resource</code> if the MainAppResource is a <code>JavaMainAppResource</code>. Otherwise, <code>baseDriverContainer</code> leaves the primary resource as-is.</p> <p><code>baseDriverContainer</code> creates a <code>ContainerBuilder</code> (for the pod of the given <code>SparkPod</code>) and adds the following arguments (in that order):</p> <ol> <li><code>driver</code></li> <li><code>--properties-file</code> with <code>/opt/spark/conf/spark.properties</code></li> <li><code>--class</code> with the mainClass of the KubernetesDriverConf</li> <li>the primary resource (possibly renamed if a <code>MainAppResource</code>)</li> <li>appArgs of the KubernetesDriverConf</li> </ol>  <p>Note</p> <p>The arguments are then used by the default <code>entrypoint.sh</code> of the official Docker image of Apache Spark (in <code>resource-managers/kubernetes/docker/src/main/dockerfiles/spark/</code>).</p> <p>Use the following <code>kubectl</code> command to see the arguments:</p> <pre><code>kubectl get po [driverPod] -o=jsonpath='{.spec.containers[0].args}'\n</code></pre>"},{"location":"DriverKubernetesCredentialsFeatureStep/","text":"<p><code>DriverKubernetesCredentialsFeatureStep</code> is a KubernetesFeatureConfigStep for KubernetesDriverBuilder (to build a driver pod spec).</p>","title":"DriverKubernetesCredentialsFeatureStep"},{"location":"DriverKubernetesCredentialsFeatureStep/#creating-instance","text":"<p><code>DriverKubernetesCredentialsFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesConf  <p><code>DriverKubernetesCredentialsFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for the driver pod spec</li> </ul>","title":"Creating Instance"},{"location":"DriverKubernetesCredentialsFeatureStep/#sparkkubernetesauthenticatedriverserviceaccountname","text":"","title":"spark.kubernetes.authenticate.driver.serviceAccountName <p><code>DriverKubernetesCredentialsFeatureStep</code> uses the spark.kubernetes.authenticate.driver.serviceAccountName configuration property for configuring a pod.</p>"},{"location":"DriverKubernetesCredentialsFeatureStep/#configuring-driver-pod","text":"","title":"Configuring Driver Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code>...FIXME</p>"},{"location":"DriverServiceFeatureStep/","text":"<p><code>DriverServiceFeatureStep</code> is a KubernetesFeatureConfigStep.</p>","title":"DriverServiceFeatureStep"},{"location":"DriverServiceFeatureStep/#creating-instance","text":"<p><code>DriverServiceFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesDriverConf <li> <code>Clock</code> (default: <code>SystemClock</code>)  <p><code>DriverServiceFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for a driver pod spec</li> </ul>","title":"Creating Instance"},{"location":"DriverServiceFeatureStep/#service-name","text":"","title":"Service Name <p><code>DriverServiceFeatureStep</code> defines Service Name based on the Preferred Service Name (if fits the name limit) or generates one:</p> <pre><code>spark-[randomServiceId]-driver-svc\n</code></pre> <p><code>DriverServiceFeatureStep</code> prints out the following WARN message when generating the service name:</p> <pre><code>Driver's hostname would preferably be [preferredServiceName], but this is too long (must be &lt;= 63 characters). Falling back to use $shorterServiceName as the driver service's name.\n</code></pre> <p>The service name is used for the following:</p> <ul> <li>Additional System Properties (as <code>spark.driver.host</code>)</li> <li>Additional Kubernetes Resources</li> </ul>"},{"location":"DriverServiceFeatureStep/#preferred-service-name","text":"","title":"Preferred Service Name <p><code>DriverServiceFeatureStep</code> uses the resourceNamePrefix (of the given KubernetesDriverConf) with <code>-driver-svc</code> postfix as the Preferred Service Name.</p> <pre><code>[resourceNamePrefix]-driver-svc\n</code></pre> <p>The preferred service name becomes the resolved service name only when shorter than <code>63</code> characters.</p>"},{"location":"DriverServiceFeatureStep/#additional-kubernetes-resources","text":"","title":"Additional Kubernetes Resources <pre><code>getAdditionalKubernetesResources(): Seq[HasMetadata]\n</code></pre> <p><code>getAdditionalKubernetesResources</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>getAdditionalKubernetesResources</code> defines a Kubernetes service with the Service Name.</p> kubectl get services<p>Use <code>k get services</code> to list Kubernetes services.</p>"},{"location":"DriverServiceFeatureStep/#additional-system-properties","text":"","title":"Additional System Properties <pre><code>getAdditionalPodSystemProperties(): Map[String, String]\n</code></pre> <p><code>getAdditionalPodSystemProperties</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>getAdditionalPodSystemProperties</code> sets the following additional Spark properties:</p>    Name Value     <code>spark.driver.host</code> serviceName<code>.</code>namespace<code>.svc</code>   <code>spark.driver.port</code> driverPort   <code>spark.driver.blockManager.port</code>  driverBlockManagerPort"},{"location":"DriverServiceFeatureStep/#configuring-driver-pod","text":"","title":"Configuring Driver Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code>...FIXME</p>"},{"location":"DriverServiceFeatureStep/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"EnvSecretsFeatureStep/","text":"<p><code>EnvSecretsFeatureStep</code> is...FIXME</p>","title":"EnvSecretsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/","text":"<p><code>ExecutorKubernetesCredentialsFeatureStep</code> is a KubernetesFeatureConfigStep for KubernetesExecutorBuilder (to build a pod spec for executors).</p>","title":"ExecutorKubernetesCredentialsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#creating-instance","text":"<p><code>ExecutorKubernetesCredentialsFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesConf  <p><code>ExecutorKubernetesCredentialsFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>","title":"Creating Instance"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#sparkkubernetesauthenticatedriverserviceaccountname","text":"","title":"spark.kubernetes.authenticate.driver.serviceAccountName <p><code>ExecutorKubernetesCredentialsFeatureStep</code> uses spark.kubernetes.authenticate.executor.serviceAccountName configuration property when requested to configure a pod.</p>"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#sparkkubernetesauthenticateexecutorserviceaccountname","text":"","title":"spark.kubernetes.authenticate.executor.serviceAccountName <p><code>ExecutorKubernetesCredentialsFeatureStep</code> uses spark.kubernetes.authenticate.executor.serviceAccountName configuration property when requested to configure a pod.</p>"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#configuring-pod","text":"","title":"Configuring Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code> buildPodWithServiceAccount when...FIXME</p>"},{"location":"ExecutorPodsAllocator/","text":"<p><code>ExecutorPodsAllocator</code> is responsible for allocating pods for executors (possibly dynamic) in a Spark application.</p> <p><code>ExecutorPodsAllocator</code> is used to create a KubernetesClusterSchedulerBackend.</p>","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#creating-instance","text":"<p><code>ExecutorPodsAllocator</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> <li> <code>SecurityManager</code> <li> KubernetesExecutorBuilder <li> <code>KubernetesClient</code> <li> ExecutorPodsSnapshotsStore <li> <code>Clock</code>  <p><code>ExecutorPodsAllocator</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>","title":"Creating Instance"},{"location":"ExecutorPodsAllocator/#executor-pod-allocation-timeout","text":"","title":"Executor Pod Allocation Timeout <p><code>ExecutorPodsAllocator</code> defines Executor Pod Allocation Timeout that is the maximum of the following values:</p> <ul> <li>5 times of spark.kubernetes.allocation.batch.delay configuration property</li> <li>spark.kubernetes.allocation.executor.timeout configuration property</li> </ul> <p><code>ExecutorPodsAllocator</code> uses the allocation timeout to detect \"old\" executor pod requests when handling executor pods snapshots.</p>"},{"location":"ExecutorPodsAllocator/#sparkdynamicallocationenabled","text":"","title":"spark.dynamicAllocation.enabled <p><code>ExecutorPodsAllocator</code> uses <code>spark.dynamicAllocation.enabled</code> configuration property to turn dynamic allocation of executors on and off.</p>  <p>The Internals of Apache Spark</p> <p>Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark.</p>"},{"location":"ExecutorPodsAllocator/#driver-pod","text":"","title":"Driver Pod <pre><code>driverPod: Option[Pod]\n</code></pre> <p><code>driverPod</code> is a driver pod with the name of spark.kubernetes.driver.pod.name configuration property (if defined).</p> <p><code>ExecutorPodsAllocator</code> throws a <code>SparkException</code> when the driver pod could not be found in a Kubernetes cluster:</p> <pre><code>No pod was found named [kubernetesDriverPodName] in the cluster in the namespace [namespace] (this was supposed to be the driver pod.).\n</code></pre>"},{"location":"ExecutorPodsAllocator/#sparkkubernetesdriverpodname","text":"","title":"spark.kubernetes.driver.pod.name <p><code>ExecutorPodsAllocator</code> uses spark.kubernetes.driver.pod.name configuration property to look up the driver pod by name when created.</p>"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchsize","text":"","title":"spark.kubernetes.allocation.batch.size <p><code>ExecutorPodsAllocator</code> uses spark.kubernetes.allocation.batch.size configuration property when allocating executor pods.</p>"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchdelay","text":"","title":"spark.kubernetes.allocation.batch.delay <p><code>ExecutorPodsAllocator</code> uses spark.kubernetes.allocation.batch.delay configuration property for the following:</p> <ul> <li>podCreationTimeout</li> <li>Registering a subscriber</li> </ul>"},{"location":"ExecutorPodsAllocator/#sparkkubernetesexecutordeleteontermination","text":"","title":"spark.kubernetes.executor.deleteOnTermination <p><code>ExecutorPodsAllocator</code> uses spark.kubernetes.executor.deleteOnTermination configuration property.</p>"},{"location":"ExecutorPodsAllocator/#starting","text":"","title":"Starting <pre><code>start(\n  applicationId: String): Unit\n</code></pre> <p><code>start</code> requests the ExecutorPodsSnapshotsStore to subscribe this <code>ExecutorPodsAllocator</code> to be notified about new snapshots (with pod allocation delay based on spark.kubernetes.allocation.batch.delay configuration property).</p> <p><code>start</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClusterSchedulerBackend</code> is requested to start</li> </ul>"},{"location":"ExecutorPodsAllocator/#processing-executor-pods-snapshots","text":"","title":"Processing Executor Pods Snapshots <pre><code>onNewSnapshots(\n  applicationId: String,\n  snapshots: Seq[ExecutorPodsSnapshot]): Unit\n</code></pre> <p><code>onNewSnapshots</code> removes the executor IDs (of the executor pods in the given snapshots) from the newlyCreatedExecutors internal registry.</p> <p>For the remaining executor IDs in the newlyCreatedExecutors internal registry, <code>onNewSnapshots</code> finds timed-out executor IDs whose creation time exceeded some podCreationTimeout threshold. For the other executor IDs, <code>onNewSnapshots</code> prints out the following DEBUG message to the logs:</p> <pre><code>Executor with id [execId] was not found in the Kubernetes cluster since it was created [time] milliseconds ago.\n</code></pre> <p>For any timed-out executor IDs, <code>onNewSnapshots</code> prints out the following WARN message to the logs:</p> <pre><code>Executors with ids [ids] were not detected in the Kubernetes cluster after [podCreationTimeout] ms despite the fact that a previous allocation attempt tried to create them. The executors may have been deleted but the application missed the deletion event.\n</code></pre> <p><code>onNewSnapshots</code> removes (forgets) the timed-out executor IDs (from the newlyCreatedExecutors internal registry). With the spark.kubernetes.executor.deleteOnTermination configuration property enabled, <code>onNewSnapshots</code> requests the KubernetesClient to delete pods with the following labels:</p> <ul> <li><code>spark-app-selector</code> with the given <code>applicationId</code></li> <li><code>spark-role</code>=<code>executor</code></li> <li><code>spark-exec-id</code> for all timed-out executor IDs</li> </ul> <p><code>onNewSnapshots</code> updates the lastSnapshot internal registry with the last <code>ExecutorPodsSnapshot</code> among the given <code>snapshots</code> if available.</p> <p> <p><code>onNewSnapshots</code>...FIXME</p>"},{"location":"ExecutorPodsAllocator/#requesting-executors-from-kubernetes","text":"","title":"Requesting Executors from Kubernetes <pre><code>requestNewExecutors(\n  expected: Int,\n  running: Int,\n  applicationId: String,\n  resourceProfileId: Int): Unit\n</code></pre> <p><code>requestNewExecutors</code> determines the number of executor pods to allocate based on the given <code>expected</code> and <code>running</code> and spark.kubernetes.allocation.batch.size configuration property.</p> <p><code>requestNewExecutors</code> prints out the following INFO message to the logs:</p> <pre><code>Going to request [numExecutorsToAllocate] executors from Kubernetes for ResourceProfile Id: [resourceProfileId], target: [expected] running: [running].\n</code></pre> <p>For every new executor pod, <code>requestNewExecutors</code> does the following:</p> <ol> <li>Increments the executor ID counter</li> <li>Creates a KubernetesExecutorConf for the executor ID, the given <code>applicationId</code> and <code>resourceProfileId</code>, and the driver pod</li> <li>Requests the KubernetesExecutorBuilder to build the pod spec for executors</li> <li>Requests the KubernetesClient to create an executor pod with an executor container attached</li> <li> <p>Requests the KubernetesClient to create <code>PersistentVolumeClaim</code> resources if there are any defined (as additional resources) and prints out the following INFO message to the logs:</p> <pre><code>Trying to create PersistentVolumeClaim [name] with StorageClass [storageClassName]\n</code></pre> </li> <li> <p>Registers the new executor ID in the newlyCreatedExecutors registry</p> </li> <li> <p>Prints out the following DEBUG message to the logs:</p> <pre><code>Requested executor with id [newExecutorId] from Kubernetes.\n</code></pre> </li> </ol> <p>In case of any exceptions, <code>requestNewExecutors</code> requests the KubernetesClient to delete the failed executor pod.</p>"},{"location":"ExecutorPodsAllocator/#setting-expected-number-of-executors-per-resourceprofile","text":"","title":"Setting Expected Number of Executors per ResourceProfile <pre><code>setTotalExpectedExecutors(\n  resourceProfileToTotalExecs: Map[ResourceProfile, Int]): Unit\n</code></pre> <p><code>setTotalExpectedExecutors</code> updates the rpIdToResourceProfile and totalExpectedExecutorsPerResourceProfileId internal registries for every <code>ResourceProfile</code>.</p> <p><code>setTotalExpectedExecutors</code> prints out the following DEBUG message to the logs:</p> <pre><code>Set total expected execs to [totalExpectedExecutorsPerResourceProfileId]\n</code></pre> <p>With no pending pods, <code>setTotalExpectedExecutors</code> requests the ExecutorPodsSnapshotsStore to notifySubscribers.</p> <p><code>setTotalExpectedExecutors</code> is used when:</p> <ul> <li><code>KubernetesClusterSchedulerBackend</code> is requested to start and doRequestTotalExecutors</li> </ul>"},{"location":"ExecutorPodsAllocator/#registries","text":"","title":"Registries"},{"location":"ExecutorPodsAllocator/#newlycreatedexecutors","text":"","title":"newlyCreatedExecutors <pre><code>newlyCreatedExecutors: Map[Long, Long]\n</code></pre> <p><code>ExecutorPodsAllocator</code> uses <code>newlyCreatedExecutors</code> internal registry to track executor IDs (with the timestamps they were created) that have been requested from Kubernetes but have not been detected in any snapshot yet.</p> <p>Used in onNewSnapshots</p>"},{"location":"ExecutorPodsAllocator/#executor_id_counter","text":"","title":"EXECUTOR_ID_COUNTER <p><code>ExecutorPodsAllocator</code> uses a Java AtomicLong for the missing executor IDs that are going to be requested (in onNewSnapshots) when...FIXME</p>"},{"location":"ExecutorPodsAllocator/#haspendingpods-flag","text":"","title":"hasPendingPods Flag <pre><code>hasPendingPods: AtomicBoolean\n</code></pre> <p><code>ExecutorPodsAllocator</code> uses a Java AtomicBoolean as a flag to avoid notifying subscribers.</p> <p>Starts as <code>false</code> and is updated every onNewSnapshots</p> <p>Used in setTotalExpectedExecutors (only when <code>false</code>)</p>"},{"location":"ExecutorPodsAllocator/#totalexpectedexecutorsperresourceprofileid","text":"","title":"totalExpectedExecutorsPerResourceProfileId <pre><code>totalExpectedExecutorsPerResourceProfileId: ConcurrentHashMap[Int, Int]\n</code></pre> <p><code>ExecutorPodsAllocator</code> uses a Java ConcurrentHashMap for...FIXME</p> <p>A new entry added while changing the total expected executors</p> <p>Used in onNewSnapshots</p>"},{"location":"ExecutorPodsAllocator/#rpidtoresourceprofile","text":"","title":"rpIdToResourceProfile <pre><code>rpIdToResourceProfile: HashMap[Int, ResourceProfile]\n</code></pre> <p><code>ExecutorPodsAllocator</code> uses a Java HashMap as a lookup table of <code>ResourceProfile</code>s by ID.</p> <p>A new entry added while changing the total expected executors</p> <p>Used in requestNewExecutors</p>"},{"location":"ExecutorPodsAllocator/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"ExecutorPodsLifecycleManager/","text":"","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#creating-instance","text":"<p><code>ExecutorPodsLifecycleManager</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> <li> <code>KubernetesClient</code> <li> ExecutorPodsSnapshotsStore <li> Guava <code>Cache</code>  <p><code>ExecutorPodsLifecycleManager</code> is created\u00a0when <code>KubernetesClusterManager</code> is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend).</p>","title":"Creating Instance"},{"location":"ExecutorPodsLifecycleManager/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutoreventprocessinginterval","text":"","title":"spark.kubernetes.executor.eventProcessingInterval <p><code>ExecutorPodsLifecycleManager</code> uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME</p>"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutordeleteontermination","text":"","title":"spark.kubernetes.executor.deleteOnTermination <p><code>ExecutorPodsLifecycleManager</code> uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState.</p>"},{"location":"ExecutorPodsLifecycleManager/#missing-pod-timeout","text":"","title":"Missing Pod Timeout <p><code>ExecutorPodsLifecycleManager</code> defines Missing Pod Timeout based on the spark.kubernetes.executor.missingPodDetectDelta configuration property.</p> <p><code>ExecutorPodsLifecycleManager</code> uses the timeout to detect lost executor pods when handling executor pods snapshots.</p>"},{"location":"ExecutorPodsLifecycleManager/#starting","text":"","title":"Starting <pre><code>start(\n  schedulerBackend: KubernetesClusterSchedulerBackend): Unit\n</code></pre> <p><code>start</code> requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods.</p> <p><code>start</code>\u00a0is used when <code>KubernetesClusterSchedulerBackend</code> is started.</p>"},{"location":"ExecutorPodsLifecycleManager/#processing-executor-pods-snapshots","text":"","title":"Processing Executor Pods Snapshots <pre><code>onNewSnapshots(\n  schedulerBackend: KubernetesClusterSchedulerBackend,\n  snapshots: Seq[ExecutorPodsSnapshot]): Unit\n</code></pre> <p><code>onNewSnapshots</code> creates an empty <code>execIdsRemovedInThisRound</code> collection of executors to be removed.</p> <p><code>onNewSnapshots</code> walks over the input <code>ExecutorPodsSnapshot</code>s and branches off based on <code>ExecutorPodState</code>:</p> <ul> <li> <p>For <code>PodDeleted</code>, <code>onNewSnapshots</code> prints out the following DEBUG message to the logs:</p> <pre><code>Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName]\n</code></pre> <p><code>onNewSnapshots</code> removeExecutorFromSpark and adds the executor ID to the <code>execIdsRemovedInThisRound</code> local collection.</p> </li> <li> <p>For <code>PodFailed</code>, <code>onNewSnapshots</code> prints out the following DEBUG message to the logs:</p> <pre><code>Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName]\n</code></pre> <p><code>onNewSnapshots</code> onFinalNonDeletedState with the <code>execIdsRemovedInThisRound</code> local collection.</p> </li> <li> <p>For <code>PodSucceeded</code>, <code>onNewSnapshots</code> requests the input <code>KubernetesClusterSchedulerBackend</code> to isExecutorActive. If so, <code>onNewSnapshots</code> prints out the following INFO message to the logs:</p> <pre><code>Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed.\n</code></pre> <p>Otherwise, <code>onNewSnapshots</code> prints out the following DEBUG message to the logs:</p> <pre><code>Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName].\n</code></pre> <p><code>onNewSnapshots</code> onFinalNonDeletedState with the <code>execIdsRemovedInThisRound</code> local collection.</p> </li> </ul>"},{"location":"ExecutorPodsLifecycleManager/#onfinalnondeletedstate","text":"","title":"onFinalNonDeletedState <pre><code>onFinalNonDeletedState(\n  podState: FinalPodState,\n  execId: Long,\n  schedulerBackend: KubernetesClusterSchedulerBackend,\n  deleteFromK8s: Boolean): Boolean\n</code></pre> <p><code>onFinalNonDeletedState</code> removeExecutorFromSpark (and records the <code>deleted</code> return flag to be returned in the end).</p> <p>With the given <code>deleteFromK8s</code> flag enabled, <code>onFinalNonDeletedState</code> removeExecutorFromK8s.</p>"},{"location":"ExecutorPodsLifecycleManager/#removeexecutorfromspark","text":"","title":"removeExecutorFromSpark <pre><code>removeExecutorFromSpark(\n  schedulerBackend: KubernetesClusterSchedulerBackend,\n  podState: FinalPodState,\n  execId: Long): Unit\n</code></pre> <p><code>removeExecutorFromSpark</code>...FIXME</p>"},{"location":"ExecutorPodsLifecycleManager/#removeexecutorfromk8s","text":"","title":"removeExecutorFromK8s <pre><code>removeExecutorFromK8s(\n  execId: Long,\n  updatedPod: Pod): Unit\n</code></pre> <p><code>removeExecutorFromK8s</code>...FIXME</p>"},{"location":"ExecutorPodsLifecycleManager/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"ExecutorPodsPollingSnapshotSource/","text":"<p><code>ExecutorPodsPollingSnapshotSource</code> manages pollingFuture (for PollRunnables) to synchronize executor pods state every spark.kubernetes.executor.apiPollingInterval (by requesting the ExecutorPodsSnapshotsStore to replace executor pods snapshot).</p>","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#creating-instance","text":"<p><code>ExecutorPodsPollingSnapshotSource</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> <li> <code>KubernetesClient</code> <li> ExecutorPodsSnapshotsStore <li> Java ScheduledExecutorService  <p><code>ExecutorPodsPollingSnapshotSource</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend)</li> </ul> <p></p>","title":"Creating Instance"},{"location":"ExecutorPodsPollingSnapshotSource/#sparkkubernetesexecutorapipollinginterval","text":"","title":"spark.kubernetes.executor.apiPollingInterval <p><code>ExecutorPodsPollingSnapshotSource</code> uses spark.kubernetes.executor.apiPollingInterval configuration property when started (to schedule a PollRunnable for regular executor pod state synchronization).</p>"},{"location":"ExecutorPodsPollingSnapshotSource/#pollingfuture","text":"","title":"pollingFuture <pre><code>pollingFuture: Future[_]\n</code></pre> <p><code>pollingFuture</code>...FIXME</p>"},{"location":"ExecutorPodsPollingSnapshotSource/#starting","text":"","title":"Starting <pre><code>start(\n  applicationId: String): Unit\n</code></pre> <p></p> <p><code>start</code> prints out the following DEBUG message to the logs (with the pollingInterval):</p> <pre><code>Starting to check for executor pod state every [pollingInterval] ms.\n</code></pre> <p><code>start</code> throws an <code>IllegalArgumentException</code> when started twice (i.e. pollingFuture has already been initialized):</p> <pre><code>Cannot start polling more than once.\n</code></pre> <p><code>start</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClusterSchedulerBackend</code> is requested to start</li> </ul>"},{"location":"ExecutorPodsPollingSnapshotSource/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"ExecutorPodsSnapshot/","text":"<p><code>ExecutorPodsSnapshot</code> is an immutable view (snapshot) of the executor pods running in a Kubernetes cluster.</p> <p><code>ExecutorPodsSnapshot</code> tracks the state of executor pods and can be updated partially or fully.</p>","title":"ExecutorPodsSnapshot"},{"location":"ExecutorPodsSnapshot/#creating-instance","text":"<p><code>ExecutorPodsSnapshot</code> takes the following to be created:</p> <ul> <li> <code>ExecutorPodState</code> by Executor ID (<code>Map[Long, ExecutorPodState]</code>) <li>Full Snapshot Timestamp</li>  <p><code>ExecutorPodsSnapshot</code> is created (indirectly using apply utility)\u00a0when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is created</li> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is created and requested to replace a snapshot</li> <li><code>ExecutorPodsSnapshot</code> is requested to update</li> </ul>","title":"Creating Instance"},{"location":"ExecutorPodsSnapshot/#full-snapshot-timestamp","text":"","title":"Full Snapshot Timestamp <p><code>ExecutorPodsSnapshot</code> keeps track of the time of the full snapshot of executor pods.</p> <p>The time is given (via apply) when:</p> <ul> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to replace a snapshot (and basically do a full synchronization)</li> </ul>"},{"location":"ExecutorPodsSnapshot/#shouldcheckallcontainers-flag","text":"","title":"shouldCheckAllContainers Flag <pre><code>shouldCheckAllContainers: Boolean\n</code></pre> <p><code>ExecutorPodsSnapshot</code> uses <code>shouldCheckAllContainers</code> internal flag to control whether or not to include all containers in a running pod when requested for the ExecutorPodState.</p> <p><code>shouldCheckAllContainers</code> is controlled by spark.kubernetes.executor.checkAllContainers configuration property (when <code>KubernetesClusterManager</code> is requested for a SchedulerBackend).</p>"},{"location":"ExecutorPodsSnapshot/#apply-utility","text":"","title":"apply Utility <pre><code>apply(): ExecutorPodsSnapshot\napply(\n  executorPods: Seq[Pod],\n  fullSnapshotTs: Long): ExecutorPodsSnapshot\n</code></pre> <p><code>apply</code> creates a ExecutorPodsSnapshot with the given arguments.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is created</li> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is created and requested to replace a snapshot</li> </ul>"},{"location":"ExecutorPodsSnapshot/#updating-executorpodssnapshot","text":"","title":"Updating ExecutorPodsSnapshot <pre><code>withUpdate(\n  updatedPod: Pod): ExecutorPodsSnapshot\n</code></pre> <p><code>withUpdate</code> creates a new ExecutorPodsSnapshot with the executorPods updated based on the given executor pod update (converted).</p> <p><code>withUpdate</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to updatePod</li> </ul>"},{"location":"ExecutorPodsSnapshot/#tostatesbyexecutorid","text":"","title":"toStatesByExecutorId <pre><code>toStatesByExecutorId(\n  executorPods: Seq[Pod]): Map[Long, ExecutorPodState]\n</code></pre> <p><code>toStatesByExecutorId</code>...FIXME</p> <p><code>toStatesByExecutorId</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsSnapshot</code> is requested to withUpdate and apply</li> </ul>"},{"location":"ExecutorPodsSnapshot/#tostate","text":"","title":"toState <pre><code>toState(\n  pod: Pod): ExecutorPodState\n</code></pre> <p><code>toState</code>...FIXME</p>"},{"location":"ExecutorPodsSnapshot/#isdeleted","text":"","title":"isDeleted <pre><code>isDeleted(\n  pod: Pod): Boolean\n</code></pre> <p><code>isDeleted</code>...FIXME</p>"},{"location":"ExecutorPodsSnapshotsStore/","text":"<p><code>ExecutorPodsSnapshotsStore</code> is an abstraction of executor pods snapshots stores that subscribers can subscribe to and be notified about single pod or full snapshots updates.</p>","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#contract","text":"","title":"Contract"},{"location":"ExecutorPodsSnapshotsStore/#registering-subscriber","text":"","title":"Registering Subscriber <pre><code>addSubscriber(\n  processBatchIntervalMillis: Long)(\n  onNewSnapshots: Seq[ExecutorPodsSnapshot] =&gt; Unit): Unit\n</code></pre> <p>Registers a new subscriber to be notified about new ExecutorPodsSnapshots every <code>processBatchIntervalMillis</code> interval</p> <p>Used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to start</li> <li><code>ExecutorPodsLifecycleManager</code> is requested to start</li> </ul>"},{"location":"ExecutorPodsSnapshotsStore/#notifying-subscribers","text":"","title":"Notifying Subscribers <pre><code>notifySubscribers(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to change the total expected executors</li> </ul>"},{"location":"ExecutorPodsSnapshotsStore/#full-executor-pod-state-synchronization","text":"","title":"Full Executor Pod State Synchronization <pre><code>replaceSnapshot(\n  newSnapshot: Seq[Pod]): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>PollRunnable</code> is requested to start (every spark.kubernetes.executor.apiPollingInterval when <code>ExecutorPodsPollingSnapshotSource</code> is started)</li> </ul>"},{"location":"ExecutorPodsSnapshotsStore/#stopping","text":"","title":"Stopping <pre><code>stop(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>KubernetesClusterSchedulerBackend</code> is requested to stop</li> </ul>"},{"location":"ExecutorPodsSnapshotsStore/#single-executor-pod-state-update","text":"","title":"Single Executor Pod State Update <pre><code>updatePod(\n  updatedPod: Pod): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ExecutorPodsWatcher</code> is requested to eventReceived</li> </ul>"},{"location":"ExecutorPodsSnapshotsStore/#implementations","text":"<ul> <li>ExecutorPodsSnapshotsStoreImpl</li> </ul>","title":"Implementations"},{"location":"ExecutorPodsSnapshotsStoreImpl/","text":"<p><code>ExecutorPodsSnapshotsStoreImpl</code> is an ExecutorPodsSnapshotsStore.</p>","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#creating-instance","text":"<p><code>ExecutorPodsSnapshotsStoreImpl</code> takes the following to be created:</p> <ul> <li> Java's ScheduledExecutorService  <p><code>ExecutorPodsSnapshotsStoreImpl</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>","title":"Creating Instance"},{"location":"ExecutorPodsSnapshotsStoreImpl/#replacesnapshot","text":"","title":"replaceSnapshot <pre><code>replaceSnapshot(\n  newSnapshot: Seq[Pod]): Unit\n</code></pre> <p><code>replaceSnapshot</code>\u00a0is part of the ExecutorPodsSnapshotsStore abstraction.</p> <p><code>replaceSnapshot</code> replaces the currentSnapshot internal registry with a new ExecutorPodsSnapshot (with the given snapshot and the current time).</p> <p>In the end, <code>updatePod</code> addCurrentSnapshotToSubscribers.</p>"},{"location":"ExecutorPodsSnapshotsStoreImpl/#updatepod","text":"","title":"updatePod <pre><code>updatePod(\n  updatedPod: Pod): Unit\n</code></pre> <p><code>updatePod</code>\u00a0is part of the ExecutorPodsSnapshotsStore abstraction.</p> <p><code>updatePod</code> requests the current ExecutorPodsSnapshot to update based on the given updated pod.</p> <p>In the end, <code>updatePod</code> addCurrentSnapshotToSubscribers.</p>"},{"location":"ExecutorPodsSnapshotsStoreImpl/#registering-subscriber","text":"","title":"Registering Subscriber <pre><code>addSubscriber(\n  processBatchIntervalMillis: Long)\n  (onNewSnapshots: Seq[ExecutorPodsSnapshot] =&gt; Unit): Unit\n</code></pre> <p><code>addSubscriber</code>\u00a0is part of the ExecutorPodsSnapshotsStore abstraction.</p> <p><code>addSubscriber</code> adds a new <code>SnapshotsSubscriber</code> to the subscribers internal registry.</p> <p><code>addSubscriber</code>\u00a0requests the ScheduledExecutorService to schedule processing executor pods by the <code>SnapshotsSubscriber</code> every given <code>processBatchIntervalMillis</code> delay (starting immediately).</p> <p>In the end, <code>addSubscriber</code>\u00a0adds the scheduled action to the pollingTasks internal registry.</p>"},{"location":"ExecutorPodsSnapshotsStoreImpl/#callsubscriber","text":"","title":"callSubscriber <pre><code>callSubscriber(\n  subscriber: SnapshotsSubscriber): Unit\n</code></pre> <p><code>callSubscriber</code>...FIXME</p> <p><code>callSubscriber</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to addSubscriber and notifySubscribers</li> </ul>"},{"location":"ExecutorPodsSnapshotsStoreImpl/#pollingtasks-registry","text":"","title":"pollingTasks Registry <pre><code>pollingTasks: CopyOnWriteArrayList[Future[_]]\n</code></pre> <p><code>ExecutorPodsSnapshotsStoreImpl</code> uses <code>pollingTasks</code> internal registry to track the recurring actions scheduled for subscribers.</p> <p><code>pollingTasks</code>\u00a0are cancelled when <code>ExecutorPodsSnapshotsStoreImpl</code> is requested to stop.</p>"},{"location":"ExecutorPodsSnapshotsStoreImpl/#subscribers-registry","text":"","title":"subscribers Registry <pre><code>subscribers: CopyOnWriteArrayList[SnapshotsSubscriber]\n</code></pre> <p><code>ExecutorPodsSnapshotsStoreImpl</code> uses <code>subscribers</code> internal registry to track subscribers that want to be notified regularly about the current state of executor pods in a cluster.</p>"},{"location":"ExecutorPodsSnapshotsStoreImpl/#addcurrentsnapshottosubscribers","text":"","title":"addCurrentSnapshotToSubscribers <pre><code>addCurrentSnapshotToSubscribers(): Unit\n</code></pre> <p><code>addCurrentSnapshotToSubscribers</code> requests every SnapshotsSubscriber to addCurrentSnapshot.</p> <p><code>addCurrentSnapshotToSubscribers</code> is used when:</p> <ul> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to updatePod and replaceSnapshot</li> </ul>"},{"location":"ExecutorPodsWatchSnapshotSource/","text":"<p><code>ExecutorPodsWatchSnapshotSource</code> is given KubernetesClient to a Kubernetes API server to watch for status updates of the executor pods of a given Spark application when started (that ExecutorPodsWatcher passes along to the ExecutorPodsSnapshotsStore).</p>","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#creating-instance","text":"<p><code>ExecutorPodsWatchSnapshotSource</code> takes the following to be created:</p> <ul> <li> ExecutorPodsSnapshotsStore <li> <code>KubernetesClient</code>  <p><code>ExecutorPodsWatchSnapshotSource</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>","title":"Creating Instance"},{"location":"ExecutorPodsWatchSnapshotSource/#watchconnection","text":"","title":"watchConnection <pre><code>watchConnection: Closeable\n</code></pre> <p><code>ExecutorPodsWatchSnapshotSource</code> defines <code>watchConnection</code> internal registry to be a \"watch connection\" to a Kubernetes API server to watch any status updates of the executor pods of a given Spark application (using ExecutorPodsWatcher).</p> <p><code>ExecutorPodsWatchSnapshotSource</code> uses <code>watchConnection</code> internal registry as an indication of whether it has been started already or not (and throws an <code>IllegalArgumentException</code> when it has).</p> <p><code>ExecutorPodsWatchSnapshotSource</code> requests the <code>watchConnection</code> to close and <code>null</code>s it when requested to stop.</p>"},{"location":"ExecutorPodsWatchSnapshotSource/#starting","text":"","title":"Starting <pre><code>start(\n  applicationId: String): Unit\n</code></pre> <p><code>start</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor.\n</code></pre> <p><code>start</code> requests the KubernetesClient to watch pods with the following labels and values and pass pod updates to ExecutorPodsWatcher.</p>    Label Name Value     <code>spark-app-selector</code> the given <code>applicationId</code>   <code>spark-role</code> <code>executor</code>    <p><code>start</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClusterSchedulerBackend</code> is requested to start</li> </ul>"},{"location":"ExecutorPodsWatchSnapshotSource/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"ExecutorPodsWatcher/","text":"<p><code>ExecutorPodsWatcher</code> is a Kubernetes' <code>Watcher[Pod]</code> to be notified about executor pod updates (and update the ExecutorPodsSnapshotsStore of the parent ExecutorPodsWatchSnapshotSource).</p> <p><code>ExecutorPodsWatcher</code> is an internal class of ExecutorPodsWatchSnapshotSource with full access to its internals.</p>","title":"ExecutorPodsWatcher"},{"location":"ExecutorPodsWatcher/#creating-instance","text":"<p><code>ExecutorPodsWatcher</code> takes no arguments to be created.</p> <p><code>ExecutorPodsWatcher</code> is created\u00a0when:</p> <ul> <li><code>ExecutorPodsWatchSnapshotSource</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"ExecutorPodsWatcher/#executor-pod-update","text":"","title":"Executor Pod Update <pre><code>eventReceived(\n  action: Action,\n  pod: Pod): Unit\n</code></pre> <p><code>eventReceived</code>\u00a0is part of the Kubernetes Client <code>Watcher</code> abstraction.</p> <p><code>eventReceived</code> prints out the following DEBUG message to the logs:</p> <pre><code>Received executor pod update for pod named [podName], action [action]\n</code></pre> <p><code>eventReceived</code> requests the ExecutorPodsSnapshotsStore (of the parent ExecutorPodsWatchSnapshotSource) to updatePod.</p>"},{"location":"ExecutorPodsWatcher/#close-notification","text":"","title":"Close Notification <pre><code>onClose(\n  e: KubernetesClientException): Unit\n</code></pre> <p><code>onClose</code>\u00a0is part of the Kubernetes Client <code>Watcher</code> abstraction.</p> <p><code>onClose</code> prints out the following WARN message to the logs:</p> <pre><code>Kubernetes client has been closed (this is expected if the application is shutting down.)\n</code></pre>"},{"location":"ExecutorPodsWatcher/#logging","text":"","title":"Logging <p><code>ExecutorPodsWatcher</code> uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger for logging.</p>"},{"location":"K8SSparkSubmitOperation/","text":"<p><code>K8SSparkSubmitOperation</code> is an extension of <code>spark-submit</code> (Apache Spark) for Spark on Kubernetes to support the operations:</p> <ul> <li>kill</li> <li>status</li> </ul> <p><code>K8SSparkSubmitOperation</code> is registered with Apache Spark using <code>META-INF/services/org.apache.spark.deploy.SparkSubmitOperation</code> service file.</p>","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#killing-submission","text":"","title":"Killing Submission <pre><code>kill(\n  submissionId: String,\n  conf: SparkConf): Unit\n</code></pre> <p><code>kill</code>\u00a0is part of the <code>SparkSubmitOperation</code> (Apache Spark) abstraction.</p> <p><code>kill</code> prints out the following message to standard error:</p> <pre><code>Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set].\n</code></pre> <p><code>kill</code> creates a <code>KillApplication</code> to execute it (with the input <code>submissionId</code> and <code>SparkConf</code>).</p>"},{"location":"K8SSparkSubmitOperation/#displaying-submission-status","text":"","title":"Displaying Submission Status <pre><code>printSubmissionStatus(\n  submissionId: String,\n  conf: SparkConf): Unit\n</code></pre> <p><code>printSubmissionStatus</code>\u00a0is part of the <code>SparkSubmitOperation</code> (Apache Spark) abstraction.</p> <p><code>printSubmissionStatus</code> prints out the following message to standard error:</p> <pre><code>Submitting a request for the status of submission [submissionId] in [spark.master].\n</code></pre> <p><code>printSubmissionStatus</code> creates a <code>ListStatus</code> to execute it (with the input <code>submissionId</code> and <code>SparkConf</code>).</p>"},{"location":"K8SSparkSubmitOperation/#checking-whether-master-url-supported","text":"","title":"Checking Whether Master URL Supported <pre><code>supports(\n  master: String): Boolean\n</code></pre> <p><code>supports</code>\u00a0is part of the <code>SparkSubmitOperation</code> (Apache Spark) abstraction.</p> <p><code>supports</code> is <code>true</code> when the input <code>master</code> starts with k8s:// prefix.</p>"},{"location":"K8SSparkSubmitOperation/#executing-operation","text":"","title":"Executing Operation <pre><code>execute(\n  submissionId: String,\n  sparkConf: SparkConf,\n  op: K8sSubmitOp): Unit\n</code></pre> <p><code>execute</code>\u00a0is used for kill and printSubmissionStatus.</p> <p><code>execute</code> parses the master URL (based on <code>spark.master</code> configuration property).</p> <p><code>execute</code> uses <code>:</code> (the colon) to split the given <code>submissionId</code> to at most two parts - an optional namespace and a driver pod name. The driver pod name can use a glob pattern as <code>*</code> (the star).</p> <p><code>execute</code> creates a KubernetesClient (with <code>Submission</code> client type).</p> <p>If the pod name uses the glob pattern (with <code>*</code>), <code>execute</code> requests the <code>KubernetesClient</code> for the driver pods (pods with the <code>spark-role</code> label with <code>driver</code> value) that it then hands over to the given <code>K8sSubmitOp</code> to <code>executeOnGlob</code> (with the optional namespace).</p> <p>Otherwise, <code>execute</code> requests the given <code>K8sSubmitOp</code> to <code>executeOnPod</code> with the pod and the optional namespace.</p> <p><code>execute</code> prints out the following message and exits when the given <code>submissionId</code> cannot be split on <code>:</code> to two parts:</p> <pre><code>Submission ID: {[submissionId]} is invalid.\n</code></pre>"},{"location":"KubernetesClientApplication/","text":"<p><code>KubernetesClientApplication</code> is a <code>SparkApplication</code> (Apache Spark) in Spark on Kubernetes in <code>cluster</code> deploy mode.</p>","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#creating-instance","text":"<p><code>KubernetesClientApplication</code> takes no arguments to be created.</p> <p><code>KubernetesClientApplication</code> is created\u00a0when:</p> <ul> <li><code>SparkSubmit</code> is requested to launch a Spark application (for kubernetes in cluster deploy mode)</li> </ul>","title":"Creating Instance"},{"location":"KubernetesClientApplication/#starting-spark-application","text":"","title":"Starting Spark Application <pre><code>start(\n  args: Array[String],\n  conf: SparkConf): Unit\n</code></pre> <p><code>start</code>\u00a0is part of the <code>SparkApplication</code> (Apache Spark) abstraction.</p> <p><code>start</code> parses the command-line arguments (<code>args</code>) and runs.</p>"},{"location":"KubernetesClientApplication/#run","text":"","title":"run <pre><code>run(\n  clientArguments: ClientArguments,\n  sparkConf: SparkConf): Unit\n</code></pre> <p><code>run</code> generates a custom Spark Application ID of the format:</p> <pre><code>spark-[randomUUID-without-dashes]\n</code></pre> <p><code>run</code> creates a KubernetesDriverConf (with the given ClientArguments, <code>SparkConf</code> and the custom Spark Application ID).</p> <p><code>run</code> removes the k8s:// prefix from the <code>spark.master</code> configuration property (which has already been validated by <code>SparkSubmit</code> itself).</p> <p><code>run</code> creates a LoggingPodStatusWatcherImpl (with the <code>KubernetesDriverConf</code>).</p> <p><code>run</code> creates a KubernetesClient (with the master URL, the namespace, and others).</p> <p>In the end, <code>run</code> creates a Client (with the KubernetesDriverConf, a new KubernetesDriverBuilder, the <code>KubernetesClient</code>, and the LoggingPodStatusWatcherImpl) and requests it to run.</p>"},{"location":"KubernetesClientUtils/","text":"","title":"KubernetesClientUtils Utility"},{"location":"KubernetesClientUtils/#name-of-config-map-for-executors","text":"","title":"Name of Config Map for Executors <pre><code>configMapNameExecutor: String\n</code></pre> <p><code>KubernetesClientUtils</code> defines the name of a config map for executors as follows:</p> <pre><code>spark-exec-[uniqueID]-conf-map\n</code></pre> <p><code>spark-exec-[uniqueID]</code> is made under 54 characters to let <code>configMapNameExecutor</code> be at 63 characters maximum.</p>"},{"location":"KubernetesClientUtils/#buildsparkconfdirfilesmap","text":"","title":"buildSparkConfDirFilesMap <pre><code>buildSparkConfDirFilesMap(\n  configMapName: String,\n  sparkConf: SparkConf,\n  resolvedPropertiesMap: Map[String, String]): Map[String, String]\n</code></pre> <p><code>buildSparkConfDirFilesMap</code>...FIXME</p> <p><code>buildSparkConfDirFilesMap</code>\u00a0is used when:</p> <ul> <li><code>BasicExecutorFeatureStep</code> is requested to configure a pod</li> <li><code>Client</code> is requested to run</li> <li><code>KubernetesClusterSchedulerBackend</code> is requested to setUpExecutorConfigMap</li> </ul>"},{"location":"KubernetesClientUtils/#loadsparkconfdirfiles","text":"","title":"loadSparkConfDirFiles <pre><code>loadSparkConfDirFiles(\n  conf: SparkConf): Map[String, String]\n</code></pre> <p><code>loadSparkConfDirFiles</code>...FIXME</p>"},{"location":"KubernetesClusterManager/","text":"<p> <code>KubernetesClusterManager</code> is an <code>ExternalClusterManager</code> (Apache Spark) that can create scheduler components for k8s master URLs: <ul> <li>TaskSchedulerImpl</li> <li>KubernetesClusterSchedulerBackend</li> </ul> <p><code>KubernetesClusterManager</code> is registered with Apache Spark using <code>META-INF/services/org.apache.spark.scheduler.ExternalClusterManager</code> service file.</p>","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#creating-instance","text":"<p><code>KubernetesClusterManager</code> takes no arguments to be created.</p> <p><code>KubernetesClusterManager</code> is created\u00a0when:</p> <ul> <li><code>SparkContext</code> is requested for an <code>ExternalClusterManager</code> (when requested for a SchedulerBackend and TaskScheduler)</li> </ul>","title":"Creating Instance"},{"location":"KubernetesClusterManager/#creating-schedulerbackend","text":"","title":"Creating SchedulerBackend <pre><code>createSchedulerBackend(\n  sc: SparkContext,\n  masterURL: String,\n  scheduler: TaskScheduler): SchedulerBackend\n</code></pre> <p><code>createSchedulerBackend</code>\u00a0is part of the <code>ExternalClusterManager</code> (Apache Spark) abstraction.</p> <p><code>createSchedulerBackend</code> creates a KubernetesClusterSchedulerBackend.</p>  <p>Note</p> <p><code>createSchedulerBackend</code> assumes that the given <code>TaskScheduler</code> is <code>TaskSchedulerImpl</code> (Apache Spark).</p>  <p><code>createSchedulerBackend</code> determines four internal values based on the spark.kubernetes.submitInDriver internal configuration property.</p>      spark.kubernetes.submitInDriver         Enabled (<code>true</code>) Disabled (<code>false</code>)    authConfPrefix <code>spark.kubernetes.authenticate.driver.mounted</code> <code>spark.kubernetes.authenticate</code>    apiServerUri spark.kubernetes.driver.master Master URL with no k8s:// prefix    defaultServiceAccountToken <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>      defaultServiceAccountCaCrt <code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code>      <p>Unless already defined, <code>createSchedulerBackend</code> sets the spark.kubernetes.executor.podNamePrefix configuration properties based on spark.app.name prefix.</p> <p><code>createSchedulerBackend</code> creates a KubernetesClient for the <code>Driver</code> client type and the following:</p> <ul> <li>spark.kubernetes.namespace configuration property</li> <li>apiServerUri</li> <li>authConfPrefix</li> <li>defaultServiceAccountToken</li> <li>defaultServiceAccountCaCrt</li> </ul> <p>With spark.kubernetes.executor.podTemplateFile configuration property enabled, <code>createSchedulerBackend</code> loads the pod spec from the pod template file with the optional spark.kubernetes.executor.podTemplateContainerName configuration property.</p> <p>In the end, <code>createSchedulerBackend</code> creates a KubernetesClusterSchedulerBackend with the following:</p> <ul> <li> <p>Java <code>ScheduledExecutorService</code> with kubernetes-executor-maintenance thread name</p> </li> <li> <p>ExecutorPodsSnapshotsStoreImpl with a Java <code>ScheduledExecutorService</code> with kubernetes-executor-snapshots-subscribers thread names and 2 threads</p> </li> <li> <p>ExecutorPodsLifecycleManager</p> </li> <li> <p>ExecutorPodsAllocator</p> </li> <li> <p>ExecutorPodsWatchSnapshotSource</p> </li> <li> <p>ExecutorPodsPollingSnapshotSource with a Java <code>ScheduledExecutorService</code> with kubernetes-executor-pod-polling-sync thread name</p> </li> </ul>"},{"location":"KubernetesClusterManager/#illegalargumentexception","text":"<p>With <code>spark.kubernetes.submitInDriver</code> enabled, <code>createSchedulerBackend</code> asserts that the name of the driver pod is configured (using spark.kubernetes.driver.pod.name configuration property) or else throws an <code>IllegalArgumentException</code>:</p> <pre><code>If the application is deployed using spark-submit in cluster mode, the driver pod name must be provided.\n</code></pre>","title":"IllegalArgumentException"},{"location":"KubernetesClusterManager/#creating-taskscheduler","text":"","title":"Creating TaskScheduler <pre><code>createTaskScheduler(\n  sc: SparkContext,\n  masterURL: String): TaskScheduler\n</code></pre> <p><code>createTaskScheduler</code>\u00a0is part of the <code>ExternalClusterManager</code> (Apache Spark) abstraction.</p> <p><code>createTaskScheduler</code> creates a <code>TaskSchedulerImpl</code> (Apache Spark).</p>"},{"location":"KubernetesClusterManager/#initializing-scheduling-components","text":"","title":"Initializing Scheduling Components <pre><code>initialize(\n  scheduler: TaskScheduler,\n  backend: SchedulerBackend): Unit\n</code></pre> <p><code>initialize</code>\u00a0is part of the <code>ExternalClusterManager</code> (Apache Spark) abstraction.</p> <p><code>initialize</code> requests the given <code>TaskSchedulerImpl</code> (Apache Spark) to initialize with the given <code>SchedulerBackend</code> (Apache Spark).</p>"},{"location":"KubernetesClusterSchedulerBackend/","text":"<p><code>KubernetesClusterSchedulerBackend</code> is a <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) for Spark on Kubernetes.</p>","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#creating-instance","text":"<p><code>KubernetesClusterSchedulerBackend</code> takes the following to be created:</p> <ul> <li> <code>TaskSchedulerImpl</code> (Apache Spark) <li> <code>SparkContext</code> (Apache Spark) <li> <code>KubernetesClient</code> <li> Java's ScheduledExecutorService <li> ExecutorPodsSnapshotsStore <li>ExecutorPodsAllocator</li> <li>ExecutorPodsLifecycleManager</li> <li> ExecutorPodsWatchSnapshotSource <li> ExecutorPodsPollingSnapshotSource  <p><code>KubernetesClusterSchedulerBackend</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>","title":"Creating Instance"},{"location":"KubernetesClusterSchedulerBackend/#executorpodslifecyclemanager","text":"","title":"ExecutorPodsLifecycleManager <p><code>KubernetesClusterSchedulerBackend</code> is given an ExecutorPodsLifecycleManager to be created.</p> <p><code>KubernetesClusterSchedulerBackend</code> requests the <code>ExecutorPodsLifecycleManager</code> to start (with itself) when started.</p>"},{"location":"KubernetesClusterSchedulerBackend/#executorpodsallocator","text":"","title":"ExecutorPodsAllocator <p><code>KubernetesClusterSchedulerBackend</code> is given an ExecutorPodsAllocator to be created.</p> <p>When started, <code>KubernetesClusterSchedulerBackend</code> requests the <code>ExecutorPodsAllocator</code> to setTotalExpectedExecutors to the number of initial executors and starts it with application Id.</p> <p>When requested for the expected number of executors, <code>KubernetesClusterSchedulerBackend</code> requests the <code>ExecutorPodsAllocator</code> to setTotalExpectedExecutors to the given total number of executors.</p> <p>When requested to isBlacklisted, <code>KubernetesClusterSchedulerBackend</code> requests the <code>ExecutorPodsAllocator</code> to isDeleted with a given executor.</p>"},{"location":"KubernetesClusterSchedulerBackend/#initial-executors","text":"","title":"Initial Executors <pre><code>initialExecutors: Int\n</code></pre> <p><code>KubernetesClusterSchedulerBackend</code> calculates the initial target number of executors when created.</p> <p><code>initialExecutors</code> is used when <code>KubernetesClusterSchedulerBackend</code> is requested to start and whether or not sufficient resources registered.</p>"},{"location":"KubernetesClusterSchedulerBackend/#application-id","text":"","title":"Application Id <pre><code>applicationId(): String\n</code></pre> <p><code>applicationId</code> is part of the <code>SchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>applicationId</code> is the value of <code>spark.app.id</code> configuration property if defined or the default <code>applicationId</code>.</p>"},{"location":"KubernetesClusterSchedulerBackend/#sufficient-resources-registered","text":"","title":"Sufficient Resources Registered <pre><code>sufficientResourcesRegistered(): Boolean\n</code></pre> <p><code>sufficientResourcesRegistered</code> is part of the <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>sufficientResourcesRegistered</code> holds (is <code>true</code>) when the <code>totalRegisteredExecutors</code> is at least the ratio of the initial executors.</p>"},{"location":"KubernetesClusterSchedulerBackend/#minimum-resources-available-ratio","text":"","title":"Minimum Resources Available Ratio <pre><code>minRegisteredRatio: Double\n</code></pre> <p><code>minRegisteredRatio</code> is part of the <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>minRegisteredRatio</code> is <code>0.8</code> unless <code>spark.scheduler.minRegisteredResourcesRatio</code> configuration property is defined.</p>"},{"location":"KubernetesClusterSchedulerBackend/#starting-schedulerbackend","text":"","title":"Starting SchedulerBackend <pre><code>start(): Unit\n</code></pre> <p><code>start</code>\u00a0is part of the <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>start</code> creates a delegation token manager.</p> <p><code>start</code> requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors.</p> <p><code>start</code> requests the ExecutorPodsLifecycleManager to start (with this <code>KubernetesClusterSchedulerBackend</code>).</p> <p><code>start</code> requests the ExecutorPodsAllocator to start (with the applicationId)</p> <p><code>start</code> requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId)</p> <p><code>start</code> requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId)</p> <p>In the end, <code>start</code> setUpExecutorConfigMap.</p>"},{"location":"KubernetesClusterSchedulerBackend/#setupexecutorconfigmap","text":"","title":"setUpExecutorConfigMap <pre><code>setUpExecutorConfigMap(): Unit\n</code></pre> <p><code>setUpExecutorConfigMap</code> takes the Name of Config Map for Executors and buildSparkConfDirFilesMap (with the <code>SparkConf</code>).</p> <p><code>setUpExecutorConfigMap</code> buildConfigMap with the labels (and the name of the config map and the configuration files).</p>    Name Value     <code>spark-app-selector</code> Application Id   <code>spark-role</code> <code>executor</code>    <p>In the end, <code>setUpExecutorConfigMap</code> requests the KubernetesClient to create a new config map.</p>"},{"location":"KubernetesClusterSchedulerBackend/#creating-driverendpoint","text":"","title":"Creating DriverEndpoint <pre><code>createDriverEndpoint(): DriverEndpoint\n</code></pre> <p><code>createDriverEndpoint</code>\u00a0is part of the <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>createDriverEndpoint</code> creates a KubernetesDriverEndpoint.</p>"},{"location":"KubernetesClusterSchedulerBackend/#requesting-executors-from-cluster-manager","text":"","title":"Requesting Executors from Cluster Manager <pre><code>doRequestTotalExecutors(\n  requestedTotal: Int): Future[Boolean]\n</code></pre> <p><code>doRequestTotalExecutors</code>\u00a0is part of the <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>doRequestTotalExecutors</code> requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given <code>requestedTotal</code>.</p> <p>In the end, <code>doRequestTotalExecutors</code> returns a completed <code>Future</code> with <code>true</code> value.</p>"},{"location":"KubernetesClusterSchedulerBackend/#stopping-schedulerbackend","text":"","title":"Stopping SchedulerBackend <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code>\u00a0is part of the <code>CoarseGrainedSchedulerBackend</code> (Apache Spark) abstraction.</p> <p><code>stop</code>...FIXME</p>"},{"location":"KubernetesConf/","text":"<p><code>KubernetesConf</code> is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors).</p>","title":"KubernetesConf"},{"location":"KubernetesConf/#contract","text":"","title":"Contract"},{"location":"KubernetesConf/#annotations","text":"","title":"annotations <pre><code>annotations: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to configurePod</li> <li><code>BasicExecutorFeatureStep</code> is requested to configurePod</li> </ul>"},{"location":"KubernetesConf/#environment","text":"","title":"environment <pre><code>environment: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to configurePod</li> <li><code>BasicExecutorFeatureStep</code> is requested to configurePod</li> </ul>"},{"location":"KubernetesConf/#labels","text":"","title":"labels <pre><code>labels: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to configurePod</li> <li><code>BasicExecutorFeatureStep</code> is requested to configurePod</li> <li><code>DriverServiceFeatureStep</code> is requested to getAdditionalKubernetesResources</li> </ul>"},{"location":"KubernetesConf/#resourcenameprefix","text":"","title":"resourceNamePrefix <pre><code>resourceNamePrefix: String\n</code></pre> <p>Prefix of resource names</p>"},{"location":"KubernetesConf/#secretenvnamestokeyrefs","text":"","title":"secretEnvNamesToKeyRefs <pre><code>secretEnvNamesToKeyRefs: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>EnvSecretsFeatureStep</code> is requested to configurePod</li> </ul>"},{"location":"KubernetesConf/#secretnamestomountpaths","text":"","title":"secretNamesToMountPaths <pre><code>secretNamesToMountPaths: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>MountSecretsFeatureStep</code> is requested to configurePod</li> </ul>"},{"location":"KubernetesConf/#volumes","text":"","title":"Volumes <pre><code>volumes: Seq[KubernetesVolumeSpec]\n</code></pre> <p>KubernetesVolumeSpecs</p> <p>Used when:</p> <ul> <li><code>MountVolumesFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"KubernetesConf/#implementations","text":"<ul> <li>KubernetesDriverConf</li> <li>KubernetesExecutorConf</li> </ul>","title":"Implementations"},{"location":"KubernetesConf/#creating-instance","text":"<p><code>KubernetesConf</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code>  Abstract Class<p><code>KubernetesConf</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs.</p>","title":"Creating Instance"},{"location":"KubernetesConf/#namespace","text":"","title":"Namespace <pre><code>namespace: String\n</code></pre> <p><code>namespace</code> is the value of spark.kubernetes.namespace configuration property.</p> <p><code>namespace</code>\u00a0is used when:</p> <ul> <li><code>DriverServiceFeatureStep</code> is requested to getAdditionalPodSystemProperties</li> <li><code>Client</code> is requested to run</li> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>"},{"location":"KubernetesConf/#imagepullpolicy","text":"","title":"imagePullPolicy <pre><code>imagePullPolicy: String\n</code></pre> <p><code>imagePullPolicy</code> is the value of spark.kubernetes.container.image.pullPolicy configuration property.</p> <p><code>imagePullPolicy</code>\u00a0is used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to configure a pod</li> <li><code>BasicExecutorFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"KubernetesConf/#creating-kubernetesdriverconf","text":"","title":"Creating KubernetesDriverConf <pre><code>createDriverConf(\n  sparkConf: SparkConf,\n  appId: String,\n  mainAppResource: MainAppResource,\n  mainClass: String,\n  appArgs: Array[String]): KubernetesDriverConf\n</code></pre>  <p>Note</p> <p>The goal of <code>createDriverConf</code> is to validate executor volumes before creating a KubernetesDriverConf.</p>  <p><code>createDriverConf</code> parse volumes for executors (with spark.kubernetes.executor.volumes prefix).</p>  <p>Note</p> <p><code>createDriverConf</code> parses executor volumes in order to verify configuration before the driver pod is created.</p>  <p>In the end, <code>createDriverConf</code> creates a KubernetesDriverConf.</p> <p><code>createDriverConf</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>"},{"location":"KubernetesConf/#creating-kubernetesexecutorconf","text":"","title":"Creating KubernetesExecutorConf <pre><code>createExecutorConf(\n  sparkConf: SparkConf,\n  executorId: String,\n  appId: String,\n  driverPod: Option[Pod]): KubernetesExecutorConf\n</code></pre> <p><code>createExecutorConf</code> (does nothing more but) creates a KubernetesExecutorConf for the given input arguments.</p> <p><code>createExecutorConf</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to onNewSnapshots (and requests missing executors from Kubernetes)</li> </ul>"},{"location":"KubernetesConf/#appname-based-unique-resource-name-prefix","text":"","title":"AppName-Based Unique Resource Name Prefix <pre><code>getResourceNamePrefix(\n  appName: String): String\n</code></pre> <p><code>getResourceNamePrefix</code>...FIXME</p> <p><code>getResourceNamePrefix</code> is used when:</p> <ul> <li><code>KubernetesDriverConf</code> is requested for the resourceNamePrefix</li> <li><code>KubernetesExecutorConf</code> is requested for the resourceNamePrefix</li> <li><code>KubernetesClusterManager</code> is requested to createSchedulerBackend</li> </ul>"},{"location":"KubernetesDriverBuilder/","text":"<p><code>KubernetesDriverBuilder</code> is used to build a specification of a driver pod (for a Spark application deployed in cluster deploy mode).</p> <p></p>","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#creating-instance","text":"<p><code>KubernetesDriverBuilder</code> takes no arguments to be created.</p> <p><code>KubernetesDriverBuilder</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"KubernetesDriverBuilder/#kubernetesdriverspec","text":"","title":"KubernetesDriverSpec <p><code>KubernetesDriverSpec</code> is the following:</p> <ul> <li> <code>SparkPod</code> <li> Driver Resources <li> System Properties"},{"location":"KubernetesDriverBuilder/#building-pod-spec-for-driver","text":"","title":"Building Pod Spec for Driver <pre><code>buildFromFeatures(\n  conf: KubernetesDriverConf,\n  client: KubernetesClient): KubernetesDriverSpec\n</code></pre> <p><code>buildFromFeatures</code> creates an initial driver pod specification.</p> <p>With spark.kubernetes.driver.podTemplateFile configuration property defined, <code>buildFromFeatures</code> loads it (with the given <code>KubernetesClient</code> and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification.</p> <p><code>buildFromFeatures</code> builds a KubernetesDriverSpec (with the initial driver pod specification).</p> <p>In the end, <code>buildFromFeatures</code> configures the driver pod specification (with additional system properties and additional resources) through a series of the feature steps:</p> <ul> <li>BasicDriverFeatureStep</li> <li>DriverKubernetesCredentialsFeatureStep</li> <li>DriverServiceFeatureStep</li> <li>MountSecretsFeatureStep</li> <li>EnvSecretsFeatureStep</li> <li>MountVolumesFeatureStep</li> <li>DriverCommandFeatureStep</li> <li>HadoopConfDriverFeatureStep</li> <li>KerberosConfDriverFeatureStep</li> <li>PodTemplateConfigMapStep</li> <li>LocalDirsFeatureStep</li> </ul> <p><code>buildFromFeatures</code>\u00a0is used when:</p> <ul> <li><code>Client</code> is requested to run</li> </ul>"},{"location":"KubernetesDriverConf/","text":"<p><code>KubernetesDriverConf</code> is a KubernetesConf.</p>","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#creating-instance","text":"<p><code>KubernetesDriverConf</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> <li> Application ID <li> <code>MainAppResource</code> <li> Name of the Main Class <li> Application Arguments  <p><code>KubernetesDriverConf</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start (via KubernetesConf utility)</li> </ul>","title":"Creating Instance"},{"location":"KubernetesDriverConf/#volumes","text":"","title":"Volumes <pre><code>volumes: Seq[KubernetesVolumeSpec]\n</code></pre> <p><code>volumes</code>\u00a0is part of the KubernetesConf abstraction.</p> <p><code>volumes</code> parses volume specs for the driver (with the spark.kubernetes.driver.volumes. prefix) from the SparkConf.</p>"},{"location":"KubernetesDriverEndpoint/","text":"<p><code>KubernetesDriverEndpoint</code> is a <code>DriverEndpoint</code> (Apache Spark).</p>","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#intercepting-executor-lost-event","text":"","title":"Intercepting Executor Lost Event <pre><code>onDisconnected(\n  rpcAddress: RpcAddress): Unit\n</code></pre> <p><code>onDisconnected</code>\u00a0is part of the <code>RpcEndpoint</code> (Apache Spark) abstraction.</p> <p><code>onDisconnected</code> disables the executor known by the <code>RpcAddress</code> (found in the <code>Executors by RpcAddress Registry</code> registry).</p>"},{"location":"KubernetesExecutorBuilder/","text":"<p><code>KubernetesExecutorBuilder</code> is used to build a pod spec of executors (when <code>ExecutorPodsAllocator</code> is requested to handle executor pods snapshots and finds executors to be requested from Kubernetes).</p> <p></p>","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorBuilder/#creating-instance","text":"<p><code>KubernetesExecutorBuilder</code> takes no arguments to be created (and could really be a Scala utility object).</p> <p><code>KubernetesExecutorBuilder</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested to create a SchedulerBackend (and creates a ExecutorPodsAllocator)</li> </ul>","title":"Creating Instance"},{"location":"KubernetesExecutorBuilder/#building-pod-spec-for-executors","text":"","title":"Building Pod Spec for Executors <pre><code>buildFromFeatures(\n  conf: KubernetesExecutorConf,\n  secMgr: SecurityManager,\n  client: KubernetesClient): SparkPod\n</code></pre> <p>When defined, <code>buildFromFeatures</code> loads the pod spec from the pod template file (based on the spark.kubernetes.executor.podTemplateFile and spark.kubernetes.executor.podTemplateContainerName configuration properties). Otherwise, <code>buildFromFeatures</code> starts from an initial empty pod specification.</p> <p>In the end, <code>buildFromFeatures</code> configures the executor pod specification through a series of the feature steps:</p> <ul> <li>BasicExecutorFeatureStep</li> <li>ExecutorKubernetesCredentialsFeatureStep</li> <li>MountSecretsFeatureStep</li> <li>EnvSecretsFeatureStep</li> <li>MountVolumesFeatureStep</li> <li>LocalDirsFeatureStep</li> </ul> <p><code>buildFromFeatures</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to handle executor pods snapshots (and requests missing executors from Kubernetes)</li> </ul>"},{"location":"KubernetesExecutorConf/","text":"<p><code>KubernetesExecutorConf</code> is a KubernetesConf (for KubernetesExecutorBuilder to build a pod spec for executors).</p> <p></p>","title":"KubernetesExecutorConf"},{"location":"KubernetesExecutorConf/#creating-instance","text":"<p><code>KubernetesExecutorConf</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> <li> Application ID <li> Executor ID <li> Driver Pod  <p><code>KubernetesExecutorConf</code> is created\u00a0when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to handle executor pods snapshots (and requests missing executors from Kubernetes via KubernetesConf utility)</li> </ul>","title":"Creating Instance"},{"location":"KubernetesExecutorConf/#volumes","text":"","title":"Volumes <pre><code>volumes: Seq[KubernetesVolumeSpec]\n</code></pre> <p><code>volumes</code>\u00a0is part of the KubernetesConf abstraction.</p> <p><code>volumes</code> parses volume specs for the executor pod (with the spark.kubernetes.executor.volumes. prefix) from the SparkConf.</p>"},{"location":"KubernetesFeatureConfigStep/","text":"<p><code>KubernetesFeatureConfigStep</code> is an abstraction of Kubernetes pod features for drivers and executors.</p>","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#contract","text":"","title":"Contract"},{"location":"KubernetesFeatureConfigStep/#configuring-pod","text":"","title":"Configuring Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p>Used when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested to build a driver pod spec</li> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>"},{"location":"KubernetesFeatureConfigStep/#additional-kubernetes-resources","text":"","title":"Additional Kubernetes Resources <pre><code>getAdditionalKubernetesResources(): Seq[HasMetadata]\n</code></pre> <p>Additional Kubernetes resources (that are going to created when <code>Client</code> is requested to run)</p> <p>Default: <code>empty</code></p> <p>Used when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for a driver pod spec</li> </ul>"},{"location":"KubernetesFeatureConfigStep/#additional-system-properties","text":"","title":"Additional System Properties <pre><code>getAdditionalPodSystemProperties(): Map[String, String]\n</code></pre> <p>Additional system properties of a driver pod (that are going to be part of <code>spark.properties</code> as a ConfigMap)</p> <p>Default: <code>empty</code></p> <p>Used when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for a driver pod spec</li> </ul>"},{"location":"KubernetesFeatureConfigStep/#implementations","text":"<ul> <li>BasicDriverFeatureStep</li> <li>BasicExecutorFeatureStep</li> <li>DriverCommandFeatureStep</li> <li>DriverKubernetesCredentialsFeatureStep</li> <li>DriverServiceFeatureStep</li> <li>EnvSecretsFeatureStep</li> <li>ExecutorKubernetesCredentialsFeatureStep</li> <li>HadoopConfDriverFeatureStep</li> <li>KerberosConfDriverFeatureStep</li> <li>LocalDirsFeatureStep</li> <li>MountSecretsFeatureStep</li> <li>MountVolumesFeatureStep</li> <li>PodTemplateConfigMapStep</li> </ul>","title":"Implementations"},{"location":"KubernetesUtils/","text":"","title":"KubernetesUtils Utility"},{"location":"KubernetesUtils/#parsing-master-url","text":"","title":"Parsing Master URL <pre><code>parseMasterUrl(\n  url: String): String\n</code></pre> <p><code>parseMasterUrl</code> takes off the <code>k8s://</code> prefix from the given url.</p> <p><code>parseMasterUrl</code> is used when:</p> <ul> <li><code>K8SSparkSubmitOperation</code> is requested to execute an operation</li> <li><code>KubernetesClientApplication</code> is requested to start</li> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>"},{"location":"KubernetesUtils/#buildpodwithserviceaccount","text":"","title":"buildPodWithServiceAccount <pre><code>buildPodWithServiceAccount(\n  serviceAccount: Option[String],\n  pod: SparkPod): Option[Pod]\n</code></pre> <p><code>buildPodWithServiceAccount</code> creates a new pod spec with the service account and service account name based on the given service account (if defined). Otherwise, <code>buildPodWithServiceAccount</code> returns <code>None</code>.</p> <p><code>buildPodWithServiceAccount</code> is used when:</p> <ul> <li><code>DriverKubernetesCredentialsFeatureStep</code> is requested to configure a pod</li> <li><code>ExecutorKubernetesCredentialsFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"KubernetesUtils/#loading-pod-spec-from-template-file","text":"","title":"Loading Pod Spec from Template File <pre><code>loadPodFromTemplate(\n  kubernetesClient: KubernetesClient,\n  templateFile: File,\n  containerName: Option[String]): SparkPod\n</code></pre> <p><code>loadPodFromTemplate</code> requests the given <code>KubernetesClient</code> to load a pod spec from the input template file.</p> <p><code>loadPodFromTemplate</code> selects the Spark container (from the pod spec and the input container name).</p> <p>In case of an <code>Exception</code>, <code>loadPodFromTemplate</code> prints out the following ERROR message to the logs:</p> <pre><code>Encountered exception while attempting to load initial pod spec from file\n</code></pre> <p><code>loadPodFromTemplate</code> (re)throws a <code>SparkException</code>:</p> <pre><code>Could not load pod from template file.\n</code></pre> <p><code>loadPodFromTemplate</code>\u00a0is used when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> <li><code>KubernetesDriverBuilder</code> is requested for a pod spec for a driver</li> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>"},{"location":"KubernetesUtils/#selectsparkcontainer","text":"","title":"selectSparkContainer <pre><code>selectSparkContainer(\n  pod: Pod,\n  containerName: Option[String]): SparkPod\n</code></pre> <p><code>selectSparkContainer</code> creates a <code>SparkPod</code> based on the containers in the given <code>Pod</code> and the <code>containerName</code>.</p> <p><code>selectSparkContainer</code> takes the container specs from the the given <code>Pod</code> spec and tries to find the one with the <code>containerName</code> or takes the first defined.</p> <p><code>selectSparkContainer</code> includes the other containers in the pod spec.</p> <p><code>selectSparkContainer</code> prints out the following WARN message to the logs when no container could be found by the given name:</p> <pre><code>specified container [name] not found on the pod template, falling back to taking the first container\n</code></pre>"},{"location":"KubernetesUtils/#uploading-local-files-to-hadoop-dfs","text":"","title":"Uploading Local Files to Hadoop DFS <pre><code>uploadAndTransformFileUris(\n  fileUris: Iterable[String],\n  conf: Option[SparkConf] = None): Iterable[String]\n</code></pre> <p><code>uploadAndTransformFileUris</code> uploads local files in the given <code>fileUris</code> to Hadoop DFS (based on spark.kubernetes.file.upload.path configuration property).</p> <p>In the end, <code>uploadAndTransformFileUris</code> returns the target URIs.</p> <p><code>uploadAndTransformFileUris</code>\u00a0is used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to getAdditionalPodSystemProperties</li> </ul>"},{"location":"KubernetesUtils/#uploadfileuri","text":"","title":"uploadFileUri <pre><code>uploadFileUri(\n  uri: String,\n  conf: Option[SparkConf] = None): String\n</code></pre> <p><code>uploadFileUri</code> resolves the given <code>uri</code> to a well-formed <code>file</code> URI.</p> <p><code>uploadFileUri</code> creates a new Hadoop <code>Configuration</code> and resolves the spark.kubernetes.file.upload.path configuration property to a Hadoop <code>FileSystem</code>.</p> <p><code>uploadFileUri</code> creates (mkdirs) the Hadoop DFS path to upload the file of the format:</p> <pre><code>[spark.kubernetes.file.upload.path]/[spark-upload-[randomUUID]]\n</code></pre> <p><code>uploadFileUri</code> prints out the following INFO message to the logs:</p> <pre><code>Uploading file: [path] to dest: [targetUri]...\n</code></pre> <p>In the end, <code>uploadFileUri</code> uploads the file to the target location (using Hadoop DFS's FileSystem.copyFromLocalFile) and returns the target URI.</p>"},{"location":"KubernetesUtils/#sparkexceptions","text":"","title":"SparkExceptions <p><code>uploadFileUri</code> throws a <code>SparkException</code> when:</p> <ol> <li> <p>Uploading the <code>uri</code> fails:</p> <pre><code>Uploading file [path] failed...\n</code></pre> </li> <li> <p>spark.kubernetes.file.upload.path configuration property is not defined:</p> <pre><code>Please specify spark.kubernetes.file.upload.path property.\n</code></pre> </li> <li> <p><code>SparkConf</code> is not defined:</p> <pre><code>Spark configuration is missing...\n</code></pre> </li> </ol>"},{"location":"KubernetesUtils/#renamemainappresource","text":"","title":"renameMainAppResource <pre><code>renameMainAppResource(\n  resource: String,\n  conf: SparkConf): String\n</code></pre> <p><code>renameMainAppResource</code> is converted to spark-internal internal name when the given <code>resource</code> is local and resolvable. Otherwise, <code>renameMainAppResource</code> returns the given resource as-is.</p> <p><code>renameMainAppResource</code> is used when:</p> <ul> <li><code>DriverCommandFeatureStep</code> is requested for the base driver container (for a <code>JavaMainAppResource</code> application)</li> </ul>"},{"location":"KubernetesUtils/#islocalandresolvable","text":"","title":"isLocalAndResolvable <pre><code>isLocalAndResolvable(\n  resource: String): Boolean\n</code></pre> <p><code>isLocalAndResolvable</code> is <code>true</code> when the given <code>resource</code> is:</p> <ol> <li>Not internal</li> <li>Uses either file or no URI scheme (after converting to a well-formed URI)</li> </ol> <p><code>isLocalAndResolvable</code> is used when:</p> <ul> <li><code>KubernetesUtils</code> is requested to renameMainAppResource</li> <li><code>BasicDriverFeatureStep</code> is requested to getAdditionalPodSystemProperties</li> </ul>"},{"location":"KubernetesUtils/#islocaldependency","text":"","title":"isLocalDependency <pre><code>isLocalDependency(\n  uri: URI): Boolean\n</code></pre> <p>An input <code>URI</code> is a local dependency when the scheme is <code>null</code> (undefined) or <code>file</code>.</p>"},{"location":"KubernetesUtils/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.deploy.k8s.KubernetesUtils</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.k8s.KubernetesUtils=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"KubernetesVolumeSpec/","text":"<p><code>KubernetesVolumeSpec</code> is a Kubernetes volume specification:</p> <ul> <li> Volume Name <li> Mount Path <li> Mount SubPath (default: empty) <li> <code>readOnly</code> flag (default: <code>false</code>) <li> KubernetesVolumeSpecificConf  <p><code>KubernetesVolumeSpec</code> is part of KubernetesConf abstraction.</p> <p><code>KubernetesVolumeSpec</code> is created using KubernetesVolumeUtils utility.</p>","title":"KubernetesVolumeSpec"},{"location":"KubernetesVolumeSpec/#kubernetesvolumespecificconf","text":"","title":"KubernetesVolumeSpecificConf <p><code>KubernetesVolumeSpecificConf</code> complements <code>KubernetesVolumeSpec</code> with a volume type-specific configuration.</p> <p><code>KubernetesVolumeSpecificConf</code> is created using KubernetesVolumeUtils utility.</p> Sealed Trait<p><code>KubernetesVolumeSpecificConf</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>"},{"location":"KubernetesVolumeSpec/#kubernetesemptydirvolumeconf","text":"","title":"KubernetesEmptyDirVolumeConf <ul> <li>Optional <code>medium</code></li> <li>Optional <code>sizeLimit</code></li> </ul>"},{"location":"KubernetesVolumeSpec/#kuberneteshostpathvolumeconf","text":"","title":"KubernetesHostPathVolumeConf <ul> <li><code>hostPath</code> (based on <code>hostPath.[volumeName].options.path</code> configuration property)</li> </ul> <p><code>KubernetesHostPathVolumeConf</code> is used when:</p> <ul> <li><code>MountVolumesFeatureStep</code> is requested to constructVolumes (that creates a hostPath volume for the given <code>hostPath</code> volume source path and an empty type)</li> </ul>"},{"location":"KubernetesVolumeSpec/#kubernetesnfsvolumeconf","text":"","title":"KubernetesNFSVolumeConf <ul> <li><code>path</code></li> <li><code>server</code></li> </ul>"},{"location":"KubernetesVolumeSpec/#kubernetespvcvolumeconf","text":"","title":"KubernetesPVCVolumeConf <p><code>KubernetesPVCVolumeConf</code> is a KubernetesVolumeSpecificConf with the following:</p> <ul> <li><code>claimName</code></li> <li>Optional <code>storageClass</code> (default: undefined)</li> <li>Optional <code>size</code> (default: undefined)</li> </ul> <p><code>KubernetesPVCVolumeConf</code> is created (using KubernetesVolumeUtils utility) based on configuration properties with <code>persistentVolumeClaim</code> volume type prefix.</p>    Attribute Configuration Property     <code>claimName</code> <code>persistentVolumeClaim.[volumeName].options.claimName</code>   <code>storageClass</code> <code>persistentVolumeClaim.[volumeName].options.storageClass</code>   <code>size</code> <code>persistentVolumeClaim.[volumeName].options.sizeLimit</code>    <p><code>KubernetesPVCVolumeConf</code> is used when:</p> <ul> <li><code>MountVolumesFeatureStep</code> is requested to constructVolumes (that creates a PersistentVolumeClaim for the given <code>claimName</code>)</li> </ul>"},{"location":"KubernetesVolumeUtils/","text":"<p><code>KubernetesVolumeUtils</code> utility is used to parseVolumesWithPrefix.</p>","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#parsevolumeswithprefix","text":"","title":"parseVolumesWithPrefix <pre><code>parseVolumesWithPrefix(\n  sparkConf: SparkConf,\n  prefix: String): Seq[KubernetesVolumeSpec]\n</code></pre> <p><code>parseVolumesWithPrefix</code> creates KubernetesVolumeSpecs based on the configuration properties (in the given <code>SparkConf</code>) with the <code>prefix</code>.</p> <p><code>parseVolumesWithPrefix</code> requests the <code>SparkConf</code> to get all properties with the given <code>prefix</code>.</p> <p><code>parseVolumesWithPrefix</code> extracts volume types and names (from the keys of the properties) and for every pair creates a KubernetesVolumeSpec with the following:</p> <ul> <li><code>volumeName</code></li> <li><code>mountPath</code> based on <code>[volumeType].[volumeName].mount.path</code> key (in the properties)</li> <li><code>mountSubPath</code> based on <code>[volumeType].[volumeName].mount.subPath</code> key (in the properties) if available or defaults to an empty path</li> <li><code>mountReadOnly</code> based on <code>[volumeType].[volumeName].mount.readOnly</code> key (in the properties) if available or <code>false</code></li> <li><code>volumeConf</code> with a KubernetesVolumeSpecificConf based on the properties and the <code>volumeType</code> and <code>volumeName</code> of the volume</li> </ul> <p><code>parseVolumesWithPrefix</code>\u00a0is used when:</p> <ul> <li><code>KubernetesDriverConf</code> is requested for volumes</li> <li><code>KubernetesExecutorConf</code> is requested for volumes</li> <li><code>KubernetesConf</code> utility is used to create a KubernetesDriverConf</li> </ul>"},{"location":"KubernetesVolumeUtils/#getvolumetypesandnames","text":"","title":"getVolumeTypesAndNames <pre><code>getVolumeTypesAndNames(\n  properties: Map[String, String]): Set[(String, String)]\n</code></pre> <p><code>getVolumeTypesAndNames</code> extracts volume types and names.</p> <p><code>getVolumeTypesAndNames</code> splits the keys (in the given <code>properties</code> key-value collection) by <code>.</code> to volume type and name pairs.</p>"},{"location":"KubernetesVolumeUtils/#parsevolumespecificconf","text":"","title":"parseVolumeSpecificConf <pre><code>parseVolumeSpecificConf(\n  options: Map[String, String],\n  volumeType: String,\n  volumeName: String): KubernetesVolumeSpecificConf\n</code></pre> <p><code>parseVolumeSpecificConf</code> extracts volume configuration given the volume type and name.</p> <p><code>parseVolumeSpecificConf</code> creates a KubernetesVolumeSpecificConf based on the given <code>volumeType</code>.</p> <p><code>parseVolumeSpecificConf</code> throws an <code>IllegalArgumentException</code> for unsupported <code>volumeType</code>:</p> <pre><code>Kubernetes Volume type `[volumeType]` is not supported\n</code></pre>"},{"location":"LocalDirsFeatureStep/","text":"<p><code>LocalDirsFeatureStep</code> is a KubernetesFeatureConfigStep.</p>","title":"LocalDirsFeatureStep"},{"location":"LocalDirsFeatureStep/#creating-instance","text":"<p><code>LocalDirsFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesConf <li> Default Local Directory (default: <code>/var/data/spark-[randomUUID]</code>)  <p><code>LocalDirsFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested to build a driver pod</li> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>","title":"Creating Instance"},{"location":"LocalDirsFeatureStep/#sparkkuberneteslocaldirstmpfs","text":"","title":"spark.kubernetes.local.dirs.tmpfs <p><code>LocalDirsFeatureStep</code> uses spark.kubernetes.local.dirs.tmpfs configuration property when configuring a pod.</p>"},{"location":"LocalDirsFeatureStep/#configuring-pod","text":"","title":"Configuring Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code> finds mount paths of the volume mounts with spark-local-dir- prefix name of the input <code>SparkPod</code> (localDirs).</p> <p>If there are no local directory mount paths, <code>configurePod</code>...FIXME</p> <p><code>configurePod</code> adds the local directory volumes to a new pod specification (there could be none).</p> <p><code>configurePod</code> defines <code>SPARK_LOCAL_DIRS</code> environment variable as a comma-separated local directories and adds the local directory volume mounts to a new container specification (there could be none).</p> <p>In the end, <code>configurePod</code> creates a new <code>SparkPod</code> with the new pod and container.</p>"},{"location":"LoggingPodStatusWatcher/","text":"<p><code>LoggingPodStatusWatcher</code> is an extension of Kubernetes' <code>Watcher[Pod]</code> for pod watchers that can watchOrStop.</p> <p><code>LoggingPodStatusWatcher</code> is used to create a Client when a KubernetesClientApplication is requested to start.</p>","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#contract","text":"","title":"Contract"},{"location":"LoggingPodStatusWatcher/#watchorstop","text":"","title":"watchOrStop <pre><code>watchOrStop(\n  submissionId: String): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>Client</code> is requested to run</li> </ul>"},{"location":"LoggingPodStatusWatcher/#implementations","text":"<ul> <li>LoggingPodStatusWatcherImpl</li> </ul>","title":"Implementations"},{"location":"LoggingPodStatusWatcherImpl/","text":"<p><code>LoggingPodStatusWatcherImpl</code> is a LoggingPodStatusWatcher that monitors and logs the application status.</p>","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#creating-instance","text":"<p><code>LoggingPodStatusWatcherImpl</code> takes the following to be created:</p> <ul> <li> KubernetesDriverConf  <p><code>LoggingPodStatusWatcherImpl</code> is created\u00a0when:</p> <ul> <li><code>KubernetesClientApplication</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"LoggingPodStatusWatcherImpl/#watchorstop","text":"","title":"watchOrStop <pre><code>watchOrStop(\n  sId: String): Unit\n</code></pre> <p><code>watchOrStop</code> is part of the LoggingPodStatusWatcher abstraction.</p> <p><code>watchOrStop</code> uses spark.kubernetes.submission.waitAppCompletion configuration property to control whether to wait for the Spark application to complete (<code>true</code>) or merely print out the following INFO message to the logs:</p> <pre><code>Deployed Spark application [appName] with submission ID [sId] into Kubernetes\n</code></pre> <p>While waiting for the Spark application to complete, <code>watchOrStop</code> prints out the following INFO message to the logs:</p> <pre><code>Waiting for application [appName] with submission ID [sId] to finish...\n</code></pre> <p>Until podCompleted flag is <code>true</code>, <code>watchOrStop</code> waits spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs:</p> <pre><code>Application status for [appId] (phase: [phase])\n</code></pre> <p>Once podCompleted flag is <code>true</code>, <code>watchOrStop</code> prints out the following INFO messages to the logs:</p> <pre><code>Container final statuses:\n\n[containersDescription]\n</code></pre> <pre><code>Application [appName] with submission ID [sId] finished\n</code></pre> <p>When no pod is available, <code>watchOrStop</code> prints out the following INFO message to the logs:</p> <pre><code>No containers were found in the driver pod.\n</code></pre>"},{"location":"LoggingPodStatusWatcherImpl/#eventreceived","text":"","title":"eventReceived <pre><code>eventReceived(\n  action: Action,\n  pod: Pod): Unit\n</code></pre> <p><code>eventReceived</code>\u00a0is part of the Kubernetes' <code>Watcher</code> abstraction.</p> <p><code>eventReceived</code> brances off based on the given <code>Action</code>:</p> <ul> <li> <p>For <code>DELETED</code> or <code>ERROR</code> actions, <code>eventReceived</code> closeWatch</p> </li> <li> <p>For any other actions, logLongStatus followed by closeWatch if hasCompleted.</p> </li> </ul>"},{"location":"LoggingPodStatusWatcherImpl/#loglongstatus","text":"","title":"logLongStatus <pre><code>logLongStatus(): Unit\n</code></pre> <p><code>logLongStatus</code> prints out the following INFO message to the logs:</p> <pre><code>State changed, new state: [formatPodState|unknown]\n</code></pre>"},{"location":"LoggingPodStatusWatcherImpl/#hascompleted","text":"","title":"hasCompleted <pre><code>hasCompleted(): Boolean\n</code></pre> <p><code>hasCompleted</code> is <code>true</code> when the phase is <code>Succeeded</code> or <code>Failed</code>.</p> <p><code>hasCompleted</code> is used when:</p> <ul> <li><code>LoggingPodStatusWatcherImpl</code> is requested to eventReceived (when an action is neither <code>DELETED</code> nor <code>ERROR</code>)</li> </ul>"},{"location":"LoggingPodStatusWatcherImpl/#podcompleted-flag","text":"","title":"podCompleted Flag <p><code>LoggingPodStatusWatcherImpl</code> turns <code>podCompleted</code> off when created.</p> <p>Until <code>podCompleted</code> is on, <code>LoggingPodStatusWatcherImpl</code> waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs:</p> <pre><code>Application status for [appId] (phase: [phase])\n</code></pre> <p><code>podCompleted</code> turns podCompleted on when closeWatch.</p>"},{"location":"LoggingPodStatusWatcherImpl/#closewatch","text":"","title":"closeWatch <pre><code>closeWatch(): Unit\n</code></pre> <p><code>closeWatch</code> turns podCompleted on.</p> <p><code>closeWatch</code> is used when:</p> <ul> <li><code>LoggingPodStatusWatcherImpl</code> is requested to eventReceived and onClose</li> </ul>"},{"location":"LoggingPodStatusWatcherImpl/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"MountSecretsFeatureStep/","text":"<p><code>MountSecretsFeatureStep</code> is...FIXME</p>","title":"MountSecretsFeatureStep"},{"location":"MountVolumesFeatureStep/","text":"<p><code>MountVolumesFeatureStep</code> is a KubernetesFeatureConfigStep.</p>","title":"MountVolumesFeatureStep"},{"location":"MountVolumesFeatureStep/#creating-instance","text":"<p><code>MountVolumesFeatureStep</code> takes the following to be created:</p> <ul> <li> KubernetesConf  <p><code>MountVolumesFeatureStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested to build a driver pod spec</li> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>","title":"Creating Instance"},{"location":"MountVolumesFeatureStep/#configuring-driver-pod","text":"","title":"Configuring Driver Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code> constructs the volumes and volume mounts from the volumes (of the KubernetesConf) and creates a new <code>SparkPod</code>:</p> <ul> <li> <p>Adds the volumes to the pod specification</p> </li> <li> <p>Adds the volume mounts to the container specification</p> </li> </ul>"},{"location":"MountVolumesFeatureStep/#constructvolumes","text":"","title":"constructVolumes <pre><code>constructVolumes(\n  volumeSpecs: Iterable[KubernetesVolumeSpec]): Iterable[(VolumeMount, Volume)]\n</code></pre> <p><code>constructVolumes</code> creates Kubernetes <code>VolumeMount</code>s and <code>Volume</code>s for the given KubernetesVolumeSpecs.</p> <p><code>VolumeMount</code>s are built based on the following (4 of 5 properties):</p> <ul> <li>mountPath</li> <li>mountReadOnly</li> <li>mountSubPath</li> <li>volumeName</li> </ul> <p><code>Volume</code>s are build based on the type of the volume (the remaining 5th property).</p> <p>In the end, <code>Volume</code>s and <code>VolumeMount</code>s are wired together using <code>volumeName</code>.</p>"},{"location":"MountVolumesFeatureStep/#claim-name-placeholders","text":"","title":"Claim Name Placeholders <p><code>MountVolumesFeatureStep</code> defines OnDemand and SPARK_EXECUTOR_ID as placeholders for runtime-replaceable parts of the claim name of a KubernetesPVCVolumeConf.</p> <p>These placeholders allow for templating claim names to include parts to be replaced at deployment.</p> <p>When constructVolumes <code>MountVolumesFeatureStep</code> replaces all <code>OnDemand</code>s with the following (using resourceNamePrefix and executorId of the KubernetesConf):</p>    Pod Replacement     driver <code>[resourceNamePrefix]-exec-[executorId]-pvc-[volumeIndex]</code>   executors <code>[resourceNamePrefix]-driver-pvc-[volumeIndex]</code>    <p>When constructVolumes <code>MountVolumesFeatureStep</code> replaces all <code>SPARK_EXECUTOR_ID</code>s with executorId of the KubernetesConf.</p>"},{"location":"PodTemplateConfigMapStep/","text":"<p><code>PodTemplateConfigMapStep</code> is a KubernetesFeatureConfigStep.</p>","title":"PodTemplateConfigMapStep"},{"location":"PodTemplateConfigMapStep/#creating-instance","text":"<p><code>PodTemplateConfigMapStep</code> takes the following to be created:</p> <ul> <li> KubernetesConf  <p><code>PodTemplateConfigMapStep</code> is created\u00a0when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for a driver pod spec</li> </ul>","title":"Creating Instance"},{"location":"PodTemplateConfigMapStep/#hastemplate-flag","text":"","title":"hasTemplate Flag <p><code>PodTemplateConfigMapStep</code> uses <code>hasTemplate</code> flag based on the spark.kubernetes.executor.podTemplateFile configuration property to control whether or not to configure a pod, and offer Additional Pod System Properties and Additional Kubernetes Resources.</p>"},{"location":"PodTemplateConfigMapStep/#additional-kubernetes-resources","text":"","title":"Additional Kubernetes Resources <pre><code>getAdditionalKubernetesResources(): Seq[HasMetadata]\n</code></pre> <p><code>getAdditionalKubernetesResources</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>getAdditionalKubernetesResources</code>...FIXME</p>"},{"location":"PodTemplateConfigMapStep/#additional-system-properties","text":"","title":"Additional System Properties <pre><code>getAdditionalPodSystemProperties(): Map[String, String]\n</code></pre> <p><code>getAdditionalPodSystemProperties</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>getAdditionalPodSystemProperties</code>...FIXME</p>"},{"location":"PodTemplateConfigMapStep/#configuring-driver-pod","text":"","title":"Configuring Driver Pod <pre><code>configurePod(\n  pod: SparkPod): SparkPod\n</code></pre> <p><code>configurePod</code>\u00a0is part of the KubernetesFeatureConfigStep abstraction.</p> <p><code>configurePod</code>...FIXME</p>"},{"location":"PollRunnable/","text":"<p><code>PollRunnable</code> is a Java Runnable that ExecutorPodsPollingSnapshotSource schedules for full executor pod state snapshots from Kubernetes every spark.kubernetes.executor.apiPollingInterval in the Spark application.</p> <p><code>PollRunnable</code> is an internal class of ExecutorPodsPollingSnapshotSource with full access to its internals.</p>","title":"PollRunnable"},{"location":"PollRunnable/#creating-instance","text":"<p><code>PollRunnable</code> takes the following to be created:</p> <ul> <li> Application Id  <p><code>PollRunnable</code> is created\u00a0when:</p> <ul> <li><code>ExecutorPodsPollingSnapshotSource</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"PollRunnable/#starting-thread","text":"","title":"Starting Thread <pre><code>run(): Unit\n</code></pre> <p><code>run</code> prints out the following DEBUG message to the logs:</p> <pre><code>Resynchronizing full executor pod state from Kubernetes.\n</code></pre> <p><code>run</code> requests the KubernetesClient for Spark executor pods with the following labels and values.</p>    Label Name Value     <code>spark-app-selector</code> application Id   <code>spark-role</code> <code>executor</code>   spark-exec-inactive any value but <code>true</code>    <p>In the end, <code>run</code> requests the ExecutorPodsSnapshotsStore to replace the snapshot.</p>"},{"location":"PollRunnable/#logging","text":"","title":"Logging <p><code>PollRunnable</code> uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.</p>"},{"location":"SnapshotsSubscriber/","text":"<p><code>SnapshotsSubscriber</code> is used by ExecutorPodsSnapshotsStoreImpl to manage snapshot subscribers that are notified about new ExecutorPodsSnapshots regularly.</p> <p><code>SnapshotsSubscriber</code> manages snapshotsBuffer queue of ExecutorPodsSnapshots.</p> <p><code>SnapshotsSubscriber</code> is a <code>private class</code> of ExecutorPodsSnapshotsStoreImpl with full access to its internals.</p>","title":"SnapshotsSubscriber"},{"location":"SnapshotsSubscriber/#creating-instance","text":"<p><code>SnapshotsSubscriber</code> takes the following to be created:</p> <ul> <li>onNewSnapshots Callback Function</li> </ul> <p><code>SnapshotsSubscriber</code> is created\u00a0when:</p> <ul> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to add a subscriber</li> </ul>","title":"Creating Instance"},{"location":"SnapshotsSubscriber/#onnewsnapshots-callback-function","text":"","title":"onNewSnapshots Callback Function <pre><code>onNewSnapshots: Seq[ExecutorPodsSnapshot] =&gt; Unit\n</code></pre> <p><code>SnapshotsSubscriber</code> is given a <code>onNewSnapshots</code> callback function when created.</p>"},{"location":"SnapshotsSubscriber/#snapshotsbuffer-queue","text":"","title":"snapshotsBuffer Queue <pre><code>snapshotsBuffer: LinkedBlockingQueue[ExecutorPodsSnapshot]\n</code></pre> <p><code>SnapshotsSubscriber</code> manages a <code>snapshotsBuffer</code> queue of ExecutorPodsSnapshots.</p>"},{"location":"SnapshotsSubscriber/#notificationcount-counter","text":"","title":"notificationCount Counter <pre><code>notificationCount: AtomicInteger\n</code></pre> <p><code>SnapshotsSubscriber</code> manages a <code>notificationCount</code> counter.</p>"},{"location":"SnapshotsSubscriber/#addcurrentsnapshot","text":"","title":"addCurrentSnapshot <pre><code>addCurrentSnapshot(): Unit\n</code></pre> <p><code>addCurrentSnapshot</code> requests the snapshotsBuffer internal queue to add the current ExecutorPodsSnapshot.</p> <p><code>addCurrentSnapshot</code>\u00a0is used when:</p> <ul> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to add a subscriber and addCurrentSnapshotToSubscribers</li> </ul>"},{"location":"SnapshotsSubscriber/#processsnapshots","text":"","title":"processSnapshots <pre><code>processSnapshots(): Unit\n</code></pre> <p><code>processSnapshots</code> increments the notificationCount counter followed by processSnapshotsInternal.</p> <p><code>processSnapshots</code>\u00a0is used when:</p> <ul> <li>ScheduledExecutorService (of <code>ExecutorPodsSnapshotsStoreImpl</code>) executes regularly (for a subscriber)</li> <li><code>ExecutorPodsSnapshotsStoreImpl</code> is requested to notify all subscribers</li> </ul>"},{"location":"SnapshotsSubscriber/#processsnapshotsinternal","text":"","title":"processSnapshotsInternal <pre><code>processSnapshotsInternal(): Unit\n</code></pre> <p><code>processSnapshotsInternal</code>...FIXME</p>  <p>Note</p> <p><code>processSnapshotsInternal</code> can be called again recursively when the notificationCount counter has been incremented while the <code>processSnapshotsInternal</code> was running.</p>"},{"location":"SparkKubernetesClientFactory/","text":"<p><code>SparkKubernetesClientFactory</code> is a Spark-opinionated builder for Kubernetes clients.</p>","title":"SparkKubernetesClientFactory Utility"},{"location":"SparkKubernetesClientFactory/#creating-kubernetesclient","text":"","title":"Creating KubernetesClient <pre><code>createKubernetesClient(\n  master: String,\n  namespace: Option[String],\n  kubernetesAuthConfPrefix: String,\n  clientType: ClientType.Value,\n  sparkConf: SparkConf,\n  defaultServiceAccountToken: Option[File],\n  defaultServiceAccountCaCert: Option[File]): KubernetesClient\n</code></pre> <p><code>createKubernetesClient</code> utility takes the OAuth token-related configuration properties from the input <code>SparkConf</code>:</p> <ul> <li><code>kubernetesAuthConfPrefix</code>.oauthTokenFile (or defaults to the input <code>defaultServiceAccountToken</code>)</li> <li><code>kubernetesAuthConfPrefix</code>.oauthToken</li> </ul> <p><code>createKubernetesClient</code> takes the spark.kubernetes.context configuraiton property (kubeContext).</p> <p><code>createKubernetesClient</code> takes the certificate-related configuration properties from the input <code>SparkConf</code>:</p> <ul> <li><code>kubernetesAuthConfPrefix</code>.caCertFile (or defaults to the input <code>defaultServiceAccountCaCert</code>)</li> <li><code>kubernetesAuthConfPrefix</code>.clientKeyFile</li> <li><code>kubernetesAuthConfPrefix</code>.clientCertFile</li> </ul> <p><code>createKubernetesClient</code> prints out the following INFO message to the logs:</p> <pre><code>Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file\n</code></pre> <p><code>createKubernetesClient</code> builds a Kubernetes <code>Config</code> (based on the configuration properties).</p> <p><code>createKubernetesClient</code> builds an <code>OkHttpClient</code> with a custom <code>kubernetes-dispatcher</code> dispatcher.</p> <p>In the end, <code>createKubernetesClient</code> creates a Kubernetes <code>DefaultKubernetesClient</code> (with the <code>OkHttpClient</code> and <code>Config</code>).</p> <p><code>createKubernetesClient</code>\u00a0is used when:</p> <ul> <li><code>K8SSparkSubmitOperation</code> is requested to execute</li> <li><code>KubernetesClientApplication</code> is requested to start</li> <li><code>KubernetesClusterManager</code> is requested to create a SchedulerBackend</li> </ul>"},{"location":"SparkKubernetesClientFactory/#exceptions","text":"<p><code>createKubernetesClient</code> throws an <code>IllegalArgumentException</code> when an OAuth token is specified through a file and a value:</p> <pre><code>Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf].\n</code></pre>","title":"Exceptions"},{"location":"SparkKubernetesClientFactory/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.deploy.k8s.SparkKubernetesClientFactory</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"configuration-properties/","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkkubernetesallocationbatchdelay","text":"","title":"spark.kubernetes.allocation.batch.delay <p>Time (in millis) to wait between each round of executor allocation</p> <p>Default: <code>1s</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is created</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesappkillpoddeletiongraceperiod","text":"","title":"spark.kubernetes.appKillPodDeletionGracePeriod <p>Grace Period that is the time (in seconds) to wait for a graceful deletion of Spark pods when <code>spark-submit --kill</code></p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>K8SSparkSubmitOperation</code> is requested to kill</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesallocationbatchsize","text":"","title":"spark.kubernetes.allocation.batch.size <p>Maximum number of executor pods to allocate at once in each round of executor allocation</p> <p>Default: <code>5</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to allocate executor pods</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesallocationexecutortimeout","text":"","title":"spark.kubernetes.allocation.executor.timeout <p>Time (in millis) to wait before a pending executor is considered timed out</p> <p>Default: <code>600s</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to handle executor pods snapshots</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesauthenticate","text":"","title":"spark.kubernetes.authenticate <p>FIXME</p>"},{"location":"configuration-properties/#sparkkubernetesauthenticatedrivermounted","text":"","title":"spark.kubernetes.authenticate.driver.mounted <p>FIXME</p>"},{"location":"configuration-properties/#sparkkubernetesauthenticatedriverserviceaccountname","text":"","title":"spark.kubernetes.authenticate.driver.serviceAccountName <p>Service account for a driver pod (for requesting executor pods from the API server)</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>DriverKubernetesCredentialsFeatureStep</code> is requested to configure a pod</li> <li><code>ExecutorKubernetesCredentialsFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesauthenticateexecutorserviceaccountname","text":"","title":"spark.kubernetes.authenticate.executor.serviceAccountName <p>Service account for executor pods</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>ExecutorKubernetesCredentialsFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesconfigmapmaxsize","text":"","title":"spark.kubernetes.configMap.maxSize <p>Max size limit (<code>long</code>) for a config map. Configurable as per https://etcd.io/docs/v3.4.0/dev-guide/limit/ on k8s server end.</p> <p>Default: <code>1572864</code> (<code>1.5 MiB</code>)</p> <p>Used when:</p> <ul> <li><code>KubernetesClientUtils</code> utility is used to loadSparkConfDirFiles</li> </ul>"},{"location":"configuration-properties/#sparkkubernetescontainerimage","text":"","title":"spark.kubernetes.container.image <p>Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined)</p> <p>Default: (undefined)</p>"},{"location":"configuration-properties/#sparkkubernetescontainerimagepullpolicy","text":"","title":"spark.kubernetes.container.image.pullPolicy <p>Kubernetes image pull policy:</p> <ul> <li><code>Always</code></li> <li><code>Never</code></li> <li><code>IfNotPresent</code></li> </ul> <p>Default: <code>IfNotPresent</code></p> <p>Used when:</p> <ul> <li><code>KubernetesConf</code> is requested for imagePullPolicy</li> </ul>"},{"location":"configuration-properties/#sparkkubernetescontext","text":"","title":"spark.kubernetes.context <p>The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the <code>KUBECONFIG</code> environment variable or by defaulting to <code>.kube/config</code> under your home directory. If not specified then your current context is used.  You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options.</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>SparkKubernetesClientFactory</code> is requested to create a KubernetesClient</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesdrivercontainerimage","text":"","title":"spark.kubernetes.driver.container.image <p>Container image for drivers</p> <p>Default: spark.kubernetes.container.image</p> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested for a driverContainerImage</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesdrivermaster","text":"","title":"spark.kubernetes.driver.master <p>The internal Kubernetes master (API server) address to be used for driver to request executors.</p> <p>Default: <code>https://kubernetes.default.svc</code></p>"},{"location":"configuration-properties/#sparkkubernetesdriverpodname","text":"","title":"spark.kubernetes.driver.pod.name <p>Name of the driver pod</p> <p>Default: (undefined)</p> <p>Must be provided if a Spark application is deployed in cluster deploy mode</p> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested for the driverPodName (and additional system properties of a driver pod)</li> <li><code>ExecutorPodsAllocator</code> is requested for the kubernetesDriverPodName</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatecontainername","text":"","title":"spark.kubernetes.driver.podTemplateContainerName <p>Name of the driver container in a pod template</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for a driver pod specification</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatefile","text":"","title":"spark.kubernetes.driver.podTemplateFile <p>Pod template file for drivers (in cluster deploy mode)</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>KubernetesDriverBuilder</code> is requested for a driver pod specification</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesdriverrequestcores","text":"","title":"spark.kubernetes.driver.request.cores <p>Specify the <code>cpu</code> request for the driver pod</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorapipollinginterval","text":"","title":"spark.kubernetes.executor.apiPollingInterval <p>Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors.</p> <p>Default: <code>30s</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsPollingSnapshotSource</code> is requested to start</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorcheckallcontainers","text":"","title":"spark.kubernetes.executor.checkAllContainers <p>Controls whether or not to check the status of all containers in a running executor pod when reporting executor status</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorcontainerimage","text":"","title":"spark.kubernetes.executor.container.image <p>Container image for executors</p> <p>Default: spark.kubernetes.container.image</p> <p>Used when:</p> <ul> <li><code>BasicExecutorFeatureStep</code> is requested for a driverContainerImage</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutordeleteontermination","text":"","title":"spark.kubernetes.executor.deleteOnTermination <p>Controls whether or not to delete executor pods after they have finished (successfully or not)</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsAllocator</code> is requested to handle executor pods snapshots</li> <li><code>ExecutorPodsLifecycleManager</code> is requested to handle executor pods snapshots</li> <li><code>KubernetesClusterSchedulerBackend</code> is requested to stop</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutoreventprocessinginterval","text":"","title":"spark.kubernetes.executor.eventProcessingInterval <p>Interval (in millis) between successive inspection of executor events sent from the Kubernetes API</p> <p>Default: <code>1s</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsLifecycleManager</code> is requested to start and register a new subscriber</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutormissingpoddetectdelta","text":"","title":"spark.kubernetes.executor.missingPodDetectDelta <p>Time (in millis) to wait before an executor is removed due to the executor's pod being missed in the Kubernetes API server's polled list of pods</p> <p>Default: <code>30s</code></p> <p>Used when:</p> <ul> <li><code>ExecutorPodsLifecycleManager</code> is requested to handle executor pods snapshots</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorpodnameprefix","text":"","title":"spark.kubernetes.executor.podNamePrefix <p>(internal) Prefix of the executor pod names</p> <p>Default: (undefined)</p> <p>Unless defined, it is set explicitly when <code>KubernetesClusterManager</code> is requested to create a SchedulerBackend</p> <p>Used when:</p> <ul> <li><code>KubernetesExecutorConf</code> is requested for the resourceNamePrefix</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatecontainername","text":"","title":"spark.kubernetes.executor.podTemplateContainerName <p>Name of the container for executors in a pod template</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatefile","text":"","title":"spark.kubernetes.executor.podTemplateFile <p>Pod template file for executors</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>KubernetesClusterManager</code> is requested to create a SchedulerBackend</li> <li><code>KubernetesExecutorBuilder</code> is requested for a pod spec for executors</li> <li><code>PodTemplateConfigMapStep</code> is created and requested to configurePod, getAdditionalPodSystemProperties, getAdditionalKubernetesResources</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorrequestcores","text":"","title":"spark.kubernetes.executor.request.cores <p>Specifies the cpu quantity request for executor pods (to be more Kubernetes-oriented when requesting resources for executor pods than Spark scheduler's approach based on <code>spark.executor.cores</code>).</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>BasicExecutorFeatureStep</code> is requested to configure an executor pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesexecutorschedulername","text":"","title":"spark.kubernetes.executor.scheduler.name <p>Name of the scheduler for executor pods (a pod's <code>spec.schedulerName</code>)</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>BasicExecutorFeatureStep</code> is requested to configure an executor pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesfileuploadpath","text":"","title":"spark.kubernetes.file.upload.path <p>Hadoop DFS-compatible file system path where files from the local file system will be uploded to in <code>cluster</code> deploy mode. The subdirectories (one per Spark application) with the local files are of the format <code>spark-upload-[uuid]</code>.</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>KubernetesUtils</code> is requested to uploadFileUri</li> </ul>"},{"location":"configuration-properties/#sparkkuberneteslocaldirstmpfs","text":"","title":"spark.kubernetes.local.dirs.tmpfs <p>If <code>true</code>, <code>emptyDir</code> volumes created to back <code>SPARK_LOCAL_DIRS</code> will have their medium set to <code>Memory</code> so that they will be created as tmpfs (i.e. RAM) backed volumes. This may improve performance but scratch space usage will count towards your pods memory limit so you may wish to request more memory.</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>LocalDirsFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesmemoryoverheadfactor","text":"","title":"spark.kubernetes.memoryOverheadFactor <p>Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs</p> <p>Must be a double between (0, 1.0)</p> <p>Default: <code>0.1</code></p> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to configure a pod</li> <li><code>BasicExecutorFeatureStep</code> is requested to configure a pod</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesnamespace","text":"","title":"spark.kubernetes.namespace <p>Namespace for running the driver and executor pods</p> <p>Default: <code>default</code></p> <p>Used when:</p> <ul> <li><code>KubernetesConf</code> is requested for namespace</li> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> <li><code>ExecutorPodsAllocator</code> is created (and initializes namespace)</li> </ul>"},{"location":"configuration-properties/#sparkkubernetesreportinterval","text":"","title":"spark.kubernetes.report.interval <p>Interval between reports of the current app status in cluster mode</p> <p>Default: <code>1s</code></p> <p>Used when:</p> <ul> <li><code>LoggingPodStatusWatcherImpl</code> is requested to watchOrStop</li> </ul>"},{"location":"configuration-properties/#sparkkubernetessubmissionwaitappcompletion","text":"","title":"spark.kubernetes.submission.waitAppCompletion <p>In <code>cluster</code> deploy mode, whether to wait for the application to finish before exiting the launcher process.</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>LoggingPodStatusWatcherImpl</code> is requested to watchOrStop</li> </ul>"},{"location":"configuration-properties/#sparkkubernetessubmitindriver","text":"","title":"spark.kubernetes.submitInDriver <p>(internal) Whether executing in <code>cluster</code> deploy mode</p> <p>Default: <code>false</code></p> <p><code>spark.kubernetes.submitInDriver</code> is <code>true</code> in BasicDriverFeatureStep.</p> <p>Used when:</p> <ul> <li><code>BasicDriverFeatureStep</code> is requested to getAdditionalPodSystemProperties</li> <li><code>KubernetesClusterManager</code> is requested for a SchedulerBackend</li> </ul>"},{"location":"overview/","text":"<p>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</p> <p>Apache Spark supports <code>Kubernetes</code> resource manager using KubernetesClusterManager (and KubernetesClusterSchedulerBackend) with k8s://-prefixed master URLs (that point at Kubernetes API servers).</p> <p>Spark on Kubernetes uses <code>TaskSchedulerImpl</code> (Apache Spark) for task scheduling.</p>","title":"Spark on Kubernetes"},{"location":"overview/#inactive-executor-pods","text":"","title":"Inactive Executor Pods <p>Spark on Kubernetes defines spark-exec-inactive label to mark executor pods as inactive after they have finished (successfully or not) but spark.kubernetes.executor.deleteOnTermination configuration property is <code>false</code> (when <code>ExecutorPodsLifecycleManager</code> is requested to handle executor pods snapshots).</p> <p>This label is used to skip executor pods when <code>PollRunnable</code> is requested to fetch status of all executor pods in a Spark application from Kubernetes API server.</p>"},{"location":"overview/#cluster-deploy-mode","text":"","title":"Cluster Deploy Mode <p>Spark on Kubernetes uses KubernetesClientApplication in <code>cluster</code> deploy mode (as the <code>SparkApplication</code> (Apache Spark) to run).</p>  <p>Note</p> <p>Use <code>spark-submit --deploy-mode</code>, <code>spark.submit.deployMode</code> or <code>DEPLOY_MODE</code> environment variable to specify the deploy mode of a Spark application.</p>"},{"location":"overview/#volumes","text":"","title":"Volumes <p>Volumes and volume mounts are configured using <code>spark.kubernetes.[type].volumes.</code>-prefixed configuration properties with <code>type</code> being <code>driver</code> or <code>executor</code> (for the driver and executor pods, respectively).</p> <p><code>KubernetesVolumeUtils</code> utility is used to extract volume configuration based on the volume type:</p>    Volume Type Configuration Property     <code>emptyDir</code> <code>[volumesPrefix].[volumeType].[volumeName].options.medium</code>     <code>[volumesPrefix].[volumeType].[volumeName].options.sizeLimit</code>   <code>hostPath</code> <code>[volumesPrefix].[volumeType].[volumeName].options.path</code>   <code>persistentVolumeClaim</code> <code>[volumesPrefix].[volumeType].[volumeName].options.claimName</code>    <p>Executor volumes (<code>spark.kubernetes.executor.volumes.</code>-prefixed configuration properties) are parsed right when <code>KubernetesConf</code> utility is used for a KubernetesDriverConf (and a driver pod created). That makes executor volumes required when driver volumes are defined.</p>"},{"location":"overview/#static-file-resources","text":"","title":"Static File Resources <p>File resources are resources with <code>file</code> or no URI scheme (that are then considered <code>file</code>-based indirectly).</p> <p>In Spark applications, file resources can be the primary resource (application jar, Python or R files) as well as files referenced by <code>spark.jars</code> and <code>spark.files</code> configuration properties (or their <code>--jars</code> and <code>--files</code> options of <code>spark-submit</code>, respectively).</p> <p>When deployed in <code>cluster</code> mode, Spark on Kubernetes uploads file resources of a Spark application to a Hadoop DFS-compatible file system defined by the required spark.kubernetes.file.upload.path configuration property.</p>"},{"location":"overview/#local-uri-scheme","text":"<p>A special case of static file resources are local resources that are resources with <code>local</code> URI scheme. They are considered already available on every Spark node (and are not added to a Spark file server for distribution when <code>SparkContext</code> is requested to add such file).</p> <p>In Spark on Kubernetes, <code>local</code> resources are used for primary application resource that are already included in a container image.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar\n</code></pre>","title":"Local URI Scheme"},{"location":"overview/#executor-pods-state-synchronization","text":"","title":"Executor Pods State Synchronization <p>Spark on Kubernetes uses ExecutorPodsPollingSnapshotSource for polling Kubernetes API server for executor pods of a Spark application every polling interval (based on spark.kubernetes.executor.apiPollingInterval configuration property).</p> <p><code>ExecutorPodsPollingSnapshotSource</code> is given an ExecutorPodsSnapshotsStore that is requested to replace a snapshot regularly.</p> <p><code>ExecutorPodsSnapshotsStore</code> keeps track of executor pods state snapshots and allows subscribers to be regularly updated (e.g. ExecutorPodsAllocator and ExecutorPodsLifecycleManager).</p>"},{"location":"overview/#dynamic-allocation-of-executors","text":"","title":"Dynamic Allocation of Executors <p>Spark on Kubernetes supports Dynamic Allocation of Executors using ExecutorPodsAllocator.</p>  <p>The Internals of Apache Spark</p> <p>Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark.</p>"},{"location":"overview/#internal-resource-marker","text":"","title":"Internal Resource Marker <p>Spark on Kubernetes uses spark-internal special name in <code>cluster</code> deploy mode for internal application resources (that are supposed to be part of an image).</p> <p>Given renameMainAppResource, <code>DriverCommandFeatureStep</code> will re-write local <code>file</code>-scheme-based primary application resources to <code>spark-internal</code> special name when requested for the base driver container (for a <code>JavaMainAppResource</code> application).</p>"},{"location":"overview/#demo","text":"","title":"Demo <p>This demo is a follow-up to Demo: Running Spark Application on minikube. Run it first.</p> <p>Note <code>--deploy-mode cluster</code> and the application jar is \"locally resolvable\" (i.e. uses <code>file:</code> scheme indirectly).</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name spark-docker-example \\\n  --class meetup.SparkApp \\\n  --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\\n  --conf spark.kubernetes.context=minikube \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\\n  --verbose \\\n  ~/dev/meetups/spark-meetup/spark-docker-example/target/scala-2.12/spark-docker-example_2.12-0.1.0.jar\n</code></pre> <pre><code>$ kubectl get po -l spark-role=driver\nNAME                                           READY   STATUS   RESTARTS   AGE\nspark-docker-example-dfd7d076e7099718-driver   0/1     Error    0          7m25s\n</code></pre> <p>Note <code>spark-internal</code> in the below output.</p> <pre><code>$ kubectl describe po spark-docker-example-dfd7d076e7099718-driver\n...\nContainers:\n  spark-kubernetes-driver:\n    ...\n    Args:\n      driver\n      --properties-file\n      /opt/spark/conf/spark.properties\n      --class\n      meetup.SparkApp\n      spark-internal\n...\n</code></pre>"},{"location":"overview/#resources","text":"","title":"Resources <ul> <li>Official documentation</li> <li>SPARK-33005 Kubernetes GA Preparation</li> <li>Spark on Kubernetes by Scott Haines</li> <li>(video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert</li> </ul>"},{"location":"spark-logging/","text":"<p>Spark uses log4j for logging.</p>  <p>Note</p> <p>Learn more on Spark Logging in The Internals of Apache Spark online book.</p>","title":"Logging"},{"location":"volumes/","text":"<p>Storage Volumes (Kubernetes) are configured using MountVolumesFeatureStep and KubernetesVolumeSpec.</p>","title":"Storage Volumes"},{"location":"volumes/#types-of-volumes","text":"<p>Spark on Kubernetes supports the following types of volumes:</p> <ul> <li><code>emptyDir</code></li> <li><code>hostPath</code></li> <li><code>nfs</code></li> <li><code>persistentVolumeClaim</code></li> </ul>","title":"Types of Volumes"},{"location":"volumes/#volume-configuration-prefixes","text":"","title":"Volume Configuration Prefixes <p>Spark on Kubernetes uses the prefixes for volume-related configuration properties for driver and executor pods:</p> <ul> <li><code>spark.kubernetes.driver.volumes.</code></li> <li><code>spark.kubernetes.executor.volumes.</code></li> </ul>"},{"location":"volumes/#claim-name-placeholders","text":"","title":"Claim Name Placeholders <p>MountVolumesFeatureStep defines OnDemand and SPARK_EXECUTOR_ID as placeholders for runtime-replaceable parts of the claim name of a KubernetesPVCVolumeConf.</p> <p>These placeholders allow for templating claim names to include parts to be replaced at deployment.</p> <p>Follow Demo: PersistentVolumeClaims to see the feature in action.</p>"},{"location":"volumes/#ondemand","text":"","title":"OnDemand <p><code>MountVolumesFeatureStep</code> replaces all <code>OnDemand</code>s with the following:</p>    Pod Replacement     driver <code>[resourceNamePrefix]-exec-[executorId]-pvc-[volumeIndex]</code>   executors <code>[resourceNamePrefix]-driver-pvc-[volumeIndex]</code>"},{"location":"volumes/#spark_executor_id","text":"","title":"SPARK_EXECUTOR_ID <p><code>MountVolumesFeatureStep</code> replaces all <code>SPARK_EXECUTOR_ID</code>s with executor IDs.</p>"},{"location":"demo/","text":"","title":"Demos"},{"location":"demo/#minikube","text":"<ul> <li>spark-shell on minikube</li> <li>Running Spark Application on minikube</li> <li>Spark and Local Filesystem in minikube</li> <li>PersistentVolumeClaims</li> <li>Running Spark Examples on minikube</li> <li>Running Spark Structured Streaming on minikube</li> </ul>","title":"minikube"},{"location":"demo/#google-kubernetes-engine","text":"<ul> <li>Running Spark Examples on Google Kubernetes Engine</li> <li>Deploying Spark Application to Google Kubernetes Engine</li> <li>Using Cloud Storage for Checkpoint Location in Spark Structured Streaming on Google Kubernetes Engine</li> </ul>","title":"Google Kubernetes Engine"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/","text":"<p>This demo shows the steps to deploy a Spark application to a Google Kubernetes Engine (GKE) cluster.</p>","title":"Demo: Deploying Spark Application to Google Kubernetes Engine"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#before-you-begin","text":"<p>Make sure to review the other demos (esp. Demo: Running Spark Examples on Google Kubernetes Engine) to get some experience with Spark on Kubernetes and Google Kubernetes Engine.</p>","title":"Before you begin"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#build-spark-application-image","text":"<pre><code>sbt clean docker:publishLocal\n</code></pre> <p>List the images using <code>docker images</code>.</p> <pre><code>$ docker images \\\n  --filter=reference='$GCP_CR/*:*' \\\n  --format \"table {{.Repository}}\\t{{.Tag}}\"\nREPOSITORY                                                TAG\neu.gcr.io/spark-on-kubernetes-2021/spark-docker-example   0.1.0\neu.gcr.io/spark-on-kubernetes-2021/spark                  v3.0.1\n</code></pre>","title":"Build Spark Application Image"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#pushing-image-to-container-registry","text":"<p>Upload the image to a registry so that your GKE cluster can download and run the container image (as described in Pushing the Docker image to Container Registry).</p> <pre><code>gcloud auth configure-docker\n</code></pre> <pre><code>$ sbt docker:publish\n...\n[info] Built image eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example with tags [0.1.0]\n[info] The push refers to repository [eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example]\n...\n[info] Published image eu.gcr.io/spark-on-kubernetes-2021/spark-docker-example:0.1.0\n</code></pre> <p>View the images in the repository.</p> <pre><code>$ gcloud container images list --repository $GCP_CR\nNAME\neu.gcr.io/spark-on-kubernetes-2021/spark\neu.gcr.io/spark-on-kubernetes-2021/spark-docker-example\n</code></pre>","title":"Pushing Image to Container Registry"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#creating-google-kubernetes-engine-cluster","text":"<p>Create a GKE cluster to run the Spark application.</p> <pre><code>export CLUSTER_NAME=spark-demo-cluster\n</code></pre> <pre><code>gcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version=1.17.15-gke.800 \\\n  --machine-type=c2-standard-4\n</code></pre>","title":"Creating Google Kubernetes Engine Cluster"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#deploying-spark-application-to-gke","text":"<p>Let's deploy the Docker image of the Spark application to the GKE cluster.</p>  <p>Important</p> <p>Create the required Kubernetes resources to run Spark applications as described in Demo: Running Spark Examples on Google Kubernetes Engine</p>  <pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>export K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}')\nexport DEMO_POD_NAME=spark-demo-gke\nexport CONTAINER_IMAGE=$GCP_CR/spark-docker-example:0.1.0\n</code></pre> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $DEMO_POD_NAME \\\n  --class meetup.SparkApp \\\n  --conf spark.kubernetes.container.image=$CONTAINER_IMAGE \\\n  --conf spark.kubernetes.driver.pod.name=$DEMO_POD_NAME \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar\n</code></pre> <p>Watch the pods in another terminal.</p> <pre><code>k get po -w\n</code></pre>","title":"Deploying Spark Application to GKE"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#accessing-logs","text":"<p>Access the logs of the driver.</p> <pre><code>k logs -f $DEMO_POD_NAME\n</code></pre>","title":"Accessing Logs"},{"location":"demo/deploying-spark-application-to-google-kubernetes-engine/#cleaning-up","text":"<p>Delete the GKE cluster.</p> <pre><code>gcloud container clusters delete $CLUSTER_NAME --quiet\n</code></pre> <p>Delete the images.</p> <pre><code>gcloud container images delete $GCP_CR/spark:v3.0.1 --force-delete-tags --quiet\ngcloud container images delete $GCP_CR/spark-docker-example:0.1.0 --force-delete-tags --quiet\n</code></pre>","title":"Cleaning up"},{"location":"demo/persistentvolumeclaims/","text":"<p>This demo shows how to use PersistentVolumeClaims.</p> <p>From Persistent Volumes:</p>  <p>minikube supports PersistentVolumes of type hostPath out of the box. These PersistentVolumes are mapped to a directory inside the running minikube instance</p>  <p>The demo uses OnDemand claim name placeholder to create different PersistentVolume claim names for the executors at deployment.</p> <p>From Binding:</p>  <p>Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.</p> <p>Claims will remain unbound indefinitely if a matching volume does not exist.</p>  <p>Given the demo uses 2 executors you could simply create 2 persistent volumes and be done. More executors would beg for some automation and Kubernetes supports this use case with Dynamic Volume Provisioning:</p>  <p>Dynamic volume provisioning allows storage volumes to be created on-demand.</p>  <p>In this demo you simply create two PVs to get going.</p>","title":"Demo: PersistentVolumeClaims"},{"location":"demo/persistentvolumeclaims/#before-you-begin","text":"<p>It is assumed that you have finished the following:</p> <ul> <li>Demo: Running Spark Application on minikube</li> </ul>","title":"Before you begin"},{"location":"demo/persistentvolumeclaims/#start-cluster","text":"<pre><code>minikube start\n</code></pre> <p>Switch the Kubernetes namespace to ours.</p> <pre><code>kubens spark-demo\n</code></pre>","title":"Start Cluster"},{"location":"demo/persistentvolumeclaims/#environment-variables","text":"<pre><code>export K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}')\n\nexport POD_NAME=meetup-spark-app\nexport IMAGE_NAME=$POD_NAME:0.1.0\n\nexport VOLUME_NAME=my-pv\nexport MOUNT_PATH=/mnt/data\nexport PVC_STORAGE_CLASS=manual\nexport PVC_SIZE_LIMIT=1Gi\n</code></pre>","title":"Environment Variables"},{"location":"demo/persistentvolumeclaims/#create-persistentvolumes","text":"<p>The following commands are a copy of Configure a Pod to Use a PersistentVolume for Storage (with some minor changes).</p> <pre><code>minikube ssh\n</code></pre> <pre><code>sudo mkdir /data/pv0001\n</code></pre> <pre><code>sudo sh -c \"echo 'Hello from Kubernetes storage' &gt; /data/pv0001/hello-message\"\n</code></pre> <p>Create a PersistentVolume as described in Configure a Pod to Use a PersistentVolume for Storage.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0001\nspec:\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 2Gi\n  hostPath:\n    path: /data/pv0001/\n  storageClassName: manual\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0002\nspec:\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 2Gi\n  hostPath:\n    path: /data/pv0001/\n  storageClassName: manual\n</code></pre> <pre><code>k apply -f k8s/pvs.yaml\n</code></pre> <pre><code>k get pv\n</code></pre> <pre><code>NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\npv0001   2Gi        RWO            Retain           Available           manual                  5s\npv0002   2Gi        RWO            Retain           Available           manual                  5s\n</code></pre> <pre><code>k describe pv pv0001\n</code></pre> <pre><code>Name:            pv0001\nLabels:          &lt;none&gt;\nAnnotations:     &lt;none&gt;\nFinalizers:      [kubernetes.io/pv-protection]\nStorageClass:    manual\nStatus:          Available\nClaim:\nReclaim Policy:  Retain\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        2Gi\nNode Affinity:   &lt;none&gt;\nMessage:\nSource:\n    Type:          HostPath (bare host directory volume)\n    Path:          /data/pv0001/\n    HostPathType:\nEvents:            &lt;none&gt;\n</code></pre>","title":"Create PersistentVolumes"},{"location":"demo/persistentvolumeclaims/#claim-persistent-volume","text":"<p>You're going to use <code>spark-shell</code> Spark application and executors only are provisioned on Kubernetes.</p> <pre><code>cd $SPARK_HOME\n</code></pre> <p>Spark on Kubernetes sets up Kubernetes' persistentVolumeClaim using <code>spark.kubernetes.executor.volumes</code>-prefixed configuration properties for executor pods.</p> <p>Spark on Kubernetes uses 2 executors by default (<code>--num-executors 2</code>) and that is why the demo uses <code>OnDemand</code> claim name to generate different PV claim names at deployment.</p>","title":"Claim Persistent Volume"},{"location":"demo/persistentvolumeclaims/#watch-persistent-volume-claims","text":"<p>In a separate terminal use the following command to watch persistent volume claims as they are created.</p> <pre><code>k get pvc -w\n</code></pre> <p>The output will be similar to the following:</p> <pre><code>NAME                                        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nspark-shell-de6ed2783a61d962-exec-1-pvc-0   Pending                                      manual         0s\nspark-shell-de6ed2783a61d962-exec-1-pvc-0   Pending   pv0001   0                         manual         0s\nspark-shell-de6ed2783a61d962-exec-1-pvc-0   Bound     pv0001   2Gi        RWO            manual         0s\nspark-shell-de6ed2783a61d962-exec-2-pvc-0   Pending                                      manual         0s\nspark-shell-de6ed2783a61d962-exec-2-pvc-0   Pending   pv0002   0                         manual         0s\nspark-shell-de6ed2783a61d962-exec-2-pvc-0   Bound     pv0002   2Gi        RWO            manual         0s\n</code></pre>","title":"Watch Persistent Volume Claims"},{"location":"demo/persistentvolumeclaims/#start-spark-shell","text":"<pre><code>./bin/spark-shell \\\n  --master k8s://$K8S_SERVER \\\n  --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.$VOLUME_NAME.mount.path=$MOUNT_PATH \\\n  --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.$VOLUME_NAME.options.claimName=OnDemand \\\n  --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.$VOLUME_NAME.options.storageClass=$PVC_STORAGE_CLASS \\\n  --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.$VOLUME_NAME.options.sizeLimit=$PVC_SIZE_LIMIT \\\n  --conf spark.kubernetes.container.image=$IMAGE_NAME \\\n  --conf spark.kubernetes.context=minikube \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose\n</code></pre>  <p>Note</p> <p>You have to recreate the persistent volumes (Clean Up and Create PersistentVolumes) before running <code>spark-shell</code> again (per Retain reclaim policy).</p>","title":"Start Spark Shell"},{"location":"demo/persistentvolumeclaims/#review-kubernetes-resources","text":"","title":"Review Kubernetes Resources"},{"location":"demo/persistentvolumeclaims/#persistentvolumes","text":"<pre><code>k describe pv $(k get pv -o=jsonpath='{.items[0].metadata.name}')\n</code></pre> <pre><code>Name:            pv0001\nLabels:          &lt;none&gt;\nAnnotations:     pv.kubernetes.io/bound-by-controller: yes\nFinalizers:      [kubernetes.io/pv-protection]\nStorageClass:    manual\nStatus:          Bound\nClaim:           spark-demo/spark-shell-de6ed2783a61d962-exec-1-pvc-0\nReclaim Policy:  Retain\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        2Gi\nNode Affinity:   &lt;none&gt;\nMessage:\nSource:\n    Type:          HostPath (bare host directory volume)\n    Path:          /data/pv0001/\n    HostPathType:\nEvents:            &lt;none&gt;\n</code></pre>","title":"persistentVolumes"},{"location":"demo/persistentvolumeclaims/#persistentvolumeclaims","text":"<pre><code>k get pvc\n</code></pre> <pre><code>NAME                                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nspark-shell-de6ed2783a61d962-exec-1-pvc-0   Bound    pv0001   2Gi        RWO            manual         2m19s\nspark-shell-de6ed2783a61d962-exec-2-pvc-0   Bound    pv0002   2Gi        RWO            manual         2m19s\n</code></pre> <pre><code>k describe pvc $(k get pvc -o=jsonpath='{.items[0].metadata.name}')\n</code></pre> <pre><code>Name:          spark-shell-de6ed2783a61d962-exec-1-pvc-0\nNamespace:     spark-demo\nStorageClass:  manual\nStatus:        Bound\nVolume:        pv0001\nLabels:        &lt;none&gt;\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n               pv.kubernetes.io/bound-by-controller: yes\nFinalizers:    [kubernetes.io/pvc-protection]\nCapacity:      2Gi\nAccess Modes:  RWO\nVolumeMode:    Filesystem\nUsed By:       spark-shell-de6ed2783a61d962-exec-1\nEvents:        &lt;none&gt;\n</code></pre>","title":"persistentVolumeClaims"},{"location":"demo/persistentvolumeclaims/#access-persistentvolume","text":"","title":"Access PersistentVolume"},{"location":"demo/persistentvolumeclaims/#command-line","text":"<pre><code>k exec -ti $(k get po -o name) -- cat $MOUNT_PATH/hello-message\n</code></pre> <pre><code>Hello from Kubernetes storage\n</code></pre>","title":"Command Line"},{"location":"demo/persistentvolumeclaims/#spark-shell","text":"<p>Note</p> <p>Wish I knew how to show that the only executor has got access to the mounted volume, but nothing comes to my mind. If you happen to know how to demo it, please contact me at jacek@japila.pl. Thank you! \u2764\ufe0f</p>","title":"Spark Shell"},{"location":"demo/persistentvolumeclaims/#clean-up","text":"<pre><code>k delete po --all\nk delete pvc --all\nk delete pv --all\n</code></pre> <p>Clean up the cluster as described in Demo: spark-shell on minikube.</p> <p>That's it. Congratulations!</p>","title":"Clean Up"},{"location":"demo/running-spark-application-on-minikube/","text":"<p>This demo shows how to deploy a Spark application to Kubernetes (using minikube).</p>","title":"Demo: Running Spark Application on minikube"},{"location":"demo/running-spark-application-on-minikube/#before-you-begin","text":"<p>It is assumed that you have finished the following:</p> <ul> <li>Demo: spark-shell on minikube</li> </ul>","title":"Before you begin"},{"location":"demo/running-spark-application-on-minikube/#start-cluster","text":"<p>Unless already started, start minikube.</p> <pre><code>minikube start\n</code></pre>","title":"Start Cluster"},{"location":"demo/running-spark-application-on-minikube/#build-spark-application-image","text":"<p>Make sure you've got a Spark image available in minikube's Docker registry.</p> <p>Point the shell to minikube's Docker daemon and make sure there is the Spark image (that your Spark application project uses).</p> <pre><code>eval $(minikube -p minikube docker-env)\n</code></pre> <p>List the Spark image.</p> <pre><code>docker images spark\n</code></pre> <pre><code>REPOSITORY   TAG       IMAGE ID       CREATED       SIZE\nspark        v3.2.0    b3412e410d67   2 hours ago   524MB\n</code></pre> <p>Use this image in your Spark application:</p> <pre><code>FROM spark:v3.2.0\n</code></pre> <p>Build and push the Docker image of your Spark application project to minikube's Docker repository. The following command assumes that you use Spark on Kubernetes Demos project.</p> <pre><code>sbt clean \\\n    'set Docker/dockerRepository in `meetup-spark-app` := None' \\\n    meetup-spark-app/docker:publishLocal\n</code></pre> <p>List the images and make sure that the image of your Spark application project is available.</p> <pre><code>docker images 'meetup*'\n</code></pre> <pre><code>REPOSITORY         TAG       IMAGE ID       CREATED          SIZE\nmeetup-spark-app   0.1.0     3a867debc6c0   11 seconds ago   524MB\n</code></pre>","title":"Build Spark Application Image"},{"location":"demo/running-spark-application-on-minikube/#docker-image-inspect","text":"<p>Use docker image inspect command to display detailed information on the Spark application image.</p> <pre><code>docker image inspect meetup-spark-app:0.1.0\n</code></pre>","title":"docker image inspect"},{"location":"demo/running-spark-application-on-minikube/#docker-image-history","text":"<p>Use docker image history command to show the history of the Spark application image.</p> <pre><code>docker image history meetup-spark-app:0.1.0\n</code></pre>","title":"docker image history"},{"location":"demo/running-spark-application-on-minikube/#create-kubernetes-resources","text":"<p>Create required Kubernetes resources to run a Spark application.</p> Spark official documentation<p>Learn more from the Spark official documentation.</p>  <p>Make sure to create the required Kubernetes resources (a service account and a cluster role binding) as without them you surely run into the following exception message:</p> <pre><code>Forbidden!Configured service account doesn't have access. Service account may have been revoked.\n</code></pre>","title":"Create Kubernetes Resources"},{"location":"demo/running-spark-application-on-minikube/#declaratively","text":"<p>Use the following <code>k8s/rbac.yml</code> file (from the Spark on Kubernetes Demos project).</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-demo\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n  namespace: spark-demo\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: spark-role\n  namespace: spark-demo\nsubjects:\n  - kind: ServiceAccount\n    name: spark\n    namespace: spark-demo\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n---\n</code></pre> <p>Create the resources in the Kubernetes cluster.</p> <pre><code>k create -f k8s/rbac.yml\n</code></pre>  <p>Tip</p> <p>With declarative approach (using <code>rbac.yml</code>) cleaning up becomes as simple as <code>k delete -f rbac.yml</code>.</p>","title":"Declaratively"},{"location":"demo/running-spark-application-on-minikube/#imperatively","text":"<pre><code>k create ns spark-demo\n</code></pre> <pre><code>k create serviceaccount spark -n spark-demo\n</code></pre> <pre><code>k create clusterrolebinding spark-role \\\n  --clusterrole edit \\\n  --serviceaccount spark-demo:spark \\\n  -n spark-demo\n</code></pre>","title":"Imperatively"},{"location":"demo/running-spark-application-on-minikube/#submit-spark-application-to-minikube","text":"<pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}')\n\nexport POD_NAME=meetup-spark-app\nexport IMAGE_NAME=$POD_NAME:0.1.0\n</code></pre> <p>Please note the configuration properties (some not really necessary but make the demo easier to guide you through, e.g. spark.kubernetes.driver.pod.name).</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $POD_NAME \\\n  --class meetup.SparkApp \\\n  --conf spark.kubernetes.container.image=$IMAGE_NAME \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.context=minikube \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/spark/jars/meetup.meetup-spark-app-0.1.0.jar STOP_THE_SPARKCONTEXT\n</code></pre>  <p>Important</p> <p><code>STOP_THE_SPARKCONTEXT</code> application argument is to stop the <code>SparkContext</code> and the driver. The following commands may not work if executed with the argument since the Spark application is stopped.</p> <p>Leave it out if you want to play with the commands that follow.</p>  <p>If all goes fine you should soon see <code>termination reason: Completed</code> message.</p> <pre><code>21/03/08 14:35:35 INFO LoggingPodStatusWatcherImpl: State changed, new state:\n     pod name: meetup-spark-app\n     namespace: spark-demo\n     labels: spark-app-selector -&gt; spark-5eba470d52c64518a555951d011ca785, spark-role -&gt; driver\n     pod uid: d61085f6-5764-4320-b77e-02dcd8334382\n     creation time: 2021-03-08T13:35:25Z\n     service account name: spark\n     volumes: spark-local-dir-1, spark-conf-volume-driver, spark-token-kzmdd\n     node name: minikube\n     start time: 2021-03-08T13:35:25Z\n     phase: Succeeded\n     container status:\n         container name: spark-kubernetes-driver\n         container image: meetup-spark-app:0.1.0\n         container state: terminated\n         container started at: 2021-03-08T13:35:27Z\n         container finished at: 2021-03-08T13:35:35Z\n         exit code: 0\n         termination reason: Completed\n21/03/08 14:35:35 INFO LoggingPodStatusWatcherImpl: Application status for spark-5eba470d52c64518a555951d011ca785 (phase: Succeeded)\n21/03/08 14:35:35 INFO LoggingPodStatusWatcherImpl: Container final statuses:\n\n\n     container name: spark-kubernetes-driver\n     container image: meetup-spark-app:0.1.0\n     container state: terminated\n     container started at: 2021-03-08T13:35:27Z\n     container finished at: 2021-03-08T13:35:35Z\n     exit code: 0\n     termination reason: Completed\n21/03/08 14:35:35 INFO LoggingPodStatusWatcherImpl: Application meetup-spark-app with submission ID spark-demo:meetup-spark-app finished\n21/03/08 14:35:35 DEBUG LoggingPodStatusWatcherImpl: Stopping watching application spark-5eba470d52c64518a555951d011ca785 with last-observed phase Succeeded\n</code></pre>","title":"Submit Spark Application to minikube"},{"location":"demo/running-spark-application-on-minikube/#accessing-web-ui","text":"<pre><code>k port-forward $POD_NAME 4040:4040\n</code></pre> <p>Open http://localhost:4040.</p>","title":"Accessing web UI"},{"location":"demo/running-spark-application-on-minikube/#accessing-logs","text":"<p>Access the logs of the driver.</p> <pre><code>k logs -f $POD_NAME\n</code></pre>","title":"Accessing Logs"},{"location":"demo/running-spark-application-on-minikube/#reviewing-spark-application-configuration","text":"","title":"Reviewing Spark Application Configuration"},{"location":"demo/running-spark-application-on-minikube/#configmap","text":"<pre><code>CONFIG_MAP=$(k get cm -o name | grep spark-drv)\n\nk describe $CONFIG_MAP\n</code></pre>","title":"ConfigMap"},{"location":"demo/running-spark-application-on-minikube/#volumes","text":"<p>Describe the driver pod and review volumes (<code>.spec.volumes</code>) and volume mounts (<code>.spec.containers[].volumeMounts</code>).</p> <pre><code>k describe po $POD_NAME\n</code></pre> <pre><code>k get po $POD_NAME -o=jsonpath='{.spec.volumes}' | jq\n</code></pre> <pre><code>[\n  {\n    \"emptyDir\": {},\n    \"name\": \"spark-local-dir-1\"\n  },\n  {\n    \"configMap\": {\n      \"defaultMode\": 420,\n      \"items\": [\n        {\n          \"key\": \"log4j.properties\",\n          \"mode\": 420,\n          \"path\": \"log4j.properties\"\n        },\n        {\n          \"key\": \"spark.properties\",\n          \"mode\": 420,\n          \"path\": \"spark.properties\"\n        }\n      ],\n      \"name\": \"spark-drv-b5cf5b7834f5a32d-conf-map\"\n    },\n    \"name\": \"spark-conf-volume-driver\"\n  },\n  {\n    \"name\": \"spark-token-sfqc9\",\n    \"secret\": {\n      \"defaultMode\": 420,\n      \"secretName\": \"spark-token-sfqc9\"\n    }\n  }\n]\n</code></pre>","title":"Volumes"},{"location":"demo/running-spark-application-on-minikube/#volume-mounts","text":"<pre><code>k get po $POD_NAME -o=jsonpath='{.spec.containers[].volumeMounts}' | jq\n</code></pre> <pre><code>[\n  {\n    \"mountPath\": \"/var/data/spark-e32e4d73-af0e-43ce-8ffa-f4b64c642b86\",\n    \"name\": \"spark-local-dir-1\"\n  },\n  {\n    \"mountPath\": \"/opt/spark/conf\",\n    \"name\": \"spark-conf-volume-driver\"\n  },\n  {\n    \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n    \"name\": \"spark-token-sfqc9\",\n    \"readOnly\": true\n  }\n]\n</code></pre>","title":"Volume Mounts"},{"location":"demo/running-spark-application-on-minikube/#services","text":"<pre><code>k get services\n</code></pre> <pre><code>NAME                                           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGE\nmeetup-spark-app-ab8a3a7834f5a022-driver-svc   ClusterIP   None         &lt;none&gt;        7078/TCP,7079/TCP,4040/TCP   31m\n</code></pre>","title":"Services"},{"location":"demo/running-spark-application-on-minikube/#spark-application-management","text":"<pre><code>K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}')\n</code></pre>","title":"Spark Application Management"},{"location":"demo/running-spark-application-on-minikube/#application-status","text":"<pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --status \"spark-demo:$POD_NAME\"\n</code></pre> <pre><code>Application status (driver):\n     pod name: meetup-spark-app\n     namespace: spark-demo\n     labels: spark-app-selector -&gt; spark-0df2be7b2d8d40299e7a406564c9833c, spark-role -&gt; driver\n     pod uid: 30a749a3-1060-49a7-b502-4a054ea33d30\n     creation time: 2021-02-09T13:18:14Z\n     service account name: spark\n     volumes: spark-local-dir-1, spark-conf-volume-driver, spark-token-hqc6k\n     node name: minikube\n     start time: 2021-02-09T13:18:14Z\n     phase: Running\n     container status:\n         container name: spark-kubernetes-driver\n         container image: meetup-spark-app:0.1.0\n         container state: running\n         container started at: 2021-02-09T13:18:15Z\n</code></pre>","title":"Application Status"},{"location":"demo/running-spark-application-on-minikube/#stop-spark-application","text":"<pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --kill \"spark-demo:$POD_NAME\"\n</code></pre>","title":"Stop Spark Application"},{"location":"demo/running-spark-application-on-minikube/#clean-up","text":"<p>Clean up the cluster as described in Demo: spark-shell on minikube.</p> <p>That's it. Congratulations!</p>","title":"Clean Up"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/","text":"<p>This demo shows how to run the official Spark Examples on a Kubernetes cluster on Google Kubernetes Engine (GKE).</p> <p>This demo focuses on the ubiquitous SparkPi example, but should let you run the other sample Spark applications too.</p> <pre><code>./bin/run-example SparkPi 10\n</code></pre>","title":"Demo: Running Spark Examples on Google Kubernetes Engine"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#before-you-begin","text":"<p>Open up a Google Cloud project in the Google Cloud Console and enable the Kubernetes Engine API (as described in Deploying a containerized web application).</p> <p>Review Demo: Running Spark Examples on minikube to build a basic understanding of the process of deploying Spark applications to a local Kubernetes cluster using minikube.</p>","title":"Before you begin"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#prepare-spark-base-image","text":"<pre><code>export PROJECT_ID=$(gcloud info --format='value(config.project)')\nexport GCP_CR=eu.gcr.io/${PROJECT_ID}\n</code></pre>","title":"Prepare Spark Base Image"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#build-spark-image","text":"<p>Build and push a Apache Spark base image to Container Registry on Google Cloud Platform.</p> <pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>./bin/docker-image-tool.sh \\\n  -r $GCP_CR \\\n  -t v3.2.0 \\\n  build\n</code></pre> <p>List the images using docker images command (and some other fancy options).</p> <pre><code>docker images \"$GCP_CR/*\" --format \"table {{.Repository}}\\t{{.Tag}}\"\n</code></pre> <pre><code>REPOSITORY                                 TAG\neu.gcr.io/spark-on-kubernetes-2021/spark   v3.2.0\n</code></pre>","title":"Build Spark Image"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#push-spark-image-to-container-registry","text":"<p>Push the container image to the Container Registry so that a GKE cluster can run it in a pod (as described in Pushing the Docker image to Container Registry).</p> <pre><code>gcloud auth configure-docker\n</code></pre> <pre><code>./bin/docker-image-tool.sh \\\n  -r $GCP_CR \\\n  -t v3.2.0 \\\n  push\n</code></pre>","title":"Push Spark Image to Container Registry"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#list-images","text":"<p>Use gcloud container images list to list the Spark image in the repository.</p> <pre><code>gcloud container images list --repository $GCP_CR\n</code></pre> <pre><code>NAME\neu.gcr.io/spark-on-kubernetes-2021/spark\n</code></pre>","title":"List Images"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#list-tags","text":"<p>Use gcloud container images list-tags to list tags and digests for the specified image.</p> <pre><code>gcloud container images list-tags $GCP_CR/spark\n</code></pre> <pre><code>DIGEST        TAGS        TIMESTAMP\n9a50d1435bbe  v3.2.0  2021-01-26T13:02:11\n</code></pre>","title":"List Tags"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#describe-spark-image","text":"<p>Use gcloud container images describe to list information about the Spark image.</p> <pre><code>gcloud container images describe $GCP_CR/spark:v3.2.0\n</code></pre> <pre><code>image_summary:\n  digest: sha256:9a50d1435bbe81dd3a23d3e43c244a0bfc37e14fb3754b68431cbf8510360b84\n  fully_qualified_digest: eu.gcr.io/spark-on-kubernetes-2021/spark@sha256:9a50d1435bbe81dd3a23d3e43c244a0bfc37e14fb3754b68431cbf8510360b84\n  registry: eu.gcr.io\n  repository: spark-on-kubernetes-2021/spark\n</code></pre>","title":"Describe Spark Image"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#create-kubernetes-cluster","text":"<pre><code>export CLUSTER_NAME=spark-examples-cluster\n</code></pre> <p>The default version of Kubernetes varies per Google Cloud zone and is often older than the latest stable release. A cluster version can be changed using <code>--cluster-version</code> option.</p> <p>Use <code>gcloud container get-server-config</code> command to check which Kubernetes versions are available and default in your zone.</p> <pre><code>gcloud container get-server-config\n</code></pre>  <p>Tip</p> <p>Use latest version alias to use the highest supported Kubernetes version currently available on GKE in the cluster's zone or region.</p>  <pre><code>gcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version=latest\n</code></pre> <p>Wait a few minutes before the GKE cluster is ready. In the end, you should see a summary of the cluster.</p>","title":"Create Kubernetes Cluster"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#list-clusters","text":"<pre><code>gcloud container clusters list\n</code></pre> <pre><code>NAME                    LOCATION        MASTER_VERSION   MASTER_IP       MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\nspark-examples-cluster  europe-west3-b  1.20.7-gke.1800  35.246.230.110  e2-medium     1.20.7-gke.1800  3          RUNNING\n</code></pre>","title":"List Clusters"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#config-view","text":"<p>Review the configuration of the GKE cluster.</p> <pre><code>k config view\n</code></pre>","title":"Config View"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#compute-instances","text":"<p>Review the cluster's VM instances.</p> <pre><code>gcloud compute instances list\n</code></pre>","title":"Compute Instances"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#run-sparkpi-on-gke","text":"<p>Note</p> <p>What follows is a more succinct version of Demo: Running Spark Application on minikube.</p>","title":"Run SparkPi on GKE"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#create-kubernetes-resources","text":"<p>Use the following yaml configuration file (<code>rbac.yml</code>) to create required resources.</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-demo\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n  namespace: spark-demo\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: spark-role\n  namespace: spark-demo\nsubjects:\n  - kind: ServiceAccount\n    name: spark\n    namespace: spark-demo\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n---\n</code></pre> <p>Use <code>k create</code> to create the Kubernetes resources.</p> <pre><code>k create -f k8s/rbac.yml\n</code></pre>","title":"Create Kubernetes Resources"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#submit-sparkpi","text":"<pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>export K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}')\nexport POD_NAME=spark-examples-pod\nexport SPARK_IMAGE=$GCP_CR/spark:v3.2.0\n</code></pre> <p>Before the real <code>spark-submit</code> happens, open another terminal and watch the pods being created and terminated while the Spark application is going up and down. Don't forget to use <code>spark-demo</code> namespace.</p> <pre><code>k get po -n spark-demo -w\n</code></pre>  <p>Important</p> <p>For the time being we're going to use <code>spark-submit</code> not <code>run-example</code>. See Demo: Running Spark Examples on minikube for more information.</p>  <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $POD_NAME \\\n  --class org.apache.spark.examples.SparkPi \\\n  --conf spark.kubernetes.driver.request.cores=400m \\\n  --conf spark.kubernetes.executor.request.cores=100m \\\n  --conf spark.kubernetes.container.image=$SPARK_IMAGE \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/spark/examples/jars/spark-examples_2.12-3.2.0.jar 10\n</code></pre>  <p>Note</p> <p><code>spark.kubernetes.*.request.cores</code> configuration properties were required due to the default machine type of a GKE cluster is too small CPU-wise. You may consider another machine type for a GKE cluster (e.g. <code>c2-standard-4</code>).</p>  <p>In the end, review the logs.</p> <pre><code>k logs -n spark-demo $POD_NAME\n</code></pre>","title":"Submit SparkPi"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#kubernetesclientexception-pods-spark-examples-pod-already-exists","text":"<p>While executing the demo you may run into the following exception:</p> <pre><code>Exception in thread \"main\" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://35.246.230.110/api/v1/namespaces/spark-demo/pods. Message: pods \"spark-examples-pod\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=spark-examples-pod, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"spark-examples-pod\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:589)\n    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:528)\n    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:492)\n    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:451)\n    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:252)\n    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:879)\n    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:341)\n    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:84)\n    at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:139)\n    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$3(KubernetesClientApplication.scala:213)\n    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$3$adapted(KubernetesClientApplication.scala:207)\n    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:207)\n    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:179)\n    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)\n    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n</code></pre> <p>As the error message says:</p> <pre><code>Message: pods \"spark-examples-pod\" already exists.\n</code></pre> <p>You are supposed to delete the pod before any other future demo attempts. The driver pods are left over (after a Spark application is finished) for log or configuration review.</p> <pre><code>k delete po -n spark-demo $POD_NAME\n</code></pre>","title":"KubernetesClientException: pods \"spark-examples-pod\" already exists"},{"location":"demo/running-spark-examples-on-google-kubernetes-engine/#clean-up","text":"<p>Delete the GKE cluster.</p> <pre><code>gcloud container clusters delete $CLUSTER_NAME --quiet\n</code></pre> <p>Delete the images.</p> <pre><code>gcloud container images delete $SPARK_IMAGE --force-delete-tags --quiet\n</code></pre>","title":"Clean Up"},{"location":"demo/running-spark-examples-on-minikube/","text":"<p>This demo shows how to run the Spark example applications on minikube to advance from spark-shell on minikube to a more serious deployment (yet with no Spark development).</p> <p>This demo lets you explore deploying a Spark application (e.g. <code>SparkPi</code>) to Kubernetes in <code>cluster</code> deploy mode.</p>","title":"Demo: Running Spark Examples on minikube"},{"location":"demo/running-spark-examples-on-minikube/#before-you-begin","text":"<p>Start up minikube with necessary Kubernetes resources.</p> <p>Follow the steps in Demo: spark-shell on minikube and Demo: Running Spark Application on minikube:</p> <ol> <li>Start minikube</li> <li>Build Spark Image</li> <li>Create Kubernetes Resources</li> </ol>","title":"Before you begin"},{"location":"demo/running-spark-examples-on-minikube/#running-sparkpi-on-minikube","text":"<pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}')\n</code></pre> <p>Let's use an environment variable for the name of the pod to be more \"stable\" and predictable. It should make viewing logs and restarting Spark examples easier. Just change the environment variable or delete the pod and off you go!</p> <pre><code>export POD_NAME=a1\n</code></pre> WIP: No use of run-example without spark.kubernetes.file.upload.path<p>Using the <code>run-example</code> shell script to run the Spark examples will not work unless you define spark.kubernetes.file.upload.path configuration property. The reason is that <code>run-example</code> uses the <code>spark-examples</code> jar file that is found locally so <code>spark-submit</code> (that is used under the covers) has to upload the locally-available resource file to be available in a cluster.</p> <p>We'll get to it later. Consider it a work in progress.</p> <pre><code>./bin/run-example \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $POD_NAME \\\n  --jars local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar \\\n  --conf spark.kubernetes.container.image=spark:v3.2.0 \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  SparkPi 10\n</code></pre>  <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $POD_NAME \\\n  --class org.apache.spark.examples.SparkPi \\\n  --conf spark.kubernetes.container.image=spark:v3.2.0 \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10\n</code></pre> <p>In another terminal, use <code>k get po -w</code> to watch the pods of the driver and executors.</p> <pre><code>k get po -w\n</code></pre> <p>Review the logs of the driver (as long as the driver pod is up and running).</p> <pre><code>k logs $POD_NAME\n</code></pre> <p>For repeatable <code>SparkPi</code> executions, delete the driver pod using <code>k delete pod</code> command.</p> <pre><code>k delete po $POD_NAME\n</code></pre>  <p>spark.kubernetes.executor.deleteOnTermination Configuration Property</p> <p>Use spark.kubernetes.executor.deleteOnTermination configuration property to keep executor pods available once a Spark application is finished (e.g. for examination).</p>","title":"Running SparkPi on minikube"},{"location":"demo/running-spark-examples-on-minikube/#using-pod-templates","text":"<p>spark.kubernetes.driver.podTemplateFile configuration property allows to define a template file for driver pods (e.g. for multi-container pods).</p>  <p>spark.kubernetes.executor.podTemplateFile Configuration Property</p> <p>Use spark.kubernetes.executor.podTemplateFile configuration property for the template file of executor pods.</p>","title":"Using Pod Templates"},{"location":"demo/running-spark-examples-on-minikube/#pod-template","text":"<p>The following is a very basic pod template file. It is incorrect Spark-wise though (as it does not really allow submitting Spark applications) but does allow playing with pod templates.</p> <pre><code>spec:\n  containers:\n  - name: spark\n    image: busybox\n    command: ['sh', '-c', 'echo \"Hello, Spark on Kubernetes!\"']\n</code></pre> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --conf spark.kubernetes.driver.podTemplateFile=pod-template.yml \\\n  --name $POD_NAME \\\n  --class org.apache.spark.examples.SparkPi \\\n  --conf spark.kubernetes.container.image=spark:v3.2.0 \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10\n</code></pre> <pre><code>k logs $POD_NAME\n</code></pre> <pre><code>Hello, Spark on Kubernetes!\n</code></pre>","title":"Pod Template"},{"location":"demo/running-spark-examples-on-minikube/#container-resources","text":"<p>In cluster deploy mode, Spark on Kubernetes may use extra Non-Heap Memory Overhead in memory requirements of the driver pod (based on spark.kubernetes.memoryOverheadFactor configuration property).</p> <pre><code>k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq\n</code></pre> <pre><code>$ k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq\n{\n  \"limits\": {\n    \"memory\": \"1408Mi\"\n  },\n  \"requests\": {\n    \"cpu\": \"1\",\n    \"memory\": \"1408Mi\"\n  }\n}\n</code></pre> <p>Note that this extra memory requirements could be part of a pod template.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --driver-memory 1g \\\n  --conf spark.kubernetes.memoryOverheadFactor=0.5 \\\n  --name $POD_NAME \\\n  --class org.apache.spark.examples.SparkPi \\\n  --conf spark.kubernetes.container.image=spark:v3.2.0 \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar 10\n</code></pre> <pre><code>k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq\n</code></pre> <pre><code>$ k get po $POD_NAME -o=jsonpath='{.spec.containers[0].resources}' | jq\n{\n  \"limits\": {\n    \"memory\": \"1536Mi\"\n  },\n  \"requests\": {\n    \"cpu\": \"1\",\n    \"memory\": \"1536Mi\"\n  }\n}\n</code></pre>","title":"Container Resources"},{"location":"demo/running-spark-structured-streaming-on-minikube/","text":"<p>This demo shows how to run a Spark Structured Streaming application on minikube:</p> <ol> <li>spark.kubernetes.submission.waitAppCompletion configuration property</li> <li><code>spark-submit --status</code> and <code>--kill</code></li> </ol>","title":"Demo: Running Spark Structured Streaming on minikube"},{"location":"demo/running-spark-structured-streaming-on-minikube/#before-you-begin","text":"<p>It is assumed that you are familiar with the basics of Spark on Kubernetes and the other demos.</p>","title":"Before you begin"},{"location":"demo/running-spark-structured-streaming-on-minikube/#start-cluster","text":"<p>Start minikube.</p> <pre><code>minikube start\n</code></pre>","title":"Start Cluster"},{"location":"demo/running-spark-structured-streaming-on-minikube/#build-spark-application-image","text":"<p>Make sure you've got a Spark image available in minikube's Docker registry. Learn the steps in Demo: spark-shell on minikube.</p> <p>Point the shell to minikube's Docker daemon.</p> <pre><code>eval $(minikube -p minikube docker-env)\n</code></pre> <p>List the Spark image. Make sure it matches the version of Spark you want to work with.</p> <pre><code>docker images spark\n</code></pre> <pre><code>REPOSITORY   TAG          IMAGE ID       CREATED             SIZE\nspark        v3.2.0   e64950545e8f   About an hour ago   509MB\n</code></pre> <p>Publish the image of the Spark Structured Streaming application. It is project-dependent, and the project uses sbt with sbt-native-packager plugin.</p> <pre><code>sbt clean docker:publishLocal\n</code></pre> <p>List the images and make sure that the image of your Spark application project is available.</p> <pre><code>docker images spark-streams-demo\n</code></pre> <pre><code>REPOSITORY           TAG       IMAGE ID       CREATED         SIZE\nspark-streams-demo   0.1.0     20145c134ca9   4 minutes ago   515MB\n</code></pre>","title":"Build Spark Application Image"},{"location":"demo/running-spark-structured-streaming-on-minikube/#submit-spark-application-to-minikube","text":"<pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}')\n</code></pre> <p>Make sure that the Kubernetes resources (e.g. a namespace and a service account) are available in the cluster. Learn more in Demo: Running Spark Application on minikube.</p> <pre><code>k create -f k8s/rbac.yml\n</code></pre> <p>The name of the pod is going to be based on the name of the container image for demo purposes. Pick what works for you.</p> <pre><code>export POD_NAME=spark-streams-demo\nexport IMAGE_NAME=$POD_NAME:0.1.0\n</code></pre> <p>You may optionally delete all pods (since we use a fixed name for the demo).</p> <pre><code>k delete po --all\n</code></pre> <p>One of the differences between streaming and batch Spark applications is that the Spark Structured Streaming application is supposed to never stop. That's why the demo uses spark.kubernetes.submission.waitAppCompletion configuration property.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $POD_NAME \\\n  --class meetup.SparkStreamsApp \\\n  --conf spark.kubernetes.container.image=$IMAGE_NAME \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.context=minikube \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --conf spark.kubernetes.submission.waitAppCompletion=false \\\n  --verbose \\\n  local:///opt/spark/jars/meetup.spark-streams-demo-0.1.0.jar\n</code></pre> <p>In the end, you should be given a so-called submission ID that you're going to use with <code>spark-submit</code> tool (via K8SSparkSubmitOperation extension).</p> <pre><code>INFO LoggingPodStatusWatcherImpl: Deployed Spark application spark-streams-demo with submission ID spark-demo:spark-streams-demo into Kubernetes\n</code></pre> <p>Take a note of it as that is how you are going to monitor the application using Spark's <code>spark-submit --status</code> (and possibly kill it with <code>spark-submit --kill</code>).</p> <pre><code>export SUBMISSION_ID=spark-demo:spark-streams-demo\n</code></pre> <p>Once submitted, observe pods in another terminal. Make sure you use <code>spark-demo</code> namespace.</p> <pre><code>k get po -w -n spark-demo\n</code></pre>","title":"Submit Spark Application to minikube"},{"location":"demo/running-spark-structured-streaming-on-minikube/#request-status-of-spark-application","text":"<p>Use <code>spark-submit --status SUBMISSION_ID</code> to requests the status of the Spark driver in <code>cluster</code> deploy mode.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --status $SUBMISSION_ID\n</code></pre> <p>You should see something similar to the following:</p> <pre><code>Submitting a request for the status of submission spark-demo:spark-streams-demo in k8s://https://127.0.0.1:55004.\n21/01/18 12:16:27 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\nApplication status (driver):\n     pod name: spark-streams-demo\n     namespace: spark-demo\n     labels: spark-app-selector -&gt; spark-46ca76cc77c242509f27af3c506eb1f5, spark-role -&gt; driver\n     pod uid: 034ed206-5804-4e9d-ab68-ec56a7678b65\n     creation time: 2021-01-18T11:09:46Z\n     service account name: spark\n     volumes: spark-local-dir-1, spark-conf-volume, spark-token-888gj\n     node name: minikube\n     start time: 2021-01-18T11:09:46Z\n     phase: Running\n     container status:\n         container name: spark-kubernetes-driver\n         container image: spark-streams-demo:0.1.0\n         container state: running\n         container started at: 2021-01-18T11:09:47Z\n</code></pre>","title":"Request Status of Spark Application"},{"location":"demo/running-spark-structured-streaming-on-minikube/#kill-spark-application","text":"<p>In the end, you can <code>spark-submit --kill</code> the Spark Structured Streaming application.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --kill $SUBMISSION_ID\n</code></pre> <p>You should see something similar to the following:</p> <pre><code>Submitting a request to kill submission spark-demo:spark-streams-demo in k8s://https://127.0.0.1:55004. Grace period in secs: not set.\n</code></pre>","title":"Kill Spark Application"},{"location":"demo/running-spark-structured-streaming-on-minikube/#clean-up","text":"<p>Clean up the cluster as described in Demo: spark-shell on minikube.</p> <p>That's it. Congratulations!</p>","title":"Clean Up"},{"location":"demo/spark-and-local-filesystem-in-minikube/","text":"<p>The demo shows how to set up a Spark application on minikube to access files on a local filesystem.</p>  <p>Danger</p> <p>The demo stopped working for some reasons I cannot explain and sort out. The error message is the following:</p> <pre><code>Warning  Failed     13s   kubelet            Error: failed to start container \"spark-kubernetes-driver\": Error response from daemon: error while creating mount source path '/tmp/spark-k8s-demo/mount': mkdir /tmp/spark-k8s-demo: file exists\n</code></pre>  <p>The demo uses <code>spark-submit --files</code> and spark.kubernetes.file.upload.path configuration property to upload a static file to a directory that is then mounted to Spark application pods.</p> <p>Volumes in Kubernetes are directories which are accessible to the containers in a pod. In order to use a volume, you should specify the volumes to provide for the Pod in <code>.spec.volumes</code> and declare where to mount those volumes into containers in <code>.spec.containers[*].volumeMounts</code>.</p>","title":"Demo: Spark and Local Filesystem in minikube"},{"location":"demo/spark-and-local-filesystem-in-minikube/#before-you-begin","text":"<p>It is assumed that you have finished the following:</p> <ul> <li>Demo: Running Spark Application on minikube</li> </ul>","title":"Before you begin"},{"location":"demo/spark-and-local-filesystem-in-minikube/#start-cluster","text":"<pre><code>minikube start\n</code></pre>","title":"Start Cluster"},{"location":"demo/spark-and-local-filesystem-in-minikube/#environment-variables","text":"<pre><code>export K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}')\nexport POD_NAME=meetup-spark-app\nexport IMAGE_NAME=$POD_NAME:0.1.0\n\nexport SOURCE_DIR=/tmp/spark-k8s-demo\nexport VOLUME_TYPE=hostPath\nexport VOLUME_NAME=demo-host-mount\nexport MOUNT_PATH=$SOURCE_DIR/mount\n</code></pre>","title":"Environment Variables"},{"location":"demo/spark-and-local-filesystem-in-minikube/#mounting-filesystems","text":"<p>Let's kick things off by mounting a host directory (<code>/tmp/spark-k8s</code>) to minikube.</p> <p>Quoting Mounting filesystems of minikube's official documentation:</p>  <p>To mount a directory from the host into the guest use the <code>mount</code> subcommand</p>  <pre><code>minikube mount $SOURCE_DIR:$MOUNT_PATH\n</code></pre> <pre><code>\ud83d\udcc1  Mounting host path /tmp/spark-k8s-demo into VM as /tmp/spark-k8s-demo ...\n    \u25aa Mount type:\n    \u25aa User ID:      docker\n    \u25aa Group ID:     docker\n    \u25aa Version:      9p2000.L\n    \u25aa Message Size: 262144\n    \u25aa Permissions:  755 (-rwxr-xr-x)\n    \u25aa Options:      map[]\n    \u25aa Bind Address: 127.0.0.1:53819\n\ud83d\ude80  Userspace file server: ufs starting\n\u2705  Successfully mounted /tmp/spark-k8s-demo to /tmp/spark-k8s-demo\n\n\ud83d\udccc  NOTE: This process must stay alive for the mount to be accessible ...\n</code></pre>","title":"Mounting Filesystems"},{"location":"demo/spark-and-local-filesystem-in-minikube/#validating-mount","text":"<p>Use <code>minikube ssh</code> to validate the volume on the minikube's VM.</p> <pre><code>minikube ssh\n</code></pre> <pre><code>docker@minikube:~$ ls -ld /tmp/spark-k8s-demo\ndrwxr-xr-x 1 docker docker 96 Mar  9 14:11 /tmp/spark-k8s-demo\n</code></pre>","title":"Validating Mount"},{"location":"demo/spark-and-local-filesystem-in-minikube/#using-kubernetes-volumes","text":"<p>Quoting Using Kubernetes Volumes of Apache Spark's official documentation:</p>  <p>users can mount the following types of Kubernetes volumes into the driver and executor pods:</p> <ul> <li>hostPath: mounts a file or directory from the host node\u2019s filesystem into a pod.</li> <li>emptyDir: an initially empty volume created when a pod is assigned to a node.</li> <li>persistentVolumeClaim: used to mount a PersistentVolume into a pod.</li> </ul>","title":"Using Kubernetes Volumes"},{"location":"demo/spark-and-local-filesystem-in-minikube/#hostpath","text":"<p>Let's use Kubernetes' hostPath that requires <code>spark.kubernetes.*.volumes</code>-prefixed configuration properties for the driver and executor pods:</p> <pre><code>--conf spark.kubernetes.driver.volumes.$VOLUME_TYPE.$VOLUME_NAME.mount.path=$MOUNT_PATH\n--conf spark.kubernetes.driver.volumes.$VOLUME_TYPE.$VOLUME_NAME.options.path=$MOUNT_PATH\n</code></pre> <p>The demo uses configuration properties to set up a <code>hostPath</code> volume type (<code>$VOLUME_TYPE</code>) with <code>$VOLUME_NAME</code> name and <code>$MOUNT_PATH</code> path on the host (for the driver and executors separately).</p> <pre><code>cd $SPARK_HOME\n</code></pre> <p>The idea is to let <code>spark-submit</code> upload the <code>--files</code> to <code>spark.kubernetes.file.upload.path</code> directory that is available under the same directory (logically by name as on the host). That's why the names of the source and target mounted directories are the same.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --files test.me \\\n  --conf spark.kubernetes.file.upload.path=$SOURCE_DIR \\\n  --conf spark.kubernetes.driver.volumes.$VOLUME_TYPE.$VOLUME_NAME.mount.path=$MOUNT_PATH \\\n  --conf spark.kubernetes.driver.volumes.$VOLUME_TYPE.$VOLUME_NAME.options.path=$MOUNT_PATH \\\n  --conf spark.kubernetes.executor.volumes.$VOLUME_TYPE.$VOLUME_NAME.mount.path=$MOUNT_PATH \\\n  --conf spark.kubernetes.executor.volumes.$VOLUME_TYPE.$VOLUME_NAME.options.path=$MOUNT_PATH \\\n  --name $POD_NAME \\\n  --class meetup.SparkApp \\\n  --conf spark.kubernetes.container.image=$IMAGE_NAME \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.context=minikube \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --verbose \\\n  local:///opt/spark/jars/meetup.meetup-spark-app-0.1.0.jar\n</code></pre>","title":"hostPath"},{"location":"demo/spark-and-local-filesystem-in-minikube/#reviewing-volumes","text":"","title":"Reviewing Volumes"},{"location":"demo/spark-and-local-filesystem-in-minikube/#describe-pod","text":"<pre><code>k describe po $POD_NAME\n</code></pre>","title":"Describe Pod"},{"location":"demo/spark-and-local-filesystem-in-minikube/#pod-volumes","text":"<pre><code>k get po $POD_NAME -o=jsonpath='{.spec.volumes}' | jq\n</code></pre> <pre><code>[\n  {\n    \"hostPath\": {\n      \"path\": \"/tmp/spark-k8s-demo\",\n      \"type\": \"\"\n    },\n    \"name\": \"demo-host-mount\"\n  },\n  {\n    \"emptyDir\": {},\n    \"name\": \"spark-local-dir-1\"\n  },\n  {\n    \"configMap\": {\n      \"defaultMode\": 420,\n      \"items\": [\n        {\n          \"key\": \"log4j.properties\",\n          \"mode\": 420,\n          \"path\": \"log4j.properties\"\n        },\n        {\n          \"key\": \"spark.properties\",\n          \"mode\": 420,\n          \"path\": \"spark.properties\"\n        }\n      ],\n      \"name\": \"spark-drv-757769781753ba14-conf-map\"\n    },\n    \"name\": \"spark-conf-volume-driver\"\n  },\n  {\n    \"name\": \"spark-token-kzmdd\",\n    \"secret\": {\n      \"defaultMode\": 420,\n      \"secretName\": \"spark-token-kzmdd\"\n    }\n  }\n]\n</code></pre>","title":"Pod Volumes"},{"location":"demo/spark-and-local-filesystem-in-minikube/#volume-mounts","text":"<pre><code>k get po $POD_NAME -o=jsonpath='{.spec.containers[].volumeMounts}' | jq\n</code></pre> <pre><code>[\n  {\n    \"mountPath\": \"/tmp/spark-k8s-demo\",\n    \"name\": \"demo-host-mount\"\n  },\n  {\n    \"mountPath\": \"/var/data/spark-67727eb6-3ca5-4e72-8ea2-20179aca2831\",\n    \"name\": \"spark-local-dir-1\"\n  },\n  {\n    \"mountPath\": \"/opt/spark/conf\",\n    \"name\": \"spark-conf-volume-driver\"\n  },\n  {\n    \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n    \"name\": \"spark-token-kzmdd\",\n    \"readOnly\": true\n  }\n]\n</code></pre>","title":"Volume Mounts"},{"location":"demo/spark-and-local-filesystem-in-minikube/#inside-pod","text":"<pre><code>k exec $POD_NAME -- ls -ltr /tmp/\n</code></pre> <pre><code>$ cat /tmp/spark-k8s/spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08/test.me\nHello World\n</code></pre>","title":"Inside Pod"},{"location":"demo/spark-shell-on-minikube/","text":"<p>This demo shows how to run <code>spark-shell</code> on minikube that touts itself as:</p>  <p>minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows.</p>   <p>Note</p> <p><code>k</code> is an alias of <code>kubectl</code>.</p>","title":"Demo: spark-shell on minikube"},{"location":"demo/spark-shell-on-minikube/#start-minikube","text":"<p>Let's start <code>minikube</code> with the recommended resources (based on the Prerequisites in the official documentation of Apache Spark).</p> <pre><code>minikube start --cpus 4 --memory 8192\n</code></pre>","title":"Start minikube"},{"location":"demo/spark-shell-on-minikube/#review-cluster-info","text":"","title":"Review Cluster Info"},{"location":"demo/spark-shell-on-minikube/#cluster-info","text":"<pre><code>k cluster-info\n</code></pre> <pre><code>k config view\n</code></pre>","title":"Cluster Info"},{"location":"demo/spark-shell-on-minikube/#pods","text":"<p>List available pods. There should be none except Kubernetes system pods (and that's the reason for <code>-A</code> to include all pods, including system's).</p> <pre><code>k get po -A\n</code></pre> <pre><code>NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE\nkube-system   coredns-74ff55c5b-vqm2f            1/1     Running   0          2m3s\nkube-system   etcd-minikube                      1/1     Running   0          2m18s\nkube-system   kube-apiserver-minikube            1/1     Running   0          2m18s\nkube-system   kube-controller-manager-minikube   1/1     Running   0          2m18s\nkube-system   kube-proxy-ms4wd                   1/1     Running   0          2m4s\nkube-system   kube-scheduler-minikube            1/1     Running   0          2m18s\nkube-system   storage-provisioner                1/1     Running   0          2m18s\n</code></pre>","title":"Pods"},{"location":"demo/spark-shell-on-minikube/#container-images","text":"<p>List available Docker images in minikube's Docker registry.</p> <p>Point the shell to minikube's Docker daemon.</p> <pre><code>eval $(minikube -p minikube docker-env)\n</code></pre> <p>List available container images.</p> <pre><code>docker images | sort\n</code></pre> <pre><code>REPOSITORY                                TAG        IMAGE ID       CREATED         SIZE\ngcr.io/k8s-minikube/storage-provisioner   v4         85069258b98a   3 months ago    29.7MB\nk8s.gcr.io/coredns                        1.7.0      bfe3a36ebd25   8 months ago    45.2MB\nk8s.gcr.io/etcd                           3.4.13-0   0369cf4303ff   6 months ago    253MB\nk8s.gcr.io/kube-apiserver                 v1.20.2    a8c2fdb8bf76   7 weeks ago     122MB\nk8s.gcr.io/kube-controller-manager        v1.20.2    a27166429d98   7 weeks ago     116MB\nk8s.gcr.io/kube-proxy                     v1.20.2    43154ddb57a8   7 weeks ago     118MB\nk8s.gcr.io/kube-scheduler                 v1.20.2    ed2c44fbdd78   7 weeks ago     46.4MB\nk8s.gcr.io/pause                          3.2        80d28bedfe5d   12 months ago   683kB\nkubernetesui/dashboard                    v2.1.0     9a07b5b4bfac   2 months ago    226MB\nkubernetesui/metrics-scraper              v1.0.4     86262685d9ab   11 months ago   36.9MB\n</code></pre>","title":"Container Images"},{"location":"demo/spark-shell-on-minikube/#kubernetes-dashboard","text":"<pre><code>minikube dashboard\n</code></pre>","title":"Kubernetes Dashboard"},{"location":"demo/spark-shell-on-minikube/#build-spark-image","text":"<p>Quoting Submitting Applications to Kubernetes in the official documentation of Apache Spark:</p>  <p>Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose, or customized to match an individual application\u2019s needs. It can be found in the <code>kubernetes/dockerfiles/</code> directory.</p>  <p>In a separate terminal...</p> <pre><code>cd $SPARK_HOME\n</code></pre>  <p>Tip</p> <p>Review <code>kubernetes/dockerfiles/spark</code> (in your Spark installation) or <code>resource-managers/kubernetes/docker</code> (in the Spark source code).</p>","title":"Build Spark Image"},{"location":"demo/spark-shell-on-minikube/#docker-image-tool","text":"<p>Build and publish the Spark image. Note <code>-m</code> option to point the shell script to use minikube's Docker daemon.</p> <pre><code>./bin/docker-image-tool.sh \\\n  -m \\\n  -b java_image_tag=11-jre-slim \\\n  -t v3.2.0 \\\n  build\n</code></pre> 11-jre-slim is the default<p>As of Spark 3.1.1, <code>java_image_tag</code> argument is assumed <code>11-jre-slim</code>. You could safely skip <code>-b java_image_tag=11-jre-slim</code> in the above command.</p>","title":"docker-image-tool"},{"location":"demo/spark-shell-on-minikube/#docker-images","text":"<p>Point the shell to minikube's Docker daemon.</p> <pre><code>eval $(minikube -p minikube docker-env)\n</code></pre> <p>List the Spark image.</p> <pre><code>docker images spark\n</code></pre> <pre><code>REPOSITORY   TAG       IMAGE ID       CREATED         SIZE\nspark        v3.2.0    b3412e410d67   3 minutes ago   524MB\n</code></pre>","title":"docker images"},{"location":"demo/spark-shell-on-minikube/#docker-image-inspect","text":"<p>Use docker image inspect command to display detailed information on the Spark image.</p> <pre><code>docker image inspect spark:v3.2.0\n</code></pre>","title":"docker image inspect"},{"location":"demo/spark-shell-on-minikube/#create-namespace","text":"<p>This step is optional, but gives a better exposure to the Kubernetes features supported by Apache Spark and is highly recommended.</p>  <p>Tip</p> <p>Learn more about Creating a new namespace in the official documentation of Kubernetes.</p>  <pre><code>k create ns spark-demo\n</code></pre> <p>Set <code>spark-demo</code> as the default namespace using kubens tool.</p> <pre><code>kubens spark-demo\n</code></pre>","title":"Create Namespace"},{"location":"demo/spark-shell-on-minikube/#spark-logging","text":"<p>Enable <code>ALL</code> logging level for Kubernetes-related loggers to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.k8s=ALL\nlog4j.logger.org.apache.spark.scheduler.cluster.k8s=ALL\nlog4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=INFO\n</code></pre> <p>Refer to Logging.</p>","title":"Spark Logging"},{"location":"demo/spark-shell-on-minikube/#launch-spark-shell","text":"<pre><code>cd $SPARK_HOME\n</code></pre> <pre><code>K8S_SERVER=$(k config view --output=jsonpath='{.clusters[].cluster.server}')\n\n./bin/spark-shell \\\n  --master k8s://$K8S_SERVER \\\n  --conf spark.kubernetes.container.image=spark:v3.2.0 \\\n  --conf spark.kubernetes.context=minikube \\\n  --conf spark.kubernetes.namespace=spark-demo \\\n  --verbose\n</code></pre> <p>Soon you should see the Spark prompt similar to the following:</p> <pre><code>Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n\nUsing Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.10)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; spark.version\nres0: String = 3.1.1\n\nscala&gt; sc.master\nres1: String = k8s://https://127.0.0.1:55020\n</code></pre>","title":"Launch spark-shell"},{"location":"demo/spark-shell-on-minikube/#web-uis","text":"<p>Open web UI of the Spark application at http://localhost:4040/.</p> <p>Review the pods in the Kubernetes UI. Make sure to use <code>spark-demo</code> namespace.</p> <p></p>","title":"web UIs"},{"location":"demo/spark-shell-on-minikube/#scale-executors","text":"<p>Just for some more fun, in <code>spark-shell</code>, request two more executors and observe the logs.</p> <pre><code>sc.requestTotalExecutors(numExecutors = 4, localityAwareTasks = 0, hostToLocalTaskCount = Map.empty)\n</code></pre> <pre><code>sc.killExecutors(Seq(\"1\", \"3\"))\n</code></pre> <p>Review the number of executors at http://localhost:4040/executors/ and in the Kubernetes UI.</p>","title":"Scale Executors"},{"location":"demo/spark-shell-on-minikube/#clean-up","text":"<pre><code>minikube stop\n</code></pre> <p>Close the terminal.</p>","title":"Clean Up"},{"location":"demo/spark-shell-on-minikube/#full-clean-up","text":"<p>Optionally you may want to clean up all the application resources (e.g. to start from scratch next time).</p> <p>Delete all of the minikube clusters.</p> <pre><code>minikube delete --all --purge\n</code></pre> <p>Remove minikube's Docker images.</p> <pre><code>docker rmi $(docker image ls 'gcr.io/k8s-minikube/*' -q)\n</code></pre> <pre><code>docker image prune --force\n</code></pre> <p>That's it. Congratulations!</p>","title":"Full Clean Up"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/","text":"<p>This demo is a follow-up to Demo: Running Spark Structured Streaming on minikube and is going to show the steps to use a persistent disk Google Cloud Storage for a checkpoint location in a Spark Structured Streaming application on Google Kubernetes Engine.</p> <p>The demo uses the Cloud Storage connector that lets Spark applications access data in Cloud Storage using the <code>gs://</code> prefix.</p> <pre><code>.option(\"checkpointLocation\", \"gs://spark-checkpoint-location/\")\n</code></pre> <p>The most challenging task in the demo has been to include necessary dependencies in a Docker image to support the <code>gs://</code> prefix.</p>","title":"Demo: Using Cloud Storage for Checkpoint Location in Spark Structured Streaming on Google Kubernetes Engine"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#before-you-begin","text":"<p>It is assumed that you have finished the following:</p> <ol> <li>Demo: Running Spark Structured Streaming on minikube</li> <li>Demo: Running Spark Examples on Google Kubernetes Engine</li> </ol>","title":"Before you begin"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#environment-variables","text":"<p>You will need the following environment variables to run the demo. They are all in one section to find them easier when needed (e.g. switching terminals).</p> <pre><code>export PROJECT_ID=$(gcloud info --format='value(config.project)')\nexport GCP_CR=eu.gcr.io/${PROJECT_ID}\n\nexport CLUSTER_NAME=spark-examples-cluster\n\n# Has to end with /\nexport BUCKET_NAME=gs://spark-on-kubernetes-2021/\n\nexport K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}')\nexport POD_NAME=spark-streams-google-storage-demo\nexport SPARK_IMAGE=$GCP_CR/spark-streams-google-storage-demo:0.1.0\n\nexport K8S_NAMESPACE=spark-demo\nexport SUBMISSION_ID=$K8S_NAMESPACE:$POD_NAME\n\nexport KEY_JSON=spark-on-kubernetes-2021.json\nexport MOUNT_PATH=/opt/spark/secrets\n</code></pre>","title":"Environment Variables"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#build-spark-application-image","text":"<p>List images and make sure that the Spark image is available. If not, follow the steps in Demo: Running Spark Examples on Google Kubernetes Engine.</p> <pre><code>gcloud container images list --repository $GCP_CR\n</code></pre> <pre><code>gcloud container images list-tags $GCP_CR/spark\n</code></pre> <p>Go to your Spark application project and build the image.</p> <pre><code>sbt spark-streams-demo/docker:publishLocal spark-streams-google-storage-demo/docker:publishLocal\n</code></pre> <pre><code>docker images $GCP_CR/spark-streams-google-storage-demo\n</code></pre> <pre><code>REPOSITORY                                                             TAG       IMAGE ID       CREATED         SIZE\neu.gcr.io/spark-on-kubernetes-2021/spark-streams-google-storage-demo   0.1.0     b9dd310765ba   3 minutes ago   542MB\n</code></pre> docker tag<p>Use docker tag unless you've done it already at build time.</p> <pre><code>docker tag spark-streams-google-storage-demo:0.1.0 $GCP_CR/spark-streams-google-storage-demo:0.1.0\n</code></pre>","title":"Build Spark Application Image"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#push-image-to-container-registry","text":"<p>Use docker image push to push the Spark application image to the Container Registry on Google Cloud Platform.</p> <pre><code>docker push $GCP_CR/spark-streams-google-storage-demo:0.1.0\n</code></pre>","title":"Push Image to Container Registry"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#display-images","text":"<p>List the available images.</p> <pre><code>gcloud container images list --repository $GCP_CR\n</code></pre> <pre><code>NAME\neu.gcr.io/spark-on-kubernetes-2021/spark-streams-google-storage-demo\n</code></pre>","title":"Display Images"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#create-kubernetes-cluster","text":"<p>Create a Kubernetes cluster as described in Demo: Running Spark Examples on Google Kubernetes Engine.</p> <pre><code>gcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version=latest\n</code></pre> <p>Wait a few minutes before the cluster is ready.</p> <p>In the end, you should see the messages as follows:</p> <pre><code>kubeconfig entry generated for spark-examples-cluster.\nNAME                    LOCATION        MASTER_VERSION    MASTER_IP      MACHINE_TYPE  NODE_VERSION      NUM_NODES  STATUS\nspark-examples-cluster  europe-west3-b  1.18.15-gke.1100  34.107.115.78  e2-medium     1.18.15-gke.1100  3          RUNNING\n</code></pre>","title":"Create Kubernetes Cluster"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#create-cloud-storage-bucket","text":"<p>Quoting Connecting to Cloud Storage buckets:</p>  <p>Cloud Storage is a flexible, scalable, and durable storage option for your virtual machine instances. You can read and write files to Cloud Storage buckets from almost anywhere, so you can use buckets as common storage between your instances, App Engine, your on-premises systems, and other cloud services.</p>   <p>Tip</p> <p>You may want to review Storage options for alternative instance storage options.</p>  <pre><code>gsutil mb -b on $BUCKET_NAME\n</code></pre>","title":"Create Cloud Storage Bucket"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#list-contents-of-bucket","text":"<pre><code>gsutil ls -l $BUCKET_NAME\n</code></pre> <p>There should be no output really since you've just created it.</p>","title":"List Contents of Bucket"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#run-spark-structured-streaming-on-gke","text":"","title":"Run Spark Structured Streaming on GKE"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#create-kubernetes-resources","text":"<p>Create Kubernetes resources as described in Demo: Running Spark Examples on Google Kubernetes Engine.</p> <pre><code>k create -f k8s/rbac.yml\n</code></pre> <pre><code>namespace/spark-demo created\nserviceaccount/spark created\nclusterrolebinding.rbac.authorization.k8s.io/spark-role created\n</code></pre>","title":"Create Kubernetes Resources"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#create-service-account-credentials","text":"<p>As the Spark application will need access to Google Cloud services, it requires a service account.</p>  <p>Tip</p> <p>Learn more in Authenticating to Google Cloud with service accounts tutorial. The most important section is Creating service account credentials.</p>  <p>You should have a JSON key file containing the credentials of the service account to authenticate the application with.</p>","title":"Create Service Account Credentials"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#import-credentials-as-kubernetes-secret","text":"<p>Tip</p> <p>Learn more in Authenticating to Google Cloud with service accounts tutorial. The most important section is Importing credentials as a Secret.</p>  <p>The recommended way of using the JSON key file with the service account in Kubernetes is using Secret resource type.</p> <pre><code>kubectl create secret generic spark-sa \\\n  --from-file=key.json=$KEY_JSON \\\n  -n $K8S_NAMESPACE\n</code></pre>","title":"Import Credentials as Kubernetes Secret"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#configure-spark-application-with-kubernetes-secret","text":"<p>Tip</p> <p>Learn more in Authenticating to Google Cloud with service accounts tutorial. The most important section is Configuring the application with the Secret.</p>  <p>In order to use the service account and access the bucket using <code>gs://</code> URI scheme you are going to use the following additional configuration properties:</p> <pre><code>--conf spark.kubernetes.driver.secrets.spark-sa=$MOUNT_PATH\n--conf spark.kubernetes.executor.secrets.spark-sa=$MOUNT_PATH\n--conf spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS=$MOUNT_PATH/key.json\n--conf spark.kubernetes.executorEnv.GOOGLE_APPLICATION_CREDENTIALS=$MOUNT_PATH/key.json\n--conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=$MOUNT_PATH/key.json\n</code></pre>","title":"Configure Spark Application with Kubernetes Secret"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#submit-spark-application","text":"<p>Submit the Spark Structured Streaming application to GKE as described in Demo: Running Spark Structured Streaming on minikube.</p> <p>You may optionally delete all pods (since we use a fixed name for the demo).</p> <pre><code>k delete po --all -n $K8S_NAMESPACE\n</code></pre> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --deploy-mode cluster \\\n  --name $POD_NAME \\\n  --class meetup.SparkStreamsApp \\\n  --conf spark.kubernetes.driver.request.cores=400m \\\n  --conf spark.kubernetes.executor.request.cores=100m \\\n  --conf spark.kubernetes.container.image=$SPARK_IMAGE \\\n  --conf spark.kubernetes.driver.pod.name=$POD_NAME \\\n  --conf spark.kubernetes.namespace=$K8S_NAMESPACE \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n  --conf spark.kubernetes.submission.waitAppCompletion=false \\\n  --conf spark.kubernetes.driver.secrets.spark-sa=$MOUNT_PATH \\\n  --conf spark.kubernetes.executor.secrets.spark-sa=$MOUNT_PATH \\\n  --conf spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS=$MOUNT_PATH/key.json \\\n  --conf spark.kubernetes.executorEnv.GOOGLE_APPLICATION_CREDENTIALS=$MOUNT_PATH/key.json \\\n  --conf spark.hadoop.google.cloud.auth.service.account.enable=true \\\n  --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=$MOUNT_PATH/key.json \\\n  --conf spark.hadoop.fs.gs.project.id=$PROJECT_ID \\\n  --verbose \\\n  local:///opt/spark/jars/meetup.spark-streams-demo-0.1.0.jar $BUCKET_NAME\n</code></pre>  <p>Installing Google Cloud Storage connector for Hadoop</p> <p>Learn more in Installing the connector.</p>","title":"Submit Spark Application"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#monitoring","text":"<p>Watch the logs of the driver and executor pods.</p> <pre><code>k logs -f $POD_NAME -n $K8S_NAMESPACE\n</code></pre> <p>Observe pods in another terminal.</p> <pre><code>k get po -w -n $K8S_NAMESPACE\n</code></pre>","title":"Monitoring"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#google-cloud-console","text":"<p>Review the Spark application in the Google Cloud Console of the project:</p> <ol> <li>Workloads</li> <li>Services &amp; Ingress</li> <li>Configuration (make sure to use <code>spark-demo</code> namespace)</li> </ol>","title":"Google Cloud Console"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#services-ingress","text":"<p>While in Services &amp; Ingress, click the service to enable Spark UI.</p> <ol> <li> <p>Go to Service details and scroll down to the Ports section.</p> </li> <li> <p>Click PORT FORWARDING button next to spark-ui entry.</p> </li> </ol> <p></p>","title":"Services &amp; Ingress"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#kill-spark-application","text":"<p>In the end, you can <code>spark-submit --kill</code> the Spark Structured Streaming application.</p> <pre><code>./bin/spark-submit \\\n  --master k8s://$K8S_SERVER \\\n  --kill $SUBMISSION_ID\n</code></pre>","title":"Kill Spark Application"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#clean-up","text":"<p>Delete the bucket.</p> <pre><code>gsutil rm -r $BUCKET_NAME\n</code></pre> <p>Delete cluster resources as described in Demo: Running Spark Examples on Google Kubernetes Engine.</p> <p>That's it. Congratulations!</p>","title":"Clean Up"},{"location":"demo/using-cloud-storage-for-checkpoint-location-in-spark-structured-streaming-on-google-kubernetes-engine/#references","text":"<ul> <li>How to use Google Cloud Storage for checkpoint location in streaming query?</li> <li>Quickstart: Using the gsutil tool</li> </ul>","title":"References"}]}