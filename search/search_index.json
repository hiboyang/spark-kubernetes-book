{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Spark on Kubernetes (Apache Spark 3.0.1) \u00b6 Welcome to The Internals of Spark on Kubernetes online book! I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB ). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark on Kubernetes .","title":"Welcome"},{"location":"#the-internals-of-spark-on-kubernetes-apache-spark-301","text":"Welcome to The Internals of Spark on Kubernetes online book! I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB ). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark on Kubernetes .","title":"The Internals of Spark on Kubernetes (Apache Spark 3.0.1)"},{"location":"BasicDriverFeatureStep/","text":"BasicDriverFeatureStep \u00b6 BasicDriverFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 BasicDriverFeatureStep takes the following to be created: KubernetesDriverConf BasicDriverFeatureStep is created when: KubernetesDriverBuilder is requested to buildFromFeatures Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional properties: Name Value spark.kubernetes.submitInDriver true spark.kubernetes.driver.pod.name driverPodName spark.kubernetes.memoryOverheadFactor overheadFactor spark.app.id appId (of the KubernetesDriverConf ) getAdditionalPodSystemProperties uploads local and resolvable files (specified using spark.jars and spark.files configuration properties) to a Hadoop-compatible file system and adds them to the additional properties (as comma-separated file URIs). Configuring Pod for Driver \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME Driver Container Image Name \u00b6 BasicDriverFeatureStep uses spark.kubernetes.driver.container.image for the name of the container image for drivers. The name must be defined or BasicDriverFeatureStep throws an SparkException : Must specify the driver container image driverContainerImage is used when requested for configurePod .","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#basicdriverfeaturestep","text":"BasicDriverFeatureStep is a KubernetesFeatureConfigStep .","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#creating-instance","text":"BasicDriverFeatureStep takes the following to be created: KubernetesDriverConf BasicDriverFeatureStep is created when: KubernetesDriverBuilder is requested to buildFromFeatures","title":"Creating Instance"},{"location":"BasicDriverFeatureStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional properties: Name Value spark.kubernetes.submitInDriver true spark.kubernetes.driver.pod.name driverPodName spark.kubernetes.memoryOverheadFactor overheadFactor spark.app.id appId (of the KubernetesDriverConf ) getAdditionalPodSystemProperties uploads local and resolvable files (specified using spark.jars and spark.files configuration properties) to a Hadoop-compatible file system and adds them to the additional properties (as comma-separated file URIs).","title":" Additional System Properties"},{"location":"BasicDriverFeatureStep/#configuring-pod-for-driver","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Pod for Driver"},{"location":"BasicDriverFeatureStep/#driver-container-image-name","text":"BasicDriverFeatureStep uses spark.kubernetes.driver.container.image for the name of the container image for drivers. The name must be defined or BasicDriverFeatureStep throws an SparkException : Must specify the driver container image driverContainerImage is used when requested for configurePod .","title":" Driver Container Image Name"},{"location":"BasicExecutorFeatureStep/","text":"BasicExecutorFeatureStep \u00b6 BasicExecutorFeatureStep is...FIXME spark.kubernetes.executor.container.image \u00b6 BasicExecutorFeatureStep asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a SparkException : Must specify the executor container image","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#basicexecutorfeaturestep","text":"BasicExecutorFeatureStep is...FIXME","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#sparkkubernetesexecutorcontainerimage","text":"BasicExecutorFeatureStep asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a SparkException : Must specify the executor container image","title":" spark.kubernetes.executor.container.image"},{"location":"Client/","text":"Client \u00b6 Client submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status). Creating Instance \u00b6 Client takes the following to be created: KubernetesDriverConf KubernetesDriverBuilder Kubernetes' KubernetesClient LoggingPodStatusWatcher Client is created when: KubernetesClientApplication is requested to start Running Driver Pod \u00b6 run () : Unit run requests the KubernetesDriverBuilder to build a KubernetesDriverSpec from features . run requests the KubernetesDriverConf for the resourceNamePrefix and uses it for the name of the driver's config map: [resourceNamePrefix]-driver-conf-map run builds a ConfigMap (with the name and the system properties of the KubernetesDriverSpec ). run creates a driver container (based on the KubernetesDriverSpec ) and adds the following: SPARK_CONF_DIR env var as /opt/spark/conf spark-conf-volume volume mount as /opt/spark/conf run creates a driver pod (based on the KubernetesDriverSpec ) with the driver container and a new spark-conf-volume volume for the ConfigMap . run requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher ) and, when available, attaches the ConfigMap . run is used when: KubernetesClientApplication is requested to start addDriverOwnerReference \u00b6 addDriverOwnerReference ( driverPod : Pod , resources : Seq [ HasMetadata ]) : Unit addDriverOwnerReference ...FIXME buildConfigMap \u00b6 buildConfigMap ( configMapName : String , conf : Map [ String , String ]) : ConfigMap buildConfigMap ...FIXME","title":"Client"},{"location":"Client/#client","text":"Client submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status).","title":"Client"},{"location":"Client/#creating-instance","text":"Client takes the following to be created: KubernetesDriverConf KubernetesDriverBuilder Kubernetes' KubernetesClient LoggingPodStatusWatcher Client is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"Client/#running-driver-pod","text":"run () : Unit run requests the KubernetesDriverBuilder to build a KubernetesDriverSpec from features . run requests the KubernetesDriverConf for the resourceNamePrefix and uses it for the name of the driver's config map: [resourceNamePrefix]-driver-conf-map run builds a ConfigMap (with the name and the system properties of the KubernetesDriverSpec ). run creates a driver container (based on the KubernetesDriverSpec ) and adds the following: SPARK_CONF_DIR env var as /opt/spark/conf spark-conf-volume volume mount as /opt/spark/conf run creates a driver pod (based on the KubernetesDriverSpec ) with the driver container and a new spark-conf-volume volume for the ConfigMap . run requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher ) and, when available, attaches the ConfigMap . run is used when: KubernetesClientApplication is requested to start","title":" Running Driver Pod"},{"location":"Client/#adddriverownerreference","text":"addDriverOwnerReference ( driverPod : Pod , resources : Seq [ HasMetadata ]) : Unit addDriverOwnerReference ...FIXME","title":" addDriverOwnerReference"},{"location":"Client/#buildconfigmap","text":"buildConfigMap ( configMapName : String , conf : Map [ String , String ]) : ConfigMap buildConfigMap ...FIXME","title":" buildConfigMap"},{"location":"ClientArguments/","text":"ClientArguments \u00b6 ClientArguments represents a KubernetesClientApplication to start. Creating Instance \u00b6 ClientArguments takes the following to be created: MainAppResource Name of the main class of a Spark application to run Driver Arguments ClientArguments is created (via fromCommandLineArgs utility) when: KubernetesClientApplication is requested to start fromCommandLineArgs Utility \u00b6 fromCommandLineArgs ( args : Array [ String ]) : ClientArguments fromCommandLineArgs slices the input args into key-value pairs and creates a ClientArguments as follows: --primary-java-resource , --primary-py-file or --primary-r-file keys are used for the MainAppResource --main-class for the name of the main class --arg for the driver arguments Important The main class must be specified via --main-class or fromCommandLineArgs throws an IllegalArgumentException . fromCommandLineArgs is used when: KubernetesClientApplication is requested to start","title":"ClientArguments"},{"location":"ClientArguments/#clientarguments","text":"ClientArguments represents a KubernetesClientApplication to start.","title":"ClientArguments"},{"location":"ClientArguments/#creating-instance","text":"ClientArguments takes the following to be created: MainAppResource Name of the main class of a Spark application to run Driver Arguments ClientArguments is created (via fromCommandLineArgs utility) when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"ClientArguments/#fromcommandlineargs-utility","text":"fromCommandLineArgs ( args : Array [ String ]) : ClientArguments fromCommandLineArgs slices the input args into key-value pairs and creates a ClientArguments as follows: --primary-java-resource , --primary-py-file or --primary-r-file keys are used for the MainAppResource --main-class for the name of the main class --arg for the driver arguments Important The main class must be specified via --main-class or fromCommandLineArgs throws an IllegalArgumentException . fromCommandLineArgs is used when: KubernetesClientApplication is requested to start","title":" fromCommandLineArgs Utility"},{"location":"DriverCommandFeatureStep/","text":"DriverCommandFeatureStep \u00b6 DriverCommandFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 DriverCommandFeatureStep takes the following to be created: KubernetesDriverConf DriverCommandFeatureStep is created when: KubernetesDriverBuilder is requested to build a KubernetesDriverSpec from features configurePod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod branches off based on the MainAppResource (of the KubernetesDriverConf ): For JavaMainAppResource , configurePod configureForJava with the primary resource (if defined) or uses spark-internal For PythonMainAppResource , configurePod configureForPython with the primary resource For RMainAppResource , configurePod configureForR with the primary resource configureForJava \u00b6 configureForJava ( pod : SparkPod , res : String ) : SparkPod configureForJava builds the base driver container for the given SparkPod and the primary resource. In the end, configureForJava creates another SparkPod (for the pod of the given SparkPod ) and the driver container. configureForJava is used when: DriverCommandFeatureStep is requested to configurePod for a JavaMainAppResource (based on the mainAppResource of the KubernetesDriverConf ) configureForPython \u00b6 configureForPython ( pod : SparkPod , res : String ) : SparkPod configureForPython ...FIXME configureForPython is used when: FIXME configureForR \u00b6 configureForR ( pod : SparkPod , res : String ) : SparkPod configureForR ...FIXME configureForR is used when: FIXME baseDriverContainer \u00b6 baseDriverContainer ( pod : SparkPod , resource : String ) : ContainerBuilder baseDriverContainer renames the given primary resource when the MainAppResource is a JavaMainAppResource . Otherwise, baseDriverContainer leaves the primary resource as-is. baseDriverContainer creates a ContainerBuilder (for the pod of the given SparkPod ) and adds the following arguments (in that order): driver --properties-file with /opt/spark/conf/spark.properties --class with the mainClass of the KubernetesDriverConf the primary resource (possibly renamed when a MainAppResource ) appArgs of the KubernetesDriverConf Note The arguments are then used by the default entrypoint.sh of the official Docker image of Apache Spark (in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/ ). baseDriverContainer is used when: DriverCommandFeatureStep is requested to configureForJava , configureForPython , and configureForR","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#drivercommandfeaturestep","text":"DriverCommandFeatureStep is a KubernetesFeatureConfigStep .","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#creating-instance","text":"DriverCommandFeatureStep takes the following to be created: KubernetesDriverConf DriverCommandFeatureStep is created when: KubernetesDriverBuilder is requested to build a KubernetesDriverSpec from features","title":"Creating Instance"},{"location":"DriverCommandFeatureStep/#configurepod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod branches off based on the MainAppResource (of the KubernetesDriverConf ): For JavaMainAppResource , configurePod configureForJava with the primary resource (if defined) or uses spark-internal For PythonMainAppResource , configurePod configureForPython with the primary resource For RMainAppResource , configurePod configureForR with the primary resource","title":" configurePod"},{"location":"DriverCommandFeatureStep/#configureforjava","text":"configureForJava ( pod : SparkPod , res : String ) : SparkPod configureForJava builds the base driver container for the given SparkPod and the primary resource. In the end, configureForJava creates another SparkPod (for the pod of the given SparkPod ) and the driver container. configureForJava is used when: DriverCommandFeatureStep is requested to configurePod for a JavaMainAppResource (based on the mainAppResource of the KubernetesDriverConf )","title":" configureForJava"},{"location":"DriverCommandFeatureStep/#configureforpython","text":"configureForPython ( pod : SparkPod , res : String ) : SparkPod configureForPython ...FIXME configureForPython is used when: FIXME","title":" configureForPython"},{"location":"DriverCommandFeatureStep/#configureforr","text":"configureForR ( pod : SparkPod , res : String ) : SparkPod configureForR ...FIXME configureForR is used when: FIXME","title":" configureForR"},{"location":"DriverCommandFeatureStep/#basedrivercontainer","text":"baseDriverContainer ( pod : SparkPod , resource : String ) : ContainerBuilder baseDriverContainer renames the given primary resource when the MainAppResource is a JavaMainAppResource . Otherwise, baseDriverContainer leaves the primary resource as-is. baseDriverContainer creates a ContainerBuilder (for the pod of the given SparkPod ) and adds the following arguments (in that order): driver --properties-file with /opt/spark/conf/spark.properties --class with the mainClass of the KubernetesDriverConf the primary resource (possibly renamed when a MainAppResource ) appArgs of the KubernetesDriverConf Note The arguments are then used by the default entrypoint.sh of the official Docker image of Apache Spark (in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/ ). baseDriverContainer is used when: DriverCommandFeatureStep is requested to configureForJava , configureForPython , and configureForR","title":" baseDriverContainer"},{"location":"DriverServiceFeatureStep/","text":"DriverServiceFeatureStep \u00b6 DriverServiceFeatureStep is...FIXME","title":"DriverServiceFeatureStep"},{"location":"DriverServiceFeatureStep/#driverservicefeaturestep","text":"DriverServiceFeatureStep is...FIXME","title":"DriverServiceFeatureStep"},{"location":"EnvSecretsFeatureStep/","text":"EnvSecretsFeatureStep \u00b6 EnvSecretsFeatureStep is...FIXME","title":"EnvSecretsFeatureStep"},{"location":"EnvSecretsFeatureStep/#envsecretsfeaturestep","text":"EnvSecretsFeatureStep is...FIXME","title":"EnvSecretsFeatureStep"},{"location":"ExecutorPodsAllocator/","text":"ExecutorPodsAllocator \u00b6 ExecutorPodsAllocator is used to create a KubernetesClusterSchedulerBackend . Creating Instance \u00b6 ExecutorPodsAllocator takes the following to be created: SparkConf SecurityManager KubernetesExecutorBuilder KubernetesClient ExecutorPodsSnapshotsStore Clock ExecutorPodsAllocator is created when: KubernetesClusterManager is requested for a SchedulerBackend spark.kubernetes.allocation.batch.delay \u00b6 ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.delay configuration property for the following: podCreationTimeout Registering a subscriber Starting \u00b6 start ( applicationId : String ) : Unit start requests the ExecutorPodsSnapshotsStore to add a new subscriber (with podAllocationDelay ) to intercept new snapshots . start is used when: KubernetesClusterSchedulerBackend is requested to start onNewSnapshots \u00b6 onNewSnapshots ( applicationId : String , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots ...FIXME setTotalExpectedExecutors \u00b6 setTotalExpectedExecutors ( total : Int ) : Unit setTotalExpectedExecutors sets totalExpectedExecutors internal registry to the input total . With no hasPendingPods , setTotalExpectedExecutors requests the ExecutorPodsSnapshotsStore to notifySubscribers . setTotalExpectedExecutors is used when: KubernetesClusterSchedulerBackend is requested to start and doRequestTotalExecutors Registries \u00b6 Total Expected Executors \u00b6 totalExpectedExecutors : AtomicInteger ExecutorPodsAllocator uses a Java AtomicInteger to track the total expected number of executors. Starts from 0 and is set to a fixed number of the total expected executors in setTotalExpectedExecutors Used in onNewSnapshots hasPendingPods Flag \u00b6 hasPendingPods : AtomicBoolean ExecutorPodsAllocator uses a Java AtomicBoolean as a flag to avoid notifying subscribers. Starts as false and is updated every onNewSnapshots Used in setTotalExpectedExecutors (only when false ) Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL Refer to Logging .","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#executorpodsallocator","text":"ExecutorPodsAllocator is used to create a KubernetesClusterSchedulerBackend .","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#creating-instance","text":"ExecutorPodsAllocator takes the following to be created: SparkConf SecurityManager KubernetesExecutorBuilder KubernetesClient ExecutorPodsSnapshotsStore Clock ExecutorPodsAllocator is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchdelay","text":"ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.delay configuration property for the following: podCreationTimeout Registering a subscriber","title":" spark.kubernetes.allocation.batch.delay"},{"location":"ExecutorPodsAllocator/#starting","text":"start ( applicationId : String ) : Unit start requests the ExecutorPodsSnapshotsStore to add a new subscriber (with podAllocationDelay ) to intercept new snapshots . start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsAllocator/#onnewsnapshots","text":"onNewSnapshots ( applicationId : String , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots ...FIXME","title":" onNewSnapshots"},{"location":"ExecutorPodsAllocator/#settotalexpectedexecutors","text":"setTotalExpectedExecutors ( total : Int ) : Unit setTotalExpectedExecutors sets totalExpectedExecutors internal registry to the input total . With no hasPendingPods , setTotalExpectedExecutors requests the ExecutorPodsSnapshotsStore to notifySubscribers . setTotalExpectedExecutors is used when: KubernetesClusterSchedulerBackend is requested to start and doRequestTotalExecutors","title":" setTotalExpectedExecutors"},{"location":"ExecutorPodsAllocator/#registries","text":"","title":"Registries"},{"location":"ExecutorPodsAllocator/#total-expected-executors","text":"totalExpectedExecutors : AtomicInteger ExecutorPodsAllocator uses a Java AtomicInteger to track the total expected number of executors. Starts from 0 and is set to a fixed number of the total expected executors in setTotalExpectedExecutors Used in onNewSnapshots","title":" Total Expected Executors"},{"location":"ExecutorPodsAllocator/#haspendingpods-flag","text":"hasPendingPods : AtomicBoolean ExecutorPodsAllocator uses a Java AtomicBoolean as a flag to avoid notifying subscribers. Starts as false and is updated every onNewSnapshots Used in setTotalExpectedExecutors (only when false )","title":" hasPendingPods Flag"},{"location":"ExecutorPodsAllocator/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsLifecycleManager/","text":"ExecutorPodsLifecycleManager \u00b6 Creating Instance \u00b6 ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ). Configuration Properties \u00b6 spark.kubernetes.executor.eventProcessingInterval \u00b6 ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME spark.kubernetes.executor.deleteOnTermination \u00b6 ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState . Starting \u00b6 start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when KubernetesClusterSchedulerBackend is started . Handling State Changes in Executor Pods \u00b6 onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots creates an empty execIdsRemovedInThisRound collection of executors to be removed. onNewSnapshots walks over the input ExecutorPodsSnapshot s and branches off based on ExecutorPodState : For PodDeleted , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots removeExecutorFromSpark and adds the executor ID to the execIdsRemovedInThisRound local collection. For PodFailed , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. For PodSucceeded , onNewSnapshots requests the input KubernetesClusterSchedulerBackend to isExecutorActive . If so, onNewSnapshots prints out the following INFO message to the logs: Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed. Otherwise, onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName]. onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. onFinalNonDeletedState \u00b6 onFinalNonDeletedState ( podState : FinalPodState , execId : Long , schedulerBackend : KubernetesClusterSchedulerBackend , execIdsRemovedInRound : mutable.Set [ Long ]) : Unit onFinalNonDeletedState removeExecutorFromSpark . With spark.kubernetes.executor.deleteOnTermination configuration property enabled, onFinalNonDeletedState removeExecutorFromK8s . In the end, onFinalNonDeletedState adds the given execId to the given execIdsRemovedInRound collection. removeExecutorFromSpark \u00b6 removeExecutorFromSpark ( schedulerBackend : KubernetesClusterSchedulerBackend , podState : FinalPodState , execId : Long ) : Unit removeExecutorFromSpark ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL Refer to Logging .","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#executorpodslifecyclemanager","text":"","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#creating-instance","text":"ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ).","title":"Creating Instance"},{"location":"ExecutorPodsLifecycleManager/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutoreventprocessinginterval","text":"ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutordeleteontermination","text":"ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState .","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"ExecutorPodsLifecycleManager/#starting","text":"start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when KubernetesClusterSchedulerBackend is started .","title":" Starting"},{"location":"ExecutorPodsLifecycleManager/#handling-state-changes-in-executor-pods","text":"onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots creates an empty execIdsRemovedInThisRound collection of executors to be removed. onNewSnapshots walks over the input ExecutorPodsSnapshot s and branches off based on ExecutorPodState : For PodDeleted , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots removeExecutorFromSpark and adds the executor ID to the execIdsRemovedInThisRound local collection. For PodFailed , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. For PodSucceeded , onNewSnapshots requests the input KubernetesClusterSchedulerBackend to isExecutorActive . If so, onNewSnapshots prints out the following INFO message to the logs: Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed. Otherwise, onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName]. onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection.","title":" Handling State Changes in Executor Pods"},{"location":"ExecutorPodsLifecycleManager/#onfinalnondeletedstate","text":"onFinalNonDeletedState ( podState : FinalPodState , execId : Long , schedulerBackend : KubernetesClusterSchedulerBackend , execIdsRemovedInRound : mutable.Set [ Long ]) : Unit onFinalNonDeletedState removeExecutorFromSpark . With spark.kubernetes.executor.deleteOnTermination configuration property enabled, onFinalNonDeletedState removeExecutorFromK8s . In the end, onFinalNonDeletedState adds the given execId to the given execIdsRemovedInRound collection.","title":" onFinalNonDeletedState"},{"location":"ExecutorPodsLifecycleManager/#removeexecutorfromspark","text":"removeExecutorFromSpark ( schedulerBackend : KubernetesClusterSchedulerBackend , podState : FinalPodState , execId : Long ) : Unit removeExecutorFromSpark ...FIXME","title":" removeExecutorFromSpark"},{"location":"ExecutorPodsLifecycleManager/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsPollingSnapshotSource/","text":"ExecutorPodsPollingSnapshotSource \u00b6 Creating Instance \u00b6 ExecutorPodsPollingSnapshotSource takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Java ScheduledExecutorService ExecutorPodsPollingSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ) spark.kubernetes.executor.apiPollingInterval \u00b6 ExecutorPodsPollingSnapshotSource uses spark.kubernetes.executor.apiPollingInterval configuration property when started to schedule a PollRunnable . pollingFuture \u00b6 pollingFuture : Future [ _ ] pollingFuture ...FIXME Starting \u00b6 start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting to check for executor pod state every [pollingInterval] ms. start throws an IllegalArgumentException when started twice (i.e. pollingFuture has already been initialized): Cannot start polling more than once. start is used when: KubernetesClusterSchedulerBackend is requested to start Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL Refer to Logging .","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#executorpodspollingsnapshotsource","text":"","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#creating-instance","text":"ExecutorPodsPollingSnapshotSource takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Java ScheduledExecutorService ExecutorPodsPollingSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend )","title":"Creating Instance"},{"location":"ExecutorPodsPollingSnapshotSource/#sparkkubernetesexecutorapipollinginterval","text":"ExecutorPodsPollingSnapshotSource uses spark.kubernetes.executor.apiPollingInterval configuration property when started to schedule a PollRunnable .","title":" spark.kubernetes.executor.apiPollingInterval"},{"location":"ExecutorPodsPollingSnapshotSource/#pollingfuture","text":"pollingFuture : Future [ _ ] pollingFuture ...FIXME","title":" pollingFuture"},{"location":"ExecutorPodsPollingSnapshotSource/#starting","text":"start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting to check for executor pod state every [pollingInterval] ms. start throws an IllegalArgumentException when started twice (i.e. pollingFuture has already been initialized): Cannot start polling more than once. start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsPollingSnapshotSource/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsSnapshotsStore/","text":"ExecutorPodsSnapshotsStore \u00b6 ExecutorPodsSnapshotsStore is an abstraction of pod stores . Contract \u00b6 addSubscriber \u00b6 addSubscriber ( processBatchIntervalMillis : Long )( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit Used when: ExecutorPodsAllocator is requested to start ExecutorPodsLifecycleManager is requested to start notifySubscribers \u00b6 notifySubscribers () : Unit Used when: ExecutorPodsAllocator is requested to setTotalExpectedExecutors replaceSnapshot \u00b6 replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit Used when: PollRunnable is requested to start stop \u00b6 stop () : Unit Used when: KubernetesClusterSchedulerBackend is requested to stop updatePod \u00b6 updatePod ( updatedPod : Pod ) : Unit Used when: ExecutorPodsWatcher is requested to eventReceived Implementations \u00b6 ExecutorPodsSnapshotsStoreImpl","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#executorpodssnapshotsstore","text":"ExecutorPodsSnapshotsStore is an abstraction of pod stores .","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#contract","text":"","title":"Contract"},{"location":"ExecutorPodsSnapshotsStore/#addsubscriber","text":"addSubscriber ( processBatchIntervalMillis : Long )( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit Used when: ExecutorPodsAllocator is requested to start ExecutorPodsLifecycleManager is requested to start","title":" addSubscriber"},{"location":"ExecutorPodsSnapshotsStore/#notifysubscribers","text":"notifySubscribers () : Unit Used when: ExecutorPodsAllocator is requested to setTotalExpectedExecutors","title":" notifySubscribers"},{"location":"ExecutorPodsSnapshotsStore/#replacesnapshot","text":"replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit Used when: PollRunnable is requested to start","title":" replaceSnapshot"},{"location":"ExecutorPodsSnapshotsStore/#stop","text":"stop () : Unit Used when: KubernetesClusterSchedulerBackend is requested to stop","title":" stop"},{"location":"ExecutorPodsSnapshotsStore/#updatepod","text":"updatePod ( updatedPod : Pod ) : Unit Used when: ExecutorPodsWatcher is requested to eventReceived","title":" updatePod"},{"location":"ExecutorPodsSnapshotsStore/#implementations","text":"ExecutorPodsSnapshotsStoreImpl","title":"Implementations"},{"location":"ExecutorPodsSnapshotsStoreImpl/","text":"ExecutorPodsSnapshotsStoreImpl \u00b6 ExecutorPodsSnapshotsStoreImpl is an ExecutorPodsSnapshotsStore . Creating Instance \u00b6 ExecutorPodsSnapshotsStoreImpl takes the following to be created: Java's ScheduledExecutorService ExecutorPodsSnapshotsStoreImpl is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#executorpodssnapshotsstoreimpl","text":"ExecutorPodsSnapshotsStoreImpl is an ExecutorPodsSnapshotsStore .","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#creating-instance","text":"ExecutorPodsSnapshotsStoreImpl takes the following to be created: Java's ScheduledExecutorService ExecutorPodsSnapshotsStoreImpl is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsWatchSnapshotSource/","text":"ExecutorPodsWatchSnapshotSource \u00b6 ExecutorPodsWatchSnapshotSource is...FIXME Creating Instance \u00b6 ExecutorPodsWatchSnapshotSource takes the following to be created: ExecutorPodsSnapshotsStore KubernetesClient ExecutorPodsWatchSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend Starting \u00b6 start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor. start requests the KubernetesClient to watch pods with the following labels using ExecutorPodsWatcher : spark-app-selector with the given applicationId spark-role as executor start is used when: KubernetesClusterSchedulerBackend is requested to start Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL Refer to Logging .","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#executorpodswatchsnapshotsource","text":"ExecutorPodsWatchSnapshotSource is...FIXME","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#creating-instance","text":"ExecutorPodsWatchSnapshotSource takes the following to be created: ExecutorPodsSnapshotsStore KubernetesClient ExecutorPodsWatchSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsWatchSnapshotSource/#starting","text":"start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor. start requests the KubernetesClient to watch pods with the following labels using ExecutorPodsWatcher : spark-app-selector with the given applicationId spark-role as executor start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsWatchSnapshotSource/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsWatcher/","text":"ExecutorPodsWatcher \u00b6 ExecutorPodsWatcher is...FIXME","title":"ExecutorPodsWatcher"},{"location":"ExecutorPodsWatcher/#executorpodswatcher","text":"ExecutorPodsWatcher is...FIXME","title":"ExecutorPodsWatcher"},{"location":"K8SSparkSubmitOperation/","text":"K8SSparkSubmitOperation \u00b6 K8SSparkSubmitOperation is a SparkSubmitOperation . Killing Submission \u00b6 kill ( submissionId : String , conf : SparkConf ) : Unit kill is part of the SparkSubmitOperation abstraction. kill prints out the following message to standard error: Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set]. kill creates a KillApplication to execute it (with the input submissionId and SparkConf ). Displaying Submission Status \u00b6 printSubmissionStatus ( submissionId : String , conf : SparkConf ) : Unit printSubmissionStatus is part of the SparkSubmitOperation abstraction. printSubmissionStatus prints out the following message to standard error: Submitting a request for the status of submission [submissionId] in [spark.master]. printSubmissionStatus creates a ListStatus to execute it (with the input submissionId and SparkConf ). Checking Whether Master URL Supported \u00b6 supports ( master : String ) : Boolean supports is part of the SparkSubmitOperation abstraction. supports is true when the input master starts with k8s:// prefix. Executing Operation \u00b6 execute ( submissionId : String , sparkConf : SparkConf , op : K8sSubmitOp ) : Unit execute ...FIXME execute is used for kill and printSubmissionStatus .","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#k8ssparksubmitoperation","text":"K8SSparkSubmitOperation is a SparkSubmitOperation .","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#killing-submission","text":"kill ( submissionId : String , conf : SparkConf ) : Unit kill is part of the SparkSubmitOperation abstraction. kill prints out the following message to standard error: Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set]. kill creates a KillApplication to execute it (with the input submissionId and SparkConf ).","title":" Killing Submission"},{"location":"K8SSparkSubmitOperation/#displaying-submission-status","text":"printSubmissionStatus ( submissionId : String , conf : SparkConf ) : Unit printSubmissionStatus is part of the SparkSubmitOperation abstraction. printSubmissionStatus prints out the following message to standard error: Submitting a request for the status of submission [submissionId] in [spark.master]. printSubmissionStatus creates a ListStatus to execute it (with the input submissionId and SparkConf ).","title":" Displaying Submission Status"},{"location":"K8SSparkSubmitOperation/#checking-whether-master-url-supported","text":"supports ( master : String ) : Boolean supports is part of the SparkSubmitOperation abstraction. supports is true when the input master starts with k8s:// prefix.","title":" Checking Whether Master URL Supported"},{"location":"K8SSparkSubmitOperation/#executing-operation","text":"execute ( submissionId : String , sparkConf : SparkConf , op : K8sSubmitOp ) : Unit execute ...FIXME execute is used for kill and printSubmissionStatus .","title":" Executing Operation"},{"location":"KubernetesClientApplication/","text":"KubernetesClientApplication \u00b6 KubernetesClientApplication is a SparkApplication in Spark on Kubernetes in cluster deploy mode. Creating Instance \u00b6 KubernetesClientApplication takes no arguments to be created. KubernetesClientApplication is created when: SparkSubmit is requested to launch a Spark application (for kubernetes in cluster deploy mode ) Starting Spark Application \u00b6 start ( args : Array [ String ], conf : SparkConf ) : Unit start is part of the SparkApplication abstraction. start parses the command-line arguments ( args ) and runs . run \u00b6 run ( clientArguments : ClientArguments , sparkConf : SparkConf ) : Unit run generates a custom Spark Application ID of the format: spark-[randomUUID-without-dashes] run creates a KubernetesDriverConf (with the given ClientArguments , SparkConf and the custom Spark Application ID). run removes the k8s:// prefix from the spark.master (which has already been validated by SparkSubmit itself). run creates a LoggingPodStatusWatcherImpl (with the KubernetesDriverConf ). run creates a KubernetesClient (with the master URL, the namespace , and others). In the end, run creates a Client (with the KubernetesDriverConf , a new KubernetesDriverBuilder , the KubernetesClient , and the LoggingPodStatusWatcherImpl ) and requests it to run .","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#kubernetesclientapplication","text":"KubernetesClientApplication is a SparkApplication in Spark on Kubernetes in cluster deploy mode.","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#creating-instance","text":"KubernetesClientApplication takes no arguments to be created. KubernetesClientApplication is created when: SparkSubmit is requested to launch a Spark application (for kubernetes in cluster deploy mode )","title":"Creating Instance"},{"location":"KubernetesClientApplication/#starting-spark-application","text":"start ( args : Array [ String ], conf : SparkConf ) : Unit start is part of the SparkApplication abstraction. start parses the command-line arguments ( args ) and runs .","title":" Starting Spark Application"},{"location":"KubernetesClientApplication/#run","text":"run ( clientArguments : ClientArguments , sparkConf : SparkConf ) : Unit run generates a custom Spark Application ID of the format: spark-[randomUUID-without-dashes] run creates a KubernetesDriverConf (with the given ClientArguments , SparkConf and the custom Spark Application ID). run removes the k8s:// prefix from the spark.master (which has already been validated by SparkSubmit itself). run creates a LoggingPodStatusWatcherImpl (with the KubernetesDriverConf ). run creates a KubernetesClient (with the master URL, the namespace , and others). In the end, run creates a Client (with the KubernetesDriverConf , a new KubernetesDriverBuilder , the KubernetesClient , and the LoggingPodStatusWatcherImpl ) and requests it to run .","title":" run"},{"location":"KubernetesClusterManager/","text":"KubernetesClusterManager \u00b6 KubernetesClusterManager is an ExternalClusterManager that supports k8s master URLs. Creating Instance \u00b6 KubernetesClusterManager takes no arguments to be created. KubernetesClusterManager is created when: SparkContext is requested for an ExternalClusterManager (for a certain master URL) Creating TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler creates a TaskSchedulerImpl . createTaskScheduler is part of the ExternalClusterManager abstraction. Creating SchedulerBackend \u00b6 createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend creates a KubernetesClusterSchedulerBackend . Note createSchedulerBackend assumes that the given TaskScheduler is TaskSchedulerImpl . createSchedulerBackend uses spark.kubernetes.submitInDriver configuration property to determine whether executing in cluster deploy mode. createSchedulerBackend ...FIXME createSchedulerBackend is part of the ExternalClusterManager abstraction. Initializing Scheduling Components \u00b6 initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize requests the given TaskSchedulerImpl to initialize with the given SchedulerBackend . initialize is part of the ExternalClusterManager abstraction.","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#kubernetesclustermanager","text":"KubernetesClusterManager is an ExternalClusterManager that supports k8s master URLs.","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#creating-instance","text":"KubernetesClusterManager takes no arguments to be created. KubernetesClusterManager is created when: SparkContext is requested for an ExternalClusterManager (for a certain master URL)","title":"Creating Instance"},{"location":"KubernetesClusterManager/#creating-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler creates a TaskSchedulerImpl . createTaskScheduler is part of the ExternalClusterManager abstraction.","title":" Creating TaskScheduler"},{"location":"KubernetesClusterManager/#creating-schedulerbackend","text":"createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend creates a KubernetesClusterSchedulerBackend . Note createSchedulerBackend assumes that the given TaskScheduler is TaskSchedulerImpl . createSchedulerBackend uses spark.kubernetes.submitInDriver configuration property to determine whether executing in cluster deploy mode. createSchedulerBackend ...FIXME createSchedulerBackend is part of the ExternalClusterManager abstraction.","title":" Creating SchedulerBackend"},{"location":"KubernetesClusterManager/#initializing-scheduling-components","text":"initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize requests the given TaskSchedulerImpl to initialize with the given SchedulerBackend . initialize is part of the ExternalClusterManager abstraction.","title":" Initializing Scheduling Components"},{"location":"KubernetesClusterSchedulerBackend/","text":"KubernetesClusterSchedulerBackend \u00b6 KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend for Kubernetes . Creating Instance \u00b6 KubernetesClusterSchedulerBackend takes the following to be created: TaskSchedulerImpl SparkContext KubernetesClient Java's ScheduledExecutorService ExecutorPodsSnapshotsStore ExecutorPodsAllocator ExecutorPodsLifecycleManager ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource KubernetesClusterSchedulerBackend is created when: KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsLifecycleManager \u00b6 KubernetesClusterSchedulerBackend is given an ExecutorPodsLifecycleManager to be created . KubernetesClusterSchedulerBackend requests the ExecutorPodsLifecycleManager to start (with itself) when started . ExecutorPodsAllocator \u00b6 KubernetesClusterSchedulerBackend is given an ExecutorPodsAllocator to be created . When started , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the number of initial executors and starts it with application Id . When requested for the expected number of executors , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given total number of executors. When requested to isBlacklisted , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to isDeleted with a given executor. Application Id \u00b6 applicationId () : String applicationId is part of the SchedulerBackend abstraction. applicationId is the value of spark.app.id configuration property if defined or the default applicationId . Sufficient Resources Registered \u00b6 sufficientResourcesRegistered () : Boolean sufficientResourcesRegistered is part of the CoarseGrainedSchedulerBackend abstraction. sufficientResourcesRegistered holds (is true ) when the totalRegisteredExecutors is at least the ratio of the initial executors . Initial Executors \u00b6 initialExecutors : Int KubernetesClusterSchedulerBackend calculates the initial target number of executors when created . initialExecutors is used when KubernetesClusterSchedulerBackend is requested to start and whether or not sufficient resources registered . Minimum Resources Available Ratio \u00b6 minRegisteredRatio : Double minRegisteredRatio is part of the CoarseGrainedSchedulerBackend abstraction. minRegisteredRatio is 0.8 unless spark.scheduler.minRegisteredResourcesRatio is defined. Starting SchedulerBackend \u00b6 start () : Unit start is part of the CoarseGrainedSchedulerBackend abstraction. start creates a delegation token manager . start requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors . start requests the ExecutorPodsLifecycleManager to start (with this KubernetesClusterSchedulerBackend ). start requests the ExecutorPodsAllocator to start (with the applicationId ) start requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId ) start requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId ) Creating DriverEndpoint \u00b6 createDriverEndpoint () : DriverEndpoint createDriverEndpoint is part of the CoarseGrainedSchedulerBackend abstraction. createDriverEndpoint creates a KubernetesDriverEndpoint . Requesting Executors from Cluster Manager \u00b6 doRequestTotalExecutors ( requestedTotal : Int ) : Future [ Boolean ] doRequestTotalExecutors is part of the CoarseGrainedSchedulerBackend abstraction. doRequestTotalExecutors requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given requestedTotal . In the end, doRequestTotalExecutors returns a completed Future with true value. Stopping SchedulerBackend \u00b6 stop () : Unit stop is part of the CoarseGrainedSchedulerBackend abstraction. stop ...FIXME","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#kubernetesclusterschedulerbackend","text":"KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend for Kubernetes .","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#creating-instance","text":"KubernetesClusterSchedulerBackend takes the following to be created: TaskSchedulerImpl SparkContext KubernetesClient Java's ScheduledExecutorService ExecutorPodsSnapshotsStore ExecutorPodsAllocator ExecutorPodsLifecycleManager ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource KubernetesClusterSchedulerBackend is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"KubernetesClusterSchedulerBackend/#executorpodslifecyclemanager","text":"KubernetesClusterSchedulerBackend is given an ExecutorPodsLifecycleManager to be created . KubernetesClusterSchedulerBackend requests the ExecutorPodsLifecycleManager to start (with itself) when started .","title":" ExecutorPodsLifecycleManager"},{"location":"KubernetesClusterSchedulerBackend/#executorpodsallocator","text":"KubernetesClusterSchedulerBackend is given an ExecutorPodsAllocator to be created . When started , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the number of initial executors and starts it with application Id . When requested for the expected number of executors , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given total number of executors. When requested to isBlacklisted , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to isDeleted with a given executor.","title":" ExecutorPodsAllocator"},{"location":"KubernetesClusterSchedulerBackend/#application-id","text":"applicationId () : String applicationId is part of the SchedulerBackend abstraction. applicationId is the value of spark.app.id configuration property if defined or the default applicationId .","title":" Application Id"},{"location":"KubernetesClusterSchedulerBackend/#sufficient-resources-registered","text":"sufficientResourcesRegistered () : Boolean sufficientResourcesRegistered is part of the CoarseGrainedSchedulerBackend abstraction. sufficientResourcesRegistered holds (is true ) when the totalRegisteredExecutors is at least the ratio of the initial executors .","title":" Sufficient Resources Registered"},{"location":"KubernetesClusterSchedulerBackend/#initial-executors","text":"initialExecutors : Int KubernetesClusterSchedulerBackend calculates the initial target number of executors when created . initialExecutors is used when KubernetesClusterSchedulerBackend is requested to start and whether or not sufficient resources registered .","title":" Initial Executors"},{"location":"KubernetesClusterSchedulerBackend/#minimum-resources-available-ratio","text":"minRegisteredRatio : Double minRegisteredRatio is part of the CoarseGrainedSchedulerBackend abstraction. minRegisteredRatio is 0.8 unless spark.scheduler.minRegisteredResourcesRatio is defined.","title":" Minimum Resources Available Ratio"},{"location":"KubernetesClusterSchedulerBackend/#starting-schedulerbackend","text":"start () : Unit start is part of the CoarseGrainedSchedulerBackend abstraction. start creates a delegation token manager . start requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors . start requests the ExecutorPodsLifecycleManager to start (with this KubernetesClusterSchedulerBackend ). start requests the ExecutorPodsAllocator to start (with the applicationId ) start requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId ) start requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId )","title":" Starting SchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#creating-driverendpoint","text":"createDriverEndpoint () : DriverEndpoint createDriverEndpoint is part of the CoarseGrainedSchedulerBackend abstraction. createDriverEndpoint creates a KubernetesDriverEndpoint .","title":" Creating DriverEndpoint"},{"location":"KubernetesClusterSchedulerBackend/#requesting-executors-from-cluster-manager","text":"doRequestTotalExecutors ( requestedTotal : Int ) : Future [ Boolean ] doRequestTotalExecutors is part of the CoarseGrainedSchedulerBackend abstraction. doRequestTotalExecutors requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given requestedTotal . In the end, doRequestTotalExecutors returns a completed Future with true value.","title":" Requesting Executors from Cluster Manager"},{"location":"KubernetesClusterSchedulerBackend/#stopping-schedulerbackend","text":"stop () : Unit stop is part of the CoarseGrainedSchedulerBackend abstraction. stop ...FIXME","title":" Stopping SchedulerBackend"},{"location":"KubernetesConf/","text":"KubernetesConf \u00b6 KubernetesConf is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors ). Contract \u00b6 annotations \u00b6 annotations : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod environment \u00b6 environment : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod labels \u00b6 labels : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod DriverServiceFeatureStep is requested to getAdditionalKubernetesResources resourceNamePrefix \u00b6 resourceNamePrefix : String Prefix of resource names secretEnvNamesToKeyRefs \u00b6 secretEnvNamesToKeyRefs : Map [ String , String ] Used when: EnvSecretsFeatureStep is requested to configurePod secretNamesToMountPaths \u00b6 secretNamesToMountPaths : Map [ String , String ] Used when: MountSecretsFeatureStep is requested to configurePod volumes \u00b6 volumes : Seq [ KubernetesVolumeSpec ] Used when: MountVolumesFeatureStep is requested to configurePod Implementations \u00b6 KubernetesDriverConf KubernetesExecutorConf Creating Instance \u00b6 KubernetesConf takes the following to be created: SparkConf Abstract Class KubernetesConf is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs . Namespace \u00b6 namespace : String namespace is the value of spark.kubernetes.namespace configuration property. namespace is used when: DriverServiceFeatureStep is requested to getAdditionalPodSystemProperties Client is requested to run KubernetesClientApplication is requested to start imagePullPolicy \u00b6 imagePullPolicy : String imagePullPolicy is the value of spark.kubernetes.container.image.pullPolicy configuration property. imagePullPolicy is used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod Creating KubernetesDriverConf \u00b6 createDriverConf ( sparkConf : SparkConf , appId : String , mainAppResource : MainAppResource , mainClass : String , appArgs : Array [ String ]) : KubernetesDriverConf Note The goal of createDriverConf is to validate executor volumes before creating a KubernetesDriverConf . createDriverConf parse volumes for executors (with spark.kubernetes.executor.volumes prefix). Note createDriverConf parses executor volumes in order to verify configuration before the driver pod is created. In the end, createDriverConf creates a KubernetesDriverConf . createDriverConf is used when: KubernetesClientApplication is requested to start","title":"KubernetesConf"},{"location":"KubernetesConf/#kubernetesconf","text":"KubernetesConf is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors ).","title":"KubernetesConf"},{"location":"KubernetesConf/#contract","text":"","title":"Contract"},{"location":"KubernetesConf/#annotations","text":"annotations : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" annotations"},{"location":"KubernetesConf/#environment","text":"environment : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" environment"},{"location":"KubernetesConf/#labels","text":"labels : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod DriverServiceFeatureStep is requested to getAdditionalKubernetesResources","title":" labels"},{"location":"KubernetesConf/#resourcenameprefix","text":"resourceNamePrefix : String Prefix of resource names","title":" resourceNamePrefix"},{"location":"KubernetesConf/#secretenvnamestokeyrefs","text":"secretEnvNamesToKeyRefs : Map [ String , String ] Used when: EnvSecretsFeatureStep is requested to configurePod","title":" secretEnvNamesToKeyRefs"},{"location":"KubernetesConf/#secretnamestomountpaths","text":"secretNamesToMountPaths : Map [ String , String ] Used when: MountSecretsFeatureStep is requested to configurePod","title":" secretNamesToMountPaths"},{"location":"KubernetesConf/#volumes","text":"volumes : Seq [ KubernetesVolumeSpec ] Used when: MountVolumesFeatureStep is requested to configurePod","title":" volumes"},{"location":"KubernetesConf/#implementations","text":"KubernetesDriverConf KubernetesExecutorConf","title":"Implementations"},{"location":"KubernetesConf/#creating-instance","text":"KubernetesConf takes the following to be created: SparkConf Abstract Class KubernetesConf is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs .","title":"Creating Instance"},{"location":"KubernetesConf/#namespace","text":"namespace : String namespace is the value of spark.kubernetes.namespace configuration property. namespace is used when: DriverServiceFeatureStep is requested to getAdditionalPodSystemProperties Client is requested to run KubernetesClientApplication is requested to start","title":" Namespace"},{"location":"KubernetesConf/#imagepullpolicy","text":"imagePullPolicy : String imagePullPolicy is the value of spark.kubernetes.container.image.pullPolicy configuration property. imagePullPolicy is used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" imagePullPolicy"},{"location":"KubernetesConf/#creating-kubernetesdriverconf","text":"createDriverConf ( sparkConf : SparkConf , appId : String , mainAppResource : MainAppResource , mainClass : String , appArgs : Array [ String ]) : KubernetesDriverConf Note The goal of createDriverConf is to validate executor volumes before creating a KubernetesDriverConf . createDriverConf parse volumes for executors (with spark.kubernetes.executor.volumes prefix). Note createDriverConf parses executor volumes in order to verify configuration before the driver pod is created. In the end, createDriverConf creates a KubernetesDriverConf . createDriverConf is used when: KubernetesClientApplication is requested to start","title":" Creating KubernetesDriverConf"},{"location":"KubernetesDriverBuilder/","text":"KubernetesDriverBuilder \u00b6 KubernetesDriverBuilder is used to build a specification of a driver pod . Creating Instance \u00b6 KubernetesDriverBuilder takes no arguments to be created. KubernetesDriverBuilder is created when: KubernetesClientApplication is requested to run KubernetesDriverSpec \u00b6 KubernetesDriverSpec is the following: SparkPod Driver Resources System Properties Building KubernetesDriverSpec from Features \u00b6 buildFromFeatures ( conf : KubernetesDriverConf , client : KubernetesClient ) : KubernetesDriverSpec buildFromFeatures creates an initial driver pod specification. With spark.kubernetes.driver.podTemplateFile configuration property defined, buildFromFeatures loads it (with the given KubernetesClient and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification. buildFromFeatures builds a KubernetesDriverSpec (with the initial driver pod specification). In the end, buildFromFeatures configures the driver pod specification using the following feature steps: BasicDriverFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep DriverCommandFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep PodTemplateConfigMapStep LocalDirsFeatureStep buildFromFeatures ...FIXME buildFromFeatures is used when: Client is requested to run","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#kubernetesdriverbuilder","text":"KubernetesDriverBuilder is used to build a specification of a driver pod .","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#creating-instance","text":"KubernetesDriverBuilder takes no arguments to be created. KubernetesDriverBuilder is created when: KubernetesClientApplication is requested to run","title":"Creating Instance"},{"location":"KubernetesDriverBuilder/#kubernetesdriverspec","text":"KubernetesDriverSpec is the following: SparkPod Driver Resources System Properties","title":" KubernetesDriverSpec"},{"location":"KubernetesDriverBuilder/#building-kubernetesdriverspec-from-features","text":"buildFromFeatures ( conf : KubernetesDriverConf , client : KubernetesClient ) : KubernetesDriverSpec buildFromFeatures creates an initial driver pod specification. With spark.kubernetes.driver.podTemplateFile configuration property defined, buildFromFeatures loads it (with the given KubernetesClient and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification. buildFromFeatures builds a KubernetesDriverSpec (with the initial driver pod specification). In the end, buildFromFeatures configures the driver pod specification using the following feature steps: BasicDriverFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep DriverCommandFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep PodTemplateConfigMapStep LocalDirsFeatureStep buildFromFeatures ...FIXME buildFromFeatures is used when: Client is requested to run","title":" Building KubernetesDriverSpec from Features"},{"location":"KubernetesDriverConf/","text":"KubernetesDriverConf \u00b6 KubernetesDriverConf is a KubernetesConf . Creating Instance \u00b6 KubernetesDriverConf takes the following to be created: SparkConf Application ID MainAppResource Name of the Main Class Application Arguments KubernetesDriverConf is created when: KubernetesClientApplication is requested to start (via KubernetesConf utility ) volumes \u00b6 volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes extracts volumes for the driver (with the spark.kubernetes.driver.volumes prefix) from the SparkConf .","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#kubernetesdriverconf","text":"KubernetesDriverConf is a KubernetesConf .","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#creating-instance","text":"KubernetesDriverConf takes the following to be created: SparkConf Application ID MainAppResource Name of the Main Class Application Arguments KubernetesDriverConf is created when: KubernetesClientApplication is requested to start (via KubernetesConf utility )","title":"Creating Instance"},{"location":"KubernetesDriverConf/#volumes","text":"volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes extracts volumes for the driver (with the spark.kubernetes.driver.volumes prefix) from the SparkConf .","title":" volumes"},{"location":"KubernetesDriverEndpoint/","text":"KubernetesDriverEndpoint \u00b6 KubernetesDriverEndpoint is a DriverEndpoint . Intercepting Executor Lost Event \u00b6 onDisconnected ( rpcAddress : RpcAddress ) : Unit onDisconnected is part of the RpcEndpoint abstraction. onDisconnected disables the executor known by the RpcAddress (found in the Executors by RpcAddress Registry registry).","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#kubernetesdriverendpoint","text":"KubernetesDriverEndpoint is a DriverEndpoint .","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#intercepting-executor-lost-event","text":"onDisconnected ( rpcAddress : RpcAddress ) : Unit onDisconnected is part of the RpcEndpoint abstraction. onDisconnected disables the executor known by the RpcAddress (found in the Executors by RpcAddress Registry registry).","title":" Intercepting Executor Lost Event"},{"location":"KubernetesExecutorBuilder/","text":"KubernetesExecutorBuilder \u00b6 KubernetesExecutorBuilder is...FIXME","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorBuilder/#kubernetesexecutorbuilder","text":"KubernetesExecutorBuilder is...FIXME","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorConf/","text":"KubernetesExecutorConf \u00b6 KubernetesExecutorConf is...FIXME","title":"KubernetesExecutorConf"},{"location":"KubernetesExecutorConf/#kubernetesexecutorconf","text":"KubernetesExecutorConf is...FIXME","title":"KubernetesExecutorConf"},{"location":"KubernetesFeatureConfigStep/","text":"KubernetesFeatureConfigStep \u00b6 KubernetesFeatureConfigStep is an abstraction of Kubernetes pod features for drivers and executors. Contract \u00b6 Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod Used when: KubernetesDriverBuilder is requested to buildFromFeatures KubernetesExecutorBuilder is requested to buildFromFeatures Additional Kubernetes Resources \u00b6 getAdditionalKubernetesResources () : Seq [ HasMetadata ] Additional Kubernetes resources Default: empty Used when: KubernetesDriverBuilder is requested to buildFromFeatures Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] System properties to set on the JVM Default: empty Used when: KubernetesDriverBuilder is requested to buildFromFeatures Implementations \u00b6 BasicDriverFeatureStep BasicExecutorFeatureStep DriverCommandFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep EnvSecretsFeatureStep ExecutorKubernetesCredentialsFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep LocalDirsFeatureStep MountSecretsFeatureStep MountVolumesFeatureStep PodTemplateConfigMapStep","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#kubernetesfeatureconfigstep","text":"KubernetesFeatureConfigStep is an abstraction of Kubernetes pod features for drivers and executors.","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#contract","text":"","title":"Contract"},{"location":"KubernetesFeatureConfigStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod Used when: KubernetesDriverBuilder is requested to buildFromFeatures KubernetesExecutorBuilder is requested to buildFromFeatures","title":" Configuring Pod"},{"location":"KubernetesFeatureConfigStep/#additional-kubernetes-resources","text":"getAdditionalKubernetesResources () : Seq [ HasMetadata ] Additional Kubernetes resources Default: empty Used when: KubernetesDriverBuilder is requested to buildFromFeatures","title":" Additional Kubernetes Resources"},{"location":"KubernetesFeatureConfigStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] System properties to set on the JVM Default: empty Used when: KubernetesDriverBuilder is requested to buildFromFeatures","title":" Additional System Properties"},{"location":"KubernetesFeatureConfigStep/#implementations","text":"BasicDriverFeatureStep BasicExecutorFeatureStep DriverCommandFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep EnvSecretsFeatureStep ExecutorKubernetesCredentialsFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep LocalDirsFeatureStep MountSecretsFeatureStep MountVolumesFeatureStep PodTemplateConfigMapStep","title":"Implementations"},{"location":"KubernetesUtils/","text":"KubernetesUtils \u00b6 Loading Pod from Template File \u00b6 loadPodFromTemplate ( kubernetesClient : KubernetesClient , templateFile : File , containerName : Option [ String ]) : SparkPod loadPodFromTemplate requests the given KubernetesClient to load a pod for the input template file. loadPodFromTemplate selectSparkContainer (with the pod and the input container name). In case of an Exception , loadPodFromTemplate prints out the following ERROR message to the logs: Encountered exception while attempting to load initial pod spec from file loadPodFromTemplate (re)throws a SparkException : Could not load pod from template file. loadPodFromTemplate is used when: KubernetesClusterManager is requested to createSchedulerBackend KubernetesDriverBuilder is requested to buildFromFeatures KubernetesExecutorBuilder is requested to buildFromFeatures uploadAndTransformFileUris \u00b6 uploadAndTransformFileUris ( fileUris : Iterable [ String ], conf : Option [ SparkConf ] = None ) : Iterable [ String ] uploadAndTransformFileUris ...FIXME uploadAndTransformFileUris is used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties uploadFileUri \u00b6 uploadFileUri ( uri : String , conf : Option [ SparkConf ] = None ) : String uploadFileUri ...FIXME renameMainAppResource \u00b6 renameMainAppResource ( resource : String , conf : SparkConf ) : String renameMainAppResource is converted to spark-internal internal name when the given resource is local and resolvable . Otherwise, renameMainAppResource returns the given resource as-is. renameMainAppResource is used when: DriverCommandFeatureStep is requested for a base container for drivers (for a JavaMainAppResource application) isLocalAndResolvable \u00b6 isLocalAndResolvable ( resource : String ) : Boolean isLocalAndResolvable is true when the given resource is not internal and a local dependency (after converting to a well-formed URI) isLocalAndResolvable is used when: KubernetesUtils is requested to renameMainAppResource BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties isLocalDependency \u00b6 isLocalDependency ( uri : URI ) : Boolean An input URI is a local dependency when the scheme is null (undefined) or file .","title":"KubernetesUtils"},{"location":"KubernetesUtils/#kubernetesutils","text":"","title":"KubernetesUtils"},{"location":"KubernetesUtils/#loading-pod-from-template-file","text":"loadPodFromTemplate ( kubernetesClient : KubernetesClient , templateFile : File , containerName : Option [ String ]) : SparkPod loadPodFromTemplate requests the given KubernetesClient to load a pod for the input template file. loadPodFromTemplate selectSparkContainer (with the pod and the input container name). In case of an Exception , loadPodFromTemplate prints out the following ERROR message to the logs: Encountered exception while attempting to load initial pod spec from file loadPodFromTemplate (re)throws a SparkException : Could not load pod from template file. loadPodFromTemplate is used when: KubernetesClusterManager is requested to createSchedulerBackend KubernetesDriverBuilder is requested to buildFromFeatures KubernetesExecutorBuilder is requested to buildFromFeatures","title":" Loading Pod from Template File"},{"location":"KubernetesUtils/#uploadandtransformfileuris","text":"uploadAndTransformFileUris ( fileUris : Iterable [ String ], conf : Option [ SparkConf ] = None ) : Iterable [ String ] uploadAndTransformFileUris ...FIXME uploadAndTransformFileUris is used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties","title":" uploadAndTransformFileUris"},{"location":"KubernetesUtils/#uploadfileuri","text":"uploadFileUri ( uri : String , conf : Option [ SparkConf ] = None ) : String uploadFileUri ...FIXME","title":" uploadFileUri"},{"location":"KubernetesUtils/#renamemainappresource","text":"renameMainAppResource ( resource : String , conf : SparkConf ) : String renameMainAppResource is converted to spark-internal internal name when the given resource is local and resolvable . Otherwise, renameMainAppResource returns the given resource as-is. renameMainAppResource is used when: DriverCommandFeatureStep is requested for a base container for drivers (for a JavaMainAppResource application)","title":" renameMainAppResource"},{"location":"KubernetesUtils/#islocalandresolvable","text":"isLocalAndResolvable ( resource : String ) : Boolean isLocalAndResolvable is true when the given resource is not internal and a local dependency (after converting to a well-formed URI) isLocalAndResolvable is used when: KubernetesUtils is requested to renameMainAppResource BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties","title":" isLocalAndResolvable"},{"location":"KubernetesUtils/#islocaldependency","text":"isLocalDependency ( uri : URI ) : Boolean An input URI is a local dependency when the scheme is null (undefined) or file .","title":" isLocalDependency"},{"location":"KubernetesVolumeUtils/","text":"KubernetesVolumeUtils \u00b6 parseVolumesWithPrefix \u00b6 parseVolumesWithPrefix ( sparkConf : SparkConf , prefix : String ) : Seq [ KubernetesVolumeSpec ] parseVolumesWithPrefix ...FIXME parseVolumesWithPrefix is used when: KubernetesDriverConf is requested for volumes KubernetesExecutorConf is requested for volumes KubernetesConf utility is used to create a KubernetesDriverConf","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#kubernetesvolumeutils","text":"","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#parsevolumeswithprefix","text":"parseVolumesWithPrefix ( sparkConf : SparkConf , prefix : String ) : Seq [ KubernetesVolumeSpec ] parseVolumesWithPrefix ...FIXME parseVolumesWithPrefix is used when: KubernetesDriverConf is requested for volumes KubernetesExecutorConf is requested for volumes KubernetesConf utility is used to create a KubernetesDriverConf","title":" parseVolumesWithPrefix"},{"location":"LoggingPodStatusWatcher/","text":"LoggingPodStatusWatcher \u00b6 LoggingPodStatusWatcher is an extension of Kubernetes' Watcher[Pod] for pod watchers that can watchOrStop . LoggingPodStatusWatcher is used to create a Client when a KubernetesClientApplication is requested to start . Contract \u00b6 watchOrStop \u00b6 watchOrStop ( submissionId : String ) : Unit Used when: Client is requested to run Implementations \u00b6 LoggingPodStatusWatcherImpl","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#loggingpodstatuswatcher","text":"LoggingPodStatusWatcher is an extension of Kubernetes' Watcher[Pod] for pod watchers that can watchOrStop . LoggingPodStatusWatcher is used to create a Client when a KubernetesClientApplication is requested to start .","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#contract","text":"","title":"Contract"},{"location":"LoggingPodStatusWatcher/#watchorstop","text":"watchOrStop ( submissionId : String ) : Unit Used when: Client is requested to run","title":" watchOrStop"},{"location":"LoggingPodStatusWatcher/#implementations","text":"LoggingPodStatusWatcherImpl","title":"Implementations"},{"location":"LoggingPodStatusWatcherImpl/","text":"LoggingPodStatusWatcherImpl \u00b6 LoggingPodStatusWatcherImpl is a LoggingPodStatusWatcher that monitors and logs the application status. Creating Instance \u00b6 LoggingPodStatusWatcherImpl takes the following to be created: KubernetesDriverConf LoggingPodStatusWatcherImpl is created when: KubernetesClientApplication is requested to start eventReceived \u00b6 eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes' Watcher abstraction. eventReceived brances off based on the given Action : For DELETED or ERROR actions, eventReceived closeWatch For any other actions, logLongStatus followed by closeWatch if hasCompleted . logLongStatus \u00b6 logLongStatus () : Unit logLongStatus prints out the following INFO message to the logs: State changed, new state: [formatPodState|unknown] hasCompleted \u00b6 hasCompleted () : Boolean hasCompleted is true when the phase is Succeeded or Failed . hasCompleted is used when: LoggingPodStatusWatcherImpl is requested to eventReceived (when an action is neither DELETED nor ERROR ) podCompleted Flag \u00b6 LoggingPodStatusWatcherImpl turns podCompleted off when created . Until podCompleted is on, LoggingPodStatusWatcherImpl waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) podCompleted turns podCompleted on when closeWatch . closeWatch \u00b6 closeWatch () : Unit closeWatch turns podCompleted on. closeWatch is used when: LoggingPodStatusWatcherImpl is requested to eventReceived and onClose Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL Refer to Logging .","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#loggingpodstatuswatcherimpl","text":"LoggingPodStatusWatcherImpl is a LoggingPodStatusWatcher that monitors and logs the application status.","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#creating-instance","text":"LoggingPodStatusWatcherImpl takes the following to be created: KubernetesDriverConf LoggingPodStatusWatcherImpl is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"LoggingPodStatusWatcherImpl/#eventreceived","text":"eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes' Watcher abstraction. eventReceived brances off based on the given Action : For DELETED or ERROR actions, eventReceived closeWatch For any other actions, logLongStatus followed by closeWatch if hasCompleted .","title":" eventReceived"},{"location":"LoggingPodStatusWatcherImpl/#loglongstatus","text":"logLongStatus () : Unit logLongStatus prints out the following INFO message to the logs: State changed, new state: [formatPodState|unknown]","title":" logLongStatus"},{"location":"LoggingPodStatusWatcherImpl/#hascompleted","text":"hasCompleted () : Boolean hasCompleted is true when the phase is Succeeded or Failed . hasCompleted is used when: LoggingPodStatusWatcherImpl is requested to eventReceived (when an action is neither DELETED nor ERROR )","title":" hasCompleted"},{"location":"LoggingPodStatusWatcherImpl/#podcompleted-flag","text":"LoggingPodStatusWatcherImpl turns podCompleted off when created . Until podCompleted is on, LoggingPodStatusWatcherImpl waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) podCompleted turns podCompleted on when closeWatch .","title":" podCompleted Flag"},{"location":"LoggingPodStatusWatcherImpl/#closewatch","text":"closeWatch () : Unit closeWatch turns podCompleted on. closeWatch is used when: LoggingPodStatusWatcherImpl is requested to eventReceived and onClose","title":" closeWatch"},{"location":"LoggingPodStatusWatcherImpl/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL Refer to Logging .","title":"Logging"},{"location":"MountSecretsFeatureStep/","text":"MountSecretsFeatureStep \u00b6 MountSecretsFeatureStep is...FIXME","title":"MountSecretsFeatureStep"},{"location":"MountSecretsFeatureStep/#mountsecretsfeaturestep","text":"MountSecretsFeatureStep is...FIXME","title":"MountSecretsFeatureStep"},{"location":"MountVolumesFeatureStep/","text":"MountVolumesFeatureStep \u00b6 MountVolumesFeatureStep is...FIXME","title":"MountVolumesFeatureStep"},{"location":"MountVolumesFeatureStep/#mountvolumesfeaturestep","text":"MountVolumesFeatureStep is...FIXME","title":"MountVolumesFeatureStep"},{"location":"PodTemplateConfigMapStep/","text":"PodTemplateConfigMapStep \u00b6 PodTemplateConfigMapStep is...FIXME","title":"PodTemplateConfigMapStep"},{"location":"PodTemplateConfigMapStep/#podtemplateconfigmapstep","text":"PodTemplateConfigMapStep is...FIXME","title":"PodTemplateConfigMapStep"},{"location":"PollRunnable/","text":"PollRunnable \u00b6 PollRunnable is a Java Runnable that ExecutorPodsPollingSnapshotSource uses to run regularly for current snapshots of the executor pods of the application . Internal Class PollRunnable is an internal class of ExecutorPodsPollingSnapshotSource with full access to internal registries. Creating Instance \u00b6 PollRunnable takes the following to be created: Application Id PollRunnable is created when: ExecutorPodsPollingSnapshotSource is requested to start Starting Thread \u00b6 run () : Unit run prints out the following DEBUG message to the logs: Resynchronizing full executor pod state from Kubernetes. run requests the KubernetesClient for Spark executor pods that are pods with the following labels and values: spark-app-selector as the application Id spark-role as executor In the end, run requests the ExecutorPodsSnapshotsStore to replace the snapshot . Logging \u00b6 PollRunnable uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.","title":"PollRunnable"},{"location":"PollRunnable/#pollrunnable","text":"PollRunnable is a Java Runnable that ExecutorPodsPollingSnapshotSource uses to run regularly for current snapshots of the executor pods of the application . Internal Class PollRunnable is an internal class of ExecutorPodsPollingSnapshotSource with full access to internal registries.","title":"PollRunnable"},{"location":"PollRunnable/#creating-instance","text":"PollRunnable takes the following to be created: Application Id PollRunnable is created when: ExecutorPodsPollingSnapshotSource is requested to start","title":"Creating Instance"},{"location":"PollRunnable/#starting-thread","text":"run () : Unit run prints out the following DEBUG message to the logs: Resynchronizing full executor pod state from Kubernetes. run requests the KubernetesClient for Spark executor pods that are pods with the following labels and values: spark-app-selector as the application Id spark-role as executor In the end, run requests the ExecutorPodsSnapshotsStore to replace the snapshot .","title":" Starting Thread"},{"location":"PollRunnable/#logging","text":"PollRunnable uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.","title":"Logging"},{"location":"SparkKubernetesClientFactory/","text":"SparkKubernetesClientFactory \u00b6 SparkKubernetesClientFactory is a Spark-opinionated builder for Kubernetes clients . Creating KubernetesClient \u00b6 createKubernetesClient ( master : String , namespace : Option [ String ], kubernetesAuthConfPrefix : String , clientType : ClientType.Value , sparkConf : SparkConf , defaultServiceAccountToken : Option [ File ], defaultServiceAccountCaCert : Option [ File ]) : KubernetesClient createKubernetesClient utility takes the OAuth token-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .oauthTokenFile (or defaults to the input defaultServiceAccountToken ) kubernetesAuthConfPrefix .oauthToken createKubernetesClient takes the spark.kubernetes.context configuraiton property ( kubeContext ). createKubernetesClient takes the certificate-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .caCertFile (or defaults to the input defaultServiceAccountCaCert ) kubernetesAuthConfPrefix .clientKeyFile kubernetesAuthConfPrefix .clientCertFile createKubernetesClient prints out the following INFO message to the logs: Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file createKubernetesClient builds a Kubernetes Config (based on the configuration properties). createKubernetesClient builds an OkHttpClient with a custom kubernetes-dispatcher dispatcher. In the end, createKubernetesClient creates a Kubernetes DefaultKubernetesClient (with the OkHttpClient and Config ). createKubernetesClient is used when: K8SSparkSubmitOperation is requested to execute KubernetesClientApplication is requested to start KubernetesClusterManager is requested to create a SchedulerBackend Exceptions \u00b6 createKubernetesClient throws an IllegalArgumentException when an OAuth token is specified through a file and a value: Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf]. Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.SparkKubernetesClientFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL Refer to Logging .","title":"SparkKubernetesClientFactory"},{"location":"SparkKubernetesClientFactory/#sparkkubernetesclientfactory","text":"SparkKubernetesClientFactory is a Spark-opinionated builder for Kubernetes clients .","title":"SparkKubernetesClientFactory"},{"location":"SparkKubernetesClientFactory/#creating-kubernetesclient","text":"createKubernetesClient ( master : String , namespace : Option [ String ], kubernetesAuthConfPrefix : String , clientType : ClientType.Value , sparkConf : SparkConf , defaultServiceAccountToken : Option [ File ], defaultServiceAccountCaCert : Option [ File ]) : KubernetesClient createKubernetesClient utility takes the OAuth token-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .oauthTokenFile (or defaults to the input defaultServiceAccountToken ) kubernetesAuthConfPrefix .oauthToken createKubernetesClient takes the spark.kubernetes.context configuraiton property ( kubeContext ). createKubernetesClient takes the certificate-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .caCertFile (or defaults to the input defaultServiceAccountCaCert ) kubernetesAuthConfPrefix .clientKeyFile kubernetesAuthConfPrefix .clientCertFile createKubernetesClient prints out the following INFO message to the logs: Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file createKubernetesClient builds a Kubernetes Config (based on the configuration properties). createKubernetesClient builds an OkHttpClient with a custom kubernetes-dispatcher dispatcher. In the end, createKubernetesClient creates a Kubernetes DefaultKubernetesClient (with the OkHttpClient and Config ). createKubernetesClient is used when: K8SSparkSubmitOperation is requested to execute KubernetesClientApplication is requested to start KubernetesClusterManager is requested to create a SchedulerBackend","title":" Creating KubernetesClient"},{"location":"SparkKubernetesClientFactory/#exceptions","text":"createKubernetesClient throws an IllegalArgumentException when an OAuth token is specified through a file and a value: Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf].","title":"Exceptions"},{"location":"SparkKubernetesClientFactory/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.SparkKubernetesClientFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL Refer to Logging .","title":"Logging"},{"location":"configuration-properties/","text":"Configuration Properties of Spark on Kubernetes \u00b6 spark.kubernetes.container.image.pullPolicy \u00b6 Kubernetes image pull policy: Always Never IfNotPresent Default: IfNotPresent Used when: KubernetesConf is requested to imagePullPolicy spark.kubernetes.file.upload.path \u00b6 Hadoop DFS-compatible file system path where files from the local file system will be uploded to in cluster mode. Default: (undefined) Used when: KubernetesUtils is requested to uploadFileUri spark.kubernetes.report.interval \u00b6 Interval between reports of the current app status in cluster mode Default: 1s Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop spark.kubernetes.executor.deleteOnTermination \u00b6 When disabled ( false ), executor pods will not be deleted in case of failure or normal termination Default: true Used when: ExecutorPodsAllocator is requested to onNewSnapshots ExecutorPodsLifecycleManager is requested to onFinalNonDeletedState KubernetesClusterSchedulerBackend is requested to stop spark.kubernetes.executor.apiPollingInterval \u00b6 Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors. Default: 30s Used when: ExecutorPodsPollingSnapshotSource is requested to start spark.kubernetes.executor.eventProcessingInterval \u00b6 Interval (in millis) between successive inspection of executor events sent from the Kubernetes API Default: 1s Used when: ExecutorPodsLifecycleManager is requested to start and register a new subscriber spark.kubernetes.namespace \u00b6 Namespace for running the driver and executor pods Default: default Used when: KubernetesConf is requested for namespace KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsAllocator is created (and initializes namespace ) spark.kubernetes.context \u00b6 The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the KUBECONFIG environment variable or by defaulting to .kube/config under your home directory. If not specified then your current context is used. You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options. Default: (undefined) Used when: SparkKubernetesClientFactory is requested to create a KubernetesClient spark.kubernetes.submitInDriver \u00b6 (internal) Whether executing in cluster deploy mode Default: false spark.kubernetes.submitInDriver is true in BasicDriverFeatureStep . Used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties KubernetesClusterManager is requested for a SchedulerBackend spark.kubernetes.driver.podTemplateFile \u00b6 Pod template file for drivers Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures spark.kubernetes.executor.podTemplateFile \u00b6 Pod template file for executors Default: (undefined) Used when: KubernetesClusterManager is requested to createSchedulerBackend KubernetesExecutorBuilder is requested to buildFromFeatures PodTemplateConfigMapStep is created and requested to configurePod , getAdditionalPodSystemProperties , getAdditionalKubernetesResources spark.kubernetes.driver.podTemplateContainerName \u00b6 Container name for a driver in the given pod template Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures spark.kubernetes.memoryOverheadFactor \u00b6 Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs Must be a double between (0, 1.0) Default: 0.1 spark.kubernetes.container.image \u00b6 Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined) Default: (undefined) spark.kubernetes.driver.container.image \u00b6 Container image for drivers Default: spark.kubernetes.container.image Used when: BasicDriverFeatureStep is requested for a driverContainerImage spark.kubernetes.executor.container.image \u00b6 Container image for executors Default: spark.kubernetes.container.image Used when: BasicExecutorFeatureStep is requested for a driverContainerImage spark.kubernetes.allocation.batch.delay \u00b6 Time (in millis) to wait between each round of executor allocation Default: 1s Used when: ExecutorPodsAllocator is created spark.kubernetes.driver.pod.name \u00b6 Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed using spark-submit in cluster mode spark.kubernetes.executor.podNamePrefix \u00b6 (internal) Prefix to use in front of the executor pod names Default: (undefined) spark.kubernetes.executor.podTemplateContainerName \u00b6 Container name to be used as a basis for executors in the given pod template Default: (undefined) spark.kubernetes.authenticate.driver.mounted \u00b6 FIXME spark.kubernetes.driver.master \u00b6 The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc spark.kubernetes.authenticate \u00b6 FIXME","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties-of-spark-on-kubernetes","text":"","title":"Configuration Properties of Spark on Kubernetes"},{"location":"configuration-properties/#sparkkubernetescontainerimagepullpolicy","text":"Kubernetes image pull policy: Always Never IfNotPresent Default: IfNotPresent Used when: KubernetesConf is requested to imagePullPolicy","title":" spark.kubernetes.container.image.pullPolicy"},{"location":"configuration-properties/#sparkkubernetesfileuploadpath","text":"Hadoop DFS-compatible file system path where files from the local file system will be uploded to in cluster mode. Default: (undefined) Used when: KubernetesUtils is requested to uploadFileUri","title":" spark.kubernetes.file.upload.path"},{"location":"configuration-properties/#sparkkubernetesreportinterval","text":"Interval between reports of the current app status in cluster mode Default: 1s Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop","title":" spark.kubernetes.report.interval"},{"location":"configuration-properties/#sparkkubernetesexecutordeleteontermination","text":"When disabled ( false ), executor pods will not be deleted in case of failure or normal termination Default: true Used when: ExecutorPodsAllocator is requested to onNewSnapshots ExecutorPodsLifecycleManager is requested to onFinalNonDeletedState KubernetesClusterSchedulerBackend is requested to stop","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"configuration-properties/#sparkkubernetesexecutorapipollinginterval","text":"Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors. Default: 30s Used when: ExecutorPodsPollingSnapshotSource is requested to start","title":" spark.kubernetes.executor.apiPollingInterval"},{"location":"configuration-properties/#sparkkubernetesexecutoreventprocessinginterval","text":"Interval (in millis) between successive inspection of executor events sent from the Kubernetes API Default: 1s Used when: ExecutorPodsLifecycleManager is requested to start and register a new subscriber","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"configuration-properties/#sparkkubernetesnamespace","text":"Namespace for running the driver and executor pods Default: default Used when: KubernetesConf is requested for namespace KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsAllocator is created (and initializes namespace )","title":" spark.kubernetes.namespace"},{"location":"configuration-properties/#sparkkubernetescontext","text":"The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the KUBECONFIG environment variable or by defaulting to .kube/config under your home directory. If not specified then your current context is used. You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options. Default: (undefined) Used when: SparkKubernetesClientFactory is requested to create a KubernetesClient","title":" spark.kubernetes.context"},{"location":"configuration-properties/#sparkkubernetessubmitindriver","text":"(internal) Whether executing in cluster deploy mode Default: false spark.kubernetes.submitInDriver is true in BasicDriverFeatureStep . Used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties KubernetesClusterManager is requested for a SchedulerBackend","title":" spark.kubernetes.submitInDriver"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatefile","text":"Pod template file for drivers Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures","title":" spark.kubernetes.driver.podTemplateFile"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatefile","text":"Pod template file for executors Default: (undefined) Used when: KubernetesClusterManager is requested to createSchedulerBackend KubernetesExecutorBuilder is requested to buildFromFeatures PodTemplateConfigMapStep is created and requested to configurePod , getAdditionalPodSystemProperties , getAdditionalKubernetesResources","title":" spark.kubernetes.executor.podTemplateFile"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatecontainername","text":"Container name for a driver in the given pod template Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures","title":" spark.kubernetes.driver.podTemplateContainerName"},{"location":"configuration-properties/#sparkkubernetesmemoryoverheadfactor","text":"Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs Must be a double between (0, 1.0) Default: 0.1","title":" spark.kubernetes.memoryOverheadFactor"},{"location":"configuration-properties/#sparkkubernetescontainerimage","text":"Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined) Default: (undefined)","title":" spark.kubernetes.container.image"},{"location":"configuration-properties/#sparkkubernetesdrivercontainerimage","text":"Container image for drivers Default: spark.kubernetes.container.image Used when: BasicDriverFeatureStep is requested for a driverContainerImage","title":" spark.kubernetes.driver.container.image"},{"location":"configuration-properties/#sparkkubernetesexecutorcontainerimage","text":"Container image for executors Default: spark.kubernetes.container.image Used when: BasicExecutorFeatureStep is requested for a driverContainerImage","title":" spark.kubernetes.executor.container.image"},{"location":"configuration-properties/#sparkkubernetesallocationbatchdelay","text":"Time (in millis) to wait between each round of executor allocation Default: 1s Used when: ExecutorPodsAllocator is created","title":" spark.kubernetes.allocation.batch.delay"},{"location":"configuration-properties/#sparkkubernetesdriverpodname","text":"Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed using spark-submit in cluster mode","title":" spark.kubernetes.driver.pod.name"},{"location":"configuration-properties/#sparkkubernetesexecutorpodnameprefix","text":"(internal) Prefix to use in front of the executor pod names Default: (undefined)","title":" spark.kubernetes.executor.podNamePrefix"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatecontainername","text":"Container name to be used as a basis for executors in the given pod template Default: (undefined)","title":" spark.kubernetes.executor.podTemplateContainerName"},{"location":"configuration-properties/#sparkkubernetesauthenticatedrivermounted","text":"FIXME","title":" spark.kubernetes.authenticate.driver.mounted"},{"location":"configuration-properties/#sparkkubernetesdrivermaster","text":"The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc","title":" spark.kubernetes.driver.master"},{"location":"configuration-properties/#sparkkubernetesauthenticate","text":"FIXME","title":" spark.kubernetes.authenticate"},{"location":"overview/","text":"Spark on Kubernetes \u00b6 Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Apache Spark supports Kubernetes resource manager as a scheduler using KubernetesClusterManager for k8s:// -prefixed master URLs (that point at a Kubernetes API server ). Demo \u00b6 spark-shell on minikube Running Spark Application on minikube Resources \u00b6 Official documentation Spark on Kubernetes by Scott Haines (video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert","title":"Overview"},{"location":"overview/#spark-on-kubernetes","text":"Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Apache Spark supports Kubernetes resource manager as a scheduler using KubernetesClusterManager for k8s:// -prefixed master URLs (that point at a Kubernetes API server ).","title":"Spark on Kubernetes"},{"location":"overview/#demo","text":"spark-shell on minikube Running Spark Application on minikube","title":"Demo"},{"location":"overview/#resources","text":"Official documentation Spark on Kubernetes by Scott Haines (video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert","title":"Resources"}]}