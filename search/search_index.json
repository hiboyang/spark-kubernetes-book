{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Spark on Kubernetes (Apache Spark 3.0.1) \u00b6 Welcome to The Internals of Spark on Kubernetes online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark on Kubernetes \ud83d\udd25","title":"Welcome"},{"location":"#the-internals-of-spark-on-kubernetes-apache-spark-301","text":"Welcome to The Internals of Spark on Kubernetes online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Spark on Kubernetes as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark on Kubernetes \ud83d\udd25","title":"The Internals of Spark on Kubernetes (Apache Spark 3.0.1)"},{"location":"BasicDriverFeatureStep/","text":"BasicDriverFeatureStep \u00b6 BasicDriverFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 BasicDriverFeatureStep takes the following to be created: KubernetesDriverConf BasicDriverFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod spec Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional properties: Name Value spark.kubernetes.submitInDriver true spark.kubernetes.driver.pod.name driverPodName spark.kubernetes.memoryOverheadFactor overheadFactor spark.app.id appId (of the KubernetesDriverConf ) In the end, getAdditionalPodSystemProperties uploads local files (in spark.jars and spark.files configuration properties) to a Hadoop-compatible file system and adds their target Hadoop paths to the additional properties. spark.kubernetes.file.upload.path Configuration Property The Hadoop DFS path to upload local files to is defined using spark.kubernetes.file.upload.path configuration property. Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME Driver Container Image Name \u00b6 BasicDriverFeatureStep uses spark.kubernetes.driver.container.image for the name of the container image for drivers. The name must be defined or BasicDriverFeatureStep throws an SparkException : Must specify the driver container image driverContainerImage is used when requested for configurePod .","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#basicdriverfeaturestep","text":"BasicDriverFeatureStep is a KubernetesFeatureConfigStep .","title":"BasicDriverFeatureStep"},{"location":"BasicDriverFeatureStep/#creating-instance","text":"BasicDriverFeatureStep takes the following to be created: KubernetesDriverConf BasicDriverFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod spec","title":"Creating Instance"},{"location":"BasicDriverFeatureStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] getAdditionalPodSystemProperties is part of the KubernetesFeatureConfigStep abstraction. getAdditionalPodSystemProperties sets the following additional properties: Name Value spark.kubernetes.submitInDriver true spark.kubernetes.driver.pod.name driverPodName spark.kubernetes.memoryOverheadFactor overheadFactor spark.app.id appId (of the KubernetesDriverConf ) In the end, getAdditionalPodSystemProperties uploads local files (in spark.jars and spark.files configuration properties) to a Hadoop-compatible file system and adds their target Hadoop paths to the additional properties. spark.kubernetes.file.upload.path Configuration Property The Hadoop DFS path to upload local files to is defined using spark.kubernetes.file.upload.path configuration property.","title":" Additional System Properties"},{"location":"BasicDriverFeatureStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod ...FIXME","title":" Configuring Driver Pod"},{"location":"BasicDriverFeatureStep/#driver-container-image-name","text":"BasicDriverFeatureStep uses spark.kubernetes.driver.container.image for the name of the container image for drivers. The name must be defined or BasicDriverFeatureStep throws an SparkException : Must specify the driver container image driverContainerImage is used when requested for configurePod .","title":" Driver Container Image Name"},{"location":"BasicExecutorFeatureStep/","text":"BasicExecutorFeatureStep \u00b6 BasicExecutorFeatureStep is a KubernetesFeatureConfigStep . spark.kubernetes.executor.container.image \u00b6 BasicExecutorFeatureStep asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a SparkException : Must specify the executor container image","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#basicexecutorfeaturestep","text":"BasicExecutorFeatureStep is a KubernetesFeatureConfigStep .","title":"BasicExecutorFeatureStep"},{"location":"BasicExecutorFeatureStep/#sparkkubernetesexecutorcontainerimage","text":"BasicExecutorFeatureStep asserts that spark.kubernetes.executor.container.image configuration property is defined or throws a SparkException : Must specify the executor container image","title":" spark.kubernetes.executor.container.image"},{"location":"Client/","text":"Client \u00b6 Client submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status). Creating Instance \u00b6 Client takes the following to be created: KubernetesDriverConf KubernetesDriverBuilder Kubernetes' KubernetesClient LoggingPodStatusWatcher Client is created when: KubernetesClientApplication is requested to start Running Driver Pod \u00b6 run () : Unit run requests the KubernetesDriverBuilder to build a KubernetesDriverSpec . run requests the KubernetesDriverConf for the resourceNamePrefix to be used for the name of the driver's config map: [resourceNamePrefix]-driver-conf-map run builds a ConfigMap (with the name and the system properties of the KubernetesDriverSpec ). run creates a driver container (based on the KubernetesDriverSpec ) and adds the following: SPARK_CONF_DIR env var as /opt/spark/conf spark-conf-volume volume mount as /opt/spark/conf run creates a driver pod (based on the KubernetesDriverSpec ) with the driver container and a new spark-conf-volume volume for the ConfigMap . run requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher ) and, when available, attaches the ConfigMap . run is used when: KubernetesClientApplication is requested to start addDriverOwnerReference \u00b6 addDriverOwnerReference ( driverPod : Pod , resources : Seq [ HasMetadata ]) : Unit addDriverOwnerReference ...FIXME Building ConfigMap \u00b6 buildConfigMap ( configMapName : String , conf : Map [ String , String ]) : ConfigMap buildConfigMap builds a Kubernetes ConfigMap with the given conf key-value pairs stored as spark.properties data and the given configMapName name. The stored data uses an extra comment: Java properties built from Kubernetes config map with name: [configMapName] Kubernetes Documentation Learn more about ConfigMaps in the official Kubernetes Documentation .","title":"Client"},{"location":"Client/#client","text":"Client submits a Spark application to run on Kubernetes (by creating the driver pod and starting a watcher that monitors and logs the application status).","title":"Client"},{"location":"Client/#creating-instance","text":"Client takes the following to be created: KubernetesDriverConf KubernetesDriverBuilder Kubernetes' KubernetesClient LoggingPodStatusWatcher Client is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"Client/#running-driver-pod","text":"run () : Unit run requests the KubernetesDriverBuilder to build a KubernetesDriverSpec . run requests the KubernetesDriverConf for the resourceNamePrefix to be used for the name of the driver's config map: [resourceNamePrefix]-driver-conf-map run builds a ConfigMap (with the name and the system properties of the KubernetesDriverSpec ). run creates a driver container (based on the KubernetesDriverSpec ) and adds the following: SPARK_CONF_DIR env var as /opt/spark/conf spark-conf-volume volume mount as /opt/spark/conf run creates a driver pod (based on the KubernetesDriverSpec ) with the driver container and a new spark-conf-volume volume for the ConfigMap . run requests the KubernetesClient to watch for the driver pod (using the LoggingPodStatusWatcher ) and, when available, attaches the ConfigMap . run is used when: KubernetesClientApplication is requested to start","title":" Running Driver Pod"},{"location":"Client/#adddriverownerreference","text":"addDriverOwnerReference ( driverPod : Pod , resources : Seq [ HasMetadata ]) : Unit addDriverOwnerReference ...FIXME","title":" addDriverOwnerReference"},{"location":"Client/#building-configmap","text":"buildConfigMap ( configMapName : String , conf : Map [ String , String ]) : ConfigMap buildConfigMap builds a Kubernetes ConfigMap with the given conf key-value pairs stored as spark.properties data and the given configMapName name. The stored data uses an extra comment: Java properties built from Kubernetes config map with name: [configMapName] Kubernetes Documentation Learn more about ConfigMaps in the official Kubernetes Documentation .","title":" Building ConfigMap"},{"location":"ClientArguments/","text":"ClientArguments \u00b6 ClientArguments represents a KubernetesClientApplication to start. Creating Instance \u00b6 ClientArguments takes the following to be created: MainAppResource Name of the main class of a Spark application to run Driver Arguments ClientArguments is created (via fromCommandLineArgs utility) when: KubernetesClientApplication is requested to start fromCommandLineArgs Utility \u00b6 fromCommandLineArgs ( args : Array [ String ]) : ClientArguments fromCommandLineArgs slices the input args into key-value pairs and creates a ClientArguments as follows: --primary-java-resource , --primary-py-file or --primary-r-file keys are used for the MainAppResource --main-class for the name of the main class --arg for the driver arguments Important The main class must be specified via --main-class or fromCommandLineArgs throws an IllegalArgumentException . fromCommandLineArgs is used when: KubernetesClientApplication is requested to start","title":"ClientArguments"},{"location":"ClientArguments/#clientarguments","text":"ClientArguments represents a KubernetesClientApplication to start.","title":"ClientArguments"},{"location":"ClientArguments/#creating-instance","text":"ClientArguments takes the following to be created: MainAppResource Name of the main class of a Spark application to run Driver Arguments ClientArguments is created (via fromCommandLineArgs utility) when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"ClientArguments/#fromcommandlineargs-utility","text":"fromCommandLineArgs ( args : Array [ String ]) : ClientArguments fromCommandLineArgs slices the input args into key-value pairs and creates a ClientArguments as follows: --primary-java-resource , --primary-py-file or --primary-r-file keys are used for the MainAppResource --main-class for the name of the main class --arg for the driver arguments Important The main class must be specified via --main-class or fromCommandLineArgs throws an IllegalArgumentException . fromCommandLineArgs is used when: KubernetesClientApplication is requested to start","title":" fromCommandLineArgs Utility"},{"location":"DriverCommandFeatureStep/","text":"DriverCommandFeatureStep \u00b6 DriverCommandFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 DriverCommandFeatureStep takes the following to be created: KubernetesDriverConf DriverCommandFeatureStep is created when: KubernetesDriverBuilder is requested to build a KubernetesDriverSpec from features Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod branches off based on the MainAppResource (of the KubernetesDriverConf ): For JavaMainAppResource , configurePod configures a pod for Java with the primary resource (if defined) or uses spark-internal special value For PythonMainAppResource , configurePod configures a pod for Python with the primary resource For RMainAppResource , configurePod configures a pod for R with the primary resource Configuring Pod for Java Application \u00b6 configureForJava ( pod : SparkPod , res : String ) : SparkPod configureForJava builds the base driver container for the given SparkPod and the primary resource. In the end, configureForJava creates another SparkPod (for the pod of the given SparkPod ) and the driver container. Configuring Pod for Python Application \u00b6 configureForPython ( pod : SparkPod , res : String ) : SparkPod configureForPython ...FIXME Configuring Pod for R Application \u00b6 configureForR ( pod : SparkPod , res : String ) : SparkPod configureForR ...FIXME Base Driver ContainerBuilder \u00b6 baseDriverContainer ( pod : SparkPod , resource : String ) : ContainerBuilder baseDriverContainer renames the given primary resource if the MainAppResource is a JavaMainAppResource . Otherwise, baseDriverContainer leaves the primary resource as-is. baseDriverContainer creates a ContainerBuilder (for the pod of the given SparkPod ) and adds the following arguments (in that order): driver --properties-file with /opt/spark/conf/spark.properties --class with the mainClass of the KubernetesDriverConf the primary resource (possibly renamed if a MainAppResource ) appArgs of the KubernetesDriverConf Note The arguments are then used by the default entrypoint.sh of the official Docker image of Apache Spark (in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/ ). Use the following kubectl command to see the arguments: kubectl get po [driverPod] -o=jsonpath='{.spec.containers[0].args}'","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#drivercommandfeaturestep","text":"DriverCommandFeatureStep is a KubernetesFeatureConfigStep .","title":"DriverCommandFeatureStep"},{"location":"DriverCommandFeatureStep/#creating-instance","text":"DriverCommandFeatureStep takes the following to be created: KubernetesDriverConf DriverCommandFeatureStep is created when: KubernetesDriverBuilder is requested to build a KubernetesDriverSpec from features","title":"Creating Instance"},{"location":"DriverCommandFeatureStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod branches off based on the MainAppResource (of the KubernetesDriverConf ): For JavaMainAppResource , configurePod configures a pod for Java with the primary resource (if defined) or uses spark-internal special value For PythonMainAppResource , configurePod configures a pod for Python with the primary resource For RMainAppResource , configurePod configures a pod for R with the primary resource","title":" Configuring Pod"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-java-application","text":"configureForJava ( pod : SparkPod , res : String ) : SparkPod configureForJava builds the base driver container for the given SparkPod and the primary resource. In the end, configureForJava creates another SparkPod (for the pod of the given SparkPod ) and the driver container.","title":" Configuring Pod for Java Application"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-python-application","text":"configureForPython ( pod : SparkPod , res : String ) : SparkPod configureForPython ...FIXME","title":" Configuring Pod for Python Application"},{"location":"DriverCommandFeatureStep/#configuring-pod-for-r-application","text":"configureForR ( pod : SparkPod , res : String ) : SparkPod configureForR ...FIXME","title":" Configuring Pod for R Application"},{"location":"DriverCommandFeatureStep/#base-driver-containerbuilder","text":"baseDriverContainer ( pod : SparkPod , resource : String ) : ContainerBuilder baseDriverContainer renames the given primary resource if the MainAppResource is a JavaMainAppResource . Otherwise, baseDriverContainer leaves the primary resource as-is. baseDriverContainer creates a ContainerBuilder (for the pod of the given SparkPod ) and adds the following arguments (in that order): driver --properties-file with /opt/spark/conf/spark.properties --class with the mainClass of the KubernetesDriverConf the primary resource (possibly renamed if a MainAppResource ) appArgs of the KubernetesDriverConf Note The arguments are then used by the default entrypoint.sh of the official Docker image of Apache Spark (in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/ ). Use the following kubectl command to see the arguments: kubectl get po [driverPod] -o=jsonpath='{.spec.containers[0].args}'","title":" Base Driver ContainerBuilder"},{"location":"DriverServiceFeatureStep/","text":"DriverServiceFeatureStep \u00b6 DriverServiceFeatureStep is...FIXME","title":"DriverServiceFeatureStep"},{"location":"DriverServiceFeatureStep/#driverservicefeaturestep","text":"DriverServiceFeatureStep is...FIXME","title":"DriverServiceFeatureStep"},{"location":"EnvSecretsFeatureStep/","text":"EnvSecretsFeatureStep \u00b6 EnvSecretsFeatureStep is...FIXME","title":"EnvSecretsFeatureStep"},{"location":"EnvSecretsFeatureStep/#envsecretsfeaturestep","text":"EnvSecretsFeatureStep is...FIXME","title":"EnvSecretsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/","text":"ExecutorKubernetesCredentialsFeatureStep \u00b6 ExecutorKubernetesCredentialsFeatureStep is...FIXME","title":"ExecutorKubernetesCredentialsFeatureStep"},{"location":"ExecutorKubernetesCredentialsFeatureStep/#executorkubernetescredentialsfeaturestep","text":"ExecutorKubernetesCredentialsFeatureStep is...FIXME","title":"ExecutorKubernetesCredentialsFeatureStep"},{"location":"ExecutorPodsAllocator/","text":"ExecutorPodsAllocator \u00b6 ExecutorPodsAllocator is responsible for executor pod allocation (possibly dynamic ) in a Spark application. ExecutorPodsAllocator is used to create a KubernetesClusterSchedulerBackend . Creating Instance \u00b6 ExecutorPodsAllocator takes the following to be created: SparkConf SecurityManager KubernetesExecutorBuilder KubernetesClient ExecutorPodsSnapshotsStore Clock ExecutorPodsAllocator is created when: KubernetesClusterManager is requested for a SchedulerBackend spark.dynamicAllocation.enabled \u00b6 ExecutorPodsAllocator uses spark.dynamicAllocation.enabled configuration property to turn dynamic allocation of executors on and off. The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark . spark.kubernetes.allocation.batch.size \u00b6 ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.size configuration property in the following: onNewSnapshots spark.kubernetes.allocation.batch.delay \u00b6 ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.delay configuration property for the following: podCreationTimeout Registering a subscriber spark.kubernetes.executor.deleteOnTermination \u00b6 ExecutorPodsAllocator uses spark.kubernetes.executor.deleteOnTermination configuration property. Starting \u00b6 start ( applicationId : String ) : Unit start requests the ExecutorPodsSnapshotsStore to subscribe this ExecutorPodsAllocator to be notified about new snapshots (with pod allocation delay based on spark.kubernetes.allocation.batch.delay configuration property). start is used when: KubernetesClusterSchedulerBackend is requested to start Processing Executor Pods Snapshots \u00b6 onNewSnapshots ( applicationId : String , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots removes the executor IDs (of the executor pods in the given snapshots) from the newlyCreatedExecutors internal registry. onNewSnapshots finds timed-out executor IDs (in the newlyCreatedExecutors internal registry) whose creation time exceeded some podCreationTimeout threshold. For the other executor IDs, onNewSnapshots prints out the following DEBUG message to the logs: Executor with id [execId] was not found in the Kubernetes cluster since it was created [time] milliseconds ago. For any timed-out executor IDs, onNewSnapshots prints out the following WARN message to the logs: Executors with ids [ids] were not detected in the Kubernetes cluster after [podCreationTimeout] ms despite the fact that a previous allocation attempt tried to create them. The executors may have been deleted but the application missed the deletion event. onNewSnapshots removes ( forgets ) the timed-out executor IDs (from the newlyCreatedExecutors internal registry). With the shouldDeleteExecutors flag enabled, onNewSnapshots requests the KubernetesClient to delete pods with the following labels: spark-app-selector with the given applicationId spark-role = executor spark-exec-id for all timed-out executor IDs onNewSnapshots updates the lastSnapshot internal registry with the last ExecutorPodsSnapshot among the given snapshots if available. onNewSnapshots counts running executor pods in the lastSnapshot internal registry. onNewSnapshots finds pending executor IDs in the lastSnapshot internal registry. For non-empty input snapshots , onNewSnapshots prints out the following DEBUG message to the logs: Pod allocation status: [currentRunningCount] running, [currentPendingExecutors] pending, [newlyCreatedExecutors] unacknowledged. onNewSnapshots ...FIXME In the end, with DEBUG logging enabled or the input snapshots is empty, onNewSnapshots prints out the following DEBUG messages. With the number of the executor pods currently running higher than the total expected executors but no dynamicAllocationEnabled , onNewSnapshots prints out the following: Current number of running executors is equal to the number of requested executors. Not scaling up further. Otherwise, when there are executor pods pending ( outstanding ), onNewSnapshots prints out the following: Still waiting for [outstanding] executors before requesting more. Total Expected Executors \u00b6 totalExpectedExecutors : AtomicInteger ExecutorPodsAllocator uses a Java AtomicInteger to track the total expected number of executors. Starts from 0 and is set to a fixed number of the total expected executors in setTotalExpectedExecutors Used in onNewSnapshots Changing Total Expected Executors \u00b6 setTotalExpectedExecutors ( total : Int ) : Unit setTotalExpectedExecutors sets totalExpectedExecutors internal registry to the input total . With no hasPendingPods , setTotalExpectedExecutors requests the ExecutorPodsSnapshotsStore to notifySubscribers . setTotalExpectedExecutors is used when: KubernetesClusterSchedulerBackend is requested to start and doRequestTotalExecutors Registries \u00b6 newlyCreatedExecutors \u00b6 newlyCreatedExecutors : Map [ Long , Long ] ExecutorPodsAllocator uses newlyCreatedExecutors internal registry to track executor IDs (with the timestamps they were created) that have been requested from Kubernetes but have not been detected in any snapshot yet. Used in onNewSnapshots EXECUTOR_ID_COUNTER \u00b6 ExecutorPodsAllocator uses a Java AtomicLong for the missing executor IDs that are going to be requested (in onNewSnapshots ) when the number of running executor pods is below the total expected executors . hasPendingPods Flag \u00b6 hasPendingPods : AtomicBoolean ExecutorPodsAllocator uses a Java AtomicBoolean as a flag to avoid notifying subscribers. Starts as false and is updated every onNewSnapshots Used in setTotalExpectedExecutors (only when false ) Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL Refer to Logging .","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#executorpodsallocator","text":"ExecutorPodsAllocator is responsible for executor pod allocation (possibly dynamic ) in a Spark application. ExecutorPodsAllocator is used to create a KubernetesClusterSchedulerBackend .","title":"ExecutorPodsAllocator"},{"location":"ExecutorPodsAllocator/#creating-instance","text":"ExecutorPodsAllocator takes the following to be created: SparkConf SecurityManager KubernetesExecutorBuilder KubernetesClient ExecutorPodsSnapshotsStore Clock ExecutorPodsAllocator is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsAllocator/#sparkdynamicallocationenabled","text":"ExecutorPodsAllocator uses spark.dynamicAllocation.enabled configuration property to turn dynamic allocation of executors on and off. The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark .","title":" spark.dynamicAllocation.enabled"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchsize","text":"ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.size configuration property in the following: onNewSnapshots","title":" spark.kubernetes.allocation.batch.size"},{"location":"ExecutorPodsAllocator/#sparkkubernetesallocationbatchdelay","text":"ExecutorPodsAllocator uses spark.kubernetes.allocation.batch.delay configuration property for the following: podCreationTimeout Registering a subscriber","title":" spark.kubernetes.allocation.batch.delay"},{"location":"ExecutorPodsAllocator/#sparkkubernetesexecutordeleteontermination","text":"ExecutorPodsAllocator uses spark.kubernetes.executor.deleteOnTermination configuration property.","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"ExecutorPodsAllocator/#starting","text":"start ( applicationId : String ) : Unit start requests the ExecutorPodsSnapshotsStore to subscribe this ExecutorPodsAllocator to be notified about new snapshots (with pod allocation delay based on spark.kubernetes.allocation.batch.delay configuration property). start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsAllocator/#processing-executor-pods-snapshots","text":"onNewSnapshots ( applicationId : String , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots removes the executor IDs (of the executor pods in the given snapshots) from the newlyCreatedExecutors internal registry. onNewSnapshots finds timed-out executor IDs (in the newlyCreatedExecutors internal registry) whose creation time exceeded some podCreationTimeout threshold. For the other executor IDs, onNewSnapshots prints out the following DEBUG message to the logs: Executor with id [execId] was not found in the Kubernetes cluster since it was created [time] milliseconds ago. For any timed-out executor IDs, onNewSnapshots prints out the following WARN message to the logs: Executors with ids [ids] were not detected in the Kubernetes cluster after [podCreationTimeout] ms despite the fact that a previous allocation attempt tried to create them. The executors may have been deleted but the application missed the deletion event. onNewSnapshots removes ( forgets ) the timed-out executor IDs (from the newlyCreatedExecutors internal registry). With the shouldDeleteExecutors flag enabled, onNewSnapshots requests the KubernetesClient to delete pods with the following labels: spark-app-selector with the given applicationId spark-role = executor spark-exec-id for all timed-out executor IDs onNewSnapshots updates the lastSnapshot internal registry with the last ExecutorPodsSnapshot among the given snapshots if available. onNewSnapshots counts running executor pods in the lastSnapshot internal registry. onNewSnapshots finds pending executor IDs in the lastSnapshot internal registry. For non-empty input snapshots , onNewSnapshots prints out the following DEBUG message to the logs: Pod allocation status: [currentRunningCount] running, [currentPendingExecutors] pending, [newlyCreatedExecutors] unacknowledged. onNewSnapshots ...FIXME In the end, with DEBUG logging enabled or the input snapshots is empty, onNewSnapshots prints out the following DEBUG messages. With the number of the executor pods currently running higher than the total expected executors but no dynamicAllocationEnabled , onNewSnapshots prints out the following: Current number of running executors is equal to the number of requested executors. Not scaling up further. Otherwise, when there are executor pods pending ( outstanding ), onNewSnapshots prints out the following: Still waiting for [outstanding] executors before requesting more.","title":" Processing Executor Pods Snapshots"},{"location":"ExecutorPodsAllocator/#total-expected-executors","text":"totalExpectedExecutors : AtomicInteger ExecutorPodsAllocator uses a Java AtomicInteger to track the total expected number of executors. Starts from 0 and is set to a fixed number of the total expected executors in setTotalExpectedExecutors Used in onNewSnapshots","title":" Total Expected Executors"},{"location":"ExecutorPodsAllocator/#changing-total-expected-executors","text":"setTotalExpectedExecutors ( total : Int ) : Unit setTotalExpectedExecutors sets totalExpectedExecutors internal registry to the input total . With no hasPendingPods , setTotalExpectedExecutors requests the ExecutorPodsSnapshotsStore to notifySubscribers . setTotalExpectedExecutors is used when: KubernetesClusterSchedulerBackend is requested to start and doRequestTotalExecutors","title":" Changing Total Expected Executors"},{"location":"ExecutorPodsAllocator/#registries","text":"","title":"Registries"},{"location":"ExecutorPodsAllocator/#newlycreatedexecutors","text":"newlyCreatedExecutors : Map [ Long , Long ] ExecutorPodsAllocator uses newlyCreatedExecutors internal registry to track executor IDs (with the timestamps they were created) that have been requested from Kubernetes but have not been detected in any snapshot yet. Used in onNewSnapshots","title":" newlyCreatedExecutors"},{"location":"ExecutorPodsAllocator/#executor_id_counter","text":"ExecutorPodsAllocator uses a Java AtomicLong for the missing executor IDs that are going to be requested (in onNewSnapshots ) when the number of running executor pods is below the total expected executors .","title":" EXECUTOR_ID_COUNTER"},{"location":"ExecutorPodsAllocator/#haspendingpods-flag","text":"hasPendingPods : AtomicBoolean ExecutorPodsAllocator uses a Java AtomicBoolean as a flag to avoid notifying subscribers. Starts as false and is updated every onNewSnapshots Used in setTotalExpectedExecutors (only when false )","title":" hasPendingPods Flag"},{"location":"ExecutorPodsAllocator/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsLifecycleManager/","text":"ExecutorPodsLifecycleManager \u00b6 Creating Instance \u00b6 ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ). Configuration Properties \u00b6 spark.kubernetes.executor.eventProcessingInterval \u00b6 ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME spark.kubernetes.executor.deleteOnTermination \u00b6 ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState . Starting \u00b6 start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when KubernetesClusterSchedulerBackend is started . Handling State Changes in Executor Pods \u00b6 onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots creates an empty execIdsRemovedInThisRound collection of executors to be removed. onNewSnapshots walks over the input ExecutorPodsSnapshot s and branches off based on ExecutorPodState : For PodDeleted , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots removeExecutorFromSpark and adds the executor ID to the execIdsRemovedInThisRound local collection. For PodFailed , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. For PodSucceeded , onNewSnapshots requests the input KubernetesClusterSchedulerBackend to isExecutorActive . If so, onNewSnapshots prints out the following INFO message to the logs: Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed. Otherwise, onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName]. onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. onFinalNonDeletedState \u00b6 onFinalNonDeletedState ( podState : FinalPodState , execId : Long , schedulerBackend : KubernetesClusterSchedulerBackend , execIdsRemovedInRound : mutable.Set [ Long ]) : Unit onFinalNonDeletedState removeExecutorFromSpark . With spark.kubernetes.executor.deleteOnTermination configuration property enabled, onFinalNonDeletedState removeExecutorFromK8s . In the end, onFinalNonDeletedState adds the given execId to the given execIdsRemovedInRound collection. removeExecutorFromSpark \u00b6 removeExecutorFromSpark ( schedulerBackend : KubernetesClusterSchedulerBackend , podState : FinalPodState , execId : Long ) : Unit removeExecutorFromSpark ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL Refer to Logging .","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#executorpodslifecyclemanager","text":"","title":"ExecutorPodsLifecycleManager"},{"location":"ExecutorPodsLifecycleManager/#creating-instance","text":"ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ).","title":"Creating Instance"},{"location":"ExecutorPodsLifecycleManager/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutoreventprocessinginterval","text":"ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.eventProcessingInterval configuration property when started to register a new subscriber for how often to...FIXME","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"ExecutorPodsLifecycleManager/#sparkkubernetesexecutordeleteontermination","text":"ExecutorPodsLifecycleManager uses the spark.kubernetes.executor.deleteOnTermination configuration property for onFinalNonDeletedState .","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"ExecutorPodsLifecycleManager/#starting","text":"start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when KubernetesClusterSchedulerBackend is started .","title":" Starting"},{"location":"ExecutorPodsLifecycleManager/#handling-state-changes-in-executor-pods","text":"onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots creates an empty execIdsRemovedInThisRound collection of executors to be removed. onNewSnapshots walks over the input ExecutorPodsSnapshot s and branches off based on ExecutorPodState : For PodDeleted , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported deleted executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots removeExecutorFromSpark and adds the executor ID to the execIdsRemovedInThisRound local collection. For PodFailed , onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported failed executor with id [execId], pod name [state.pod.getMetadata.getName] onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection. For PodSucceeded , onNewSnapshots requests the input KubernetesClusterSchedulerBackend to isExecutorActive . If so, onNewSnapshots prints out the following INFO message to the logs: Snapshot reported succeeded executor with id [execId], even though the application has not requested for it to be removed. Otherwise, onNewSnapshots prints out the following DEBUG message to the logs: Snapshot reported succeeded executor with id [execId], pod name [state.pod.getMetadata.getName]. onNewSnapshots onFinalNonDeletedState with the execIdsRemovedInThisRound local collection.","title":" Handling State Changes in Executor Pods"},{"location":"ExecutorPodsLifecycleManager/#onfinalnondeletedstate","text":"onFinalNonDeletedState ( podState : FinalPodState , execId : Long , schedulerBackend : KubernetesClusterSchedulerBackend , execIdsRemovedInRound : mutable.Set [ Long ]) : Unit onFinalNonDeletedState removeExecutorFromSpark . With spark.kubernetes.executor.deleteOnTermination configuration property enabled, onFinalNonDeletedState removeExecutorFromK8s . In the end, onFinalNonDeletedState adds the given execId to the given execIdsRemovedInRound collection.","title":" onFinalNonDeletedState"},{"location":"ExecutorPodsLifecycleManager/#removeexecutorfromspark","text":"removeExecutorFromSpark ( schedulerBackend : KubernetesClusterSchedulerBackend , podState : FinalPodState , execId : Long ) : Unit removeExecutorFromSpark ...FIXME","title":" removeExecutorFromSpark"},{"location":"ExecutorPodsLifecycleManager/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsLifecycleManager=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsPollingSnapshotSource/","text":"ExecutorPodsPollingSnapshotSource \u00b6 ExecutorPodsPollingSnapshotSource manages pollingFuture (with a PollRunnable ) to synchronize executor pods state every pollingInterval (and requests the ExecutorPodsSnapshotsStore to replaceSnapshot ). Creating Instance \u00b6 ExecutorPodsPollingSnapshotSource takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Java ScheduledExecutorService ExecutorPodsPollingSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend ) spark.kubernetes.executor.apiPollingInterval \u00b6 ExecutorPodsPollingSnapshotSource uses spark.kubernetes.executor.apiPollingInterval configuration property when started (to schedule a PollRunnable for regular executor pod state synchronization). pollingFuture \u00b6 pollingFuture : Future [ _ ] pollingFuture ...FIXME Starting \u00b6 start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting to check for executor pod state every [pollingInterval] ms. start throws an IllegalArgumentException when started twice (i.e. pollingFuture has already been initialized): Cannot start polling more than once. start is used when: KubernetesClusterSchedulerBackend is requested to start Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL Refer to Logging .","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#executorpodspollingsnapshotsource","text":"ExecutorPodsPollingSnapshotSource manages pollingFuture (with a PollRunnable ) to synchronize executor pods state every pollingInterval (and requests the ExecutorPodsSnapshotsStore to replaceSnapshot ).","title":"ExecutorPodsPollingSnapshotSource"},{"location":"ExecutorPodsPollingSnapshotSource/#creating-instance","text":"ExecutorPodsPollingSnapshotSource takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Java ScheduledExecutorService ExecutorPodsPollingSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend (and creates a KubernetesClusterSchedulerBackend )","title":"Creating Instance"},{"location":"ExecutorPodsPollingSnapshotSource/#sparkkubernetesexecutorapipollinginterval","text":"ExecutorPodsPollingSnapshotSource uses spark.kubernetes.executor.apiPollingInterval configuration property when started (to schedule a PollRunnable for regular executor pod state synchronization).","title":" spark.kubernetes.executor.apiPollingInterval"},{"location":"ExecutorPodsPollingSnapshotSource/#pollingfuture","text":"pollingFuture : Future [ _ ] pollingFuture ...FIXME","title":" pollingFuture"},{"location":"ExecutorPodsPollingSnapshotSource/#starting","text":"start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting to check for executor pod state every [pollingInterval] ms. start throws an IllegalArgumentException when started twice (i.e. pollingFuture has already been initialized): Cannot start polling more than once. start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsPollingSnapshotSource/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsSnapshotsStore/","text":"ExecutorPodsSnapshotsStore \u00b6 ExecutorPodsSnapshotsStore is an abstraction of pod stores . Contract \u00b6 addSubscriber \u00b6 addSubscriber ( processBatchIntervalMillis : Long )( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit Used when: ExecutorPodsAllocator is requested to start ExecutorPodsLifecycleManager is requested to start notifySubscribers \u00b6 notifySubscribers () : Unit Used when: ExecutorPodsAllocator is requested to setTotalExpectedExecutors replaceSnapshot \u00b6 replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit Used when: PollRunnable is requested to start stop \u00b6 stop () : Unit Used when: KubernetesClusterSchedulerBackend is requested to stop updatePod \u00b6 updatePod ( updatedPod : Pod ) : Unit Used when: ExecutorPodsWatcher is requested to eventReceived Implementations \u00b6 ExecutorPodsSnapshotsStoreImpl","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#executorpodssnapshotsstore","text":"ExecutorPodsSnapshotsStore is an abstraction of pod stores .","title":"ExecutorPodsSnapshotsStore"},{"location":"ExecutorPodsSnapshotsStore/#contract","text":"","title":"Contract"},{"location":"ExecutorPodsSnapshotsStore/#addsubscriber","text":"addSubscriber ( processBatchIntervalMillis : Long )( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit Used when: ExecutorPodsAllocator is requested to start ExecutorPodsLifecycleManager is requested to start","title":" addSubscriber"},{"location":"ExecutorPodsSnapshotsStore/#notifysubscribers","text":"notifySubscribers () : Unit Used when: ExecutorPodsAllocator is requested to setTotalExpectedExecutors","title":" notifySubscribers"},{"location":"ExecutorPodsSnapshotsStore/#replacesnapshot","text":"replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit Used when: PollRunnable is requested to start","title":" replaceSnapshot"},{"location":"ExecutorPodsSnapshotsStore/#stop","text":"stop () : Unit Used when: KubernetesClusterSchedulerBackend is requested to stop","title":" stop"},{"location":"ExecutorPodsSnapshotsStore/#updatepod","text":"updatePod ( updatedPod : Pod ) : Unit Used when: ExecutorPodsWatcher is requested to eventReceived","title":" updatePod"},{"location":"ExecutorPodsSnapshotsStore/#implementations","text":"ExecutorPodsSnapshotsStoreImpl","title":"Implementations"},{"location":"ExecutorPodsSnapshotsStoreImpl/","text":"ExecutorPodsSnapshotsStoreImpl \u00b6 ExecutorPodsSnapshotsStoreImpl is an ExecutorPodsSnapshotsStore . Creating Instance \u00b6 ExecutorPodsSnapshotsStoreImpl takes the following to be created: Java's ScheduledExecutorService ExecutorPodsSnapshotsStoreImpl is created when: KubernetesClusterManager is requested for a SchedulerBackend replaceSnapshot \u00b6 replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit replaceSnapshot is part of the ExecutorPodsSnapshotsStore abstraction. replaceSnapshot ...FIXME updatePod \u00b6 updatePod ( updatedPod : Pod ) : Unit updatePod is part of the ExecutorPodsSnapshotsStore abstraction. updatePod ...FIXME Registering Subscriber \u00b6 addSubscriber ( processBatchIntervalMillis : Long ) ( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit addSubscriber is part of the ExecutorPodsSnapshotsStore abstraction. addSubscriber adds a new SnapshotsSubscriber to the subscribers internal registry. addSubscriber requests the ScheduledExecutorService to schedule processing executor pods by the SnapshotsSubscriber every given processBatchIntervalMillis delay (starting immediately). In the end, addSubscriber adds the scheduled action to the pollingTasks internal registry. callSubscriber \u00b6 callSubscriber ( subscriber : SnapshotsSubscriber ) : Unit callSubscriber ...FIXME callSubscriber is used when: ExecutorPodsSnapshotsStoreImpl is requested to addSubscriber and notifySubscribers pollingTasks Registry \u00b6 pollingTasks : CopyOnWriteArrayList [ Future [ _ ]] ExecutorPodsSnapshotsStoreImpl uses pollingTasks internal registry to track the recurring actions scheduled for subscribers . pollingTasks are cancelled when ExecutorPodsSnapshotsStoreImpl is requested to stop . subscribers Registry \u00b6 subscribers : CopyOnWriteArrayList [ SnapshotsSubscriber ] ExecutorPodsSnapshotsStoreImpl uses subscribers internal registry to track subscribers that want to be notified regularly about the current state of executor pods in a cluster. addCurrentSnapshotToSubscribers \u00b6 addCurrentSnapshotToSubscribers () : Unit addCurrentSnapshotToSubscribers takes the subscribers and...FIXME addCurrentSnapshotToSubscribers is used when: ExecutorPodsSnapshotsStoreImpl is requested to updatePod and replaceSnapshot","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#executorpodssnapshotsstoreimpl","text":"ExecutorPodsSnapshotsStoreImpl is an ExecutorPodsSnapshotsStore .","title":"ExecutorPodsSnapshotsStoreImpl"},{"location":"ExecutorPodsSnapshotsStoreImpl/#creating-instance","text":"ExecutorPodsSnapshotsStoreImpl takes the following to be created: Java's ScheduledExecutorService ExecutorPodsSnapshotsStoreImpl is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsSnapshotsStoreImpl/#replacesnapshot","text":"replaceSnapshot ( newSnapshot : Seq [ Pod ]) : Unit replaceSnapshot is part of the ExecutorPodsSnapshotsStore abstraction. replaceSnapshot ...FIXME","title":" replaceSnapshot"},{"location":"ExecutorPodsSnapshotsStoreImpl/#updatepod","text":"updatePod ( updatedPod : Pod ) : Unit updatePod is part of the ExecutorPodsSnapshotsStore abstraction. updatePod ...FIXME","title":" updatePod"},{"location":"ExecutorPodsSnapshotsStoreImpl/#registering-subscriber","text":"addSubscriber ( processBatchIntervalMillis : Long ) ( onNewSnapshots : Seq [ ExecutorPodsSnapshot ] => Unit ) : Unit addSubscriber is part of the ExecutorPodsSnapshotsStore abstraction. addSubscriber adds a new SnapshotsSubscriber to the subscribers internal registry. addSubscriber requests the ScheduledExecutorService to schedule processing executor pods by the SnapshotsSubscriber every given processBatchIntervalMillis delay (starting immediately). In the end, addSubscriber adds the scheduled action to the pollingTasks internal registry.","title":" Registering Subscriber"},{"location":"ExecutorPodsSnapshotsStoreImpl/#callsubscriber","text":"callSubscriber ( subscriber : SnapshotsSubscriber ) : Unit callSubscriber ...FIXME callSubscriber is used when: ExecutorPodsSnapshotsStoreImpl is requested to addSubscriber and notifySubscribers","title":" callSubscriber"},{"location":"ExecutorPodsSnapshotsStoreImpl/#pollingtasks-registry","text":"pollingTasks : CopyOnWriteArrayList [ Future [ _ ]] ExecutorPodsSnapshotsStoreImpl uses pollingTasks internal registry to track the recurring actions scheduled for subscribers . pollingTasks are cancelled when ExecutorPodsSnapshotsStoreImpl is requested to stop .","title":" pollingTasks Registry"},{"location":"ExecutorPodsSnapshotsStoreImpl/#subscribers-registry","text":"subscribers : CopyOnWriteArrayList [ SnapshotsSubscriber ] ExecutorPodsSnapshotsStoreImpl uses subscribers internal registry to track subscribers that want to be notified regularly about the current state of executor pods in a cluster.","title":" subscribers Registry"},{"location":"ExecutorPodsSnapshotsStoreImpl/#addcurrentsnapshottosubscribers","text":"addCurrentSnapshotToSubscribers () : Unit addCurrentSnapshotToSubscribers takes the subscribers and...FIXME addCurrentSnapshotToSubscribers is used when: ExecutorPodsSnapshotsStoreImpl is requested to updatePod and replaceSnapshot","title":" addCurrentSnapshotToSubscribers"},{"location":"ExecutorPodsWatchSnapshotSource/","text":"ExecutorPodsWatchSnapshotSource \u00b6 Creating Instance \u00b6 ExecutorPodsWatchSnapshotSource takes the following to be created: ExecutorPodsSnapshotsStore KubernetesClient ExecutorPodsWatchSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend Starting \u00b6 start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor. start requests the KubernetesClient to watch pods with the following labels using ExecutorPodsWatcher : spark-app-selector with the given applicationId spark-role as executor start is used when: KubernetesClusterSchedulerBackend is requested to start Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL Refer to Logging .","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#executorpodswatchsnapshotsource","text":"","title":"ExecutorPodsWatchSnapshotSource"},{"location":"ExecutorPodsWatchSnapshotSource/#creating-instance","text":"ExecutorPodsWatchSnapshotSource takes the following to be created: ExecutorPodsSnapshotsStore KubernetesClient ExecutorPodsWatchSnapshotSource is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"ExecutorPodsWatchSnapshotSource/#starting","text":"start ( applicationId : String ) : Unit start prints out the following DEBUG message to the logs: Starting watch for pods with labels spark-app-selector=[applicationId], spark-role=executor. start requests the KubernetesClient to watch pods with the following labels using ExecutorPodsWatcher : spark-app-selector with the given applicationId spark-role as executor start is used when: KubernetesClusterSchedulerBackend is requested to start","title":" Starting"},{"location":"ExecutorPodsWatchSnapshotSource/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource=ALL Refer to Logging .","title":"Logging"},{"location":"ExecutorPodsWatcher/","text":"ExecutorPodsWatcher \u00b6 ExecutorPodsWatcher is a Kubernetes' Watcher[Pod] to be notified about executor pod updates (and update the ExecutorPodsSnapshotsStore of the parent ExecutorPodsWatchSnapshotSource ). ExecutorPodsWatcher is an internal class of ExecutorPodsWatchSnapshotSource with full access to its internals. Creating Instance \u00b6 ExecutorPodsWatcher takes no arguments to be created. ExecutorPodsWatcher is created when: Executor Pod Update \u00b6 eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes Client Watcher abstraction. eventReceived prints out the following DEBUG message to the logs: Received executor pod update for pod named [podName], action [action] eventReceived requests the ExecutorPodsSnapshotsStore (of the parent ExecutorPodsWatchSnapshotSource ) to updatePod . Close Notification \u00b6 onClose ( e : KubernetesClientException ) : Unit onClose is part of the Kubernetes Client Watcher abstraction. onClose prints out the following WARN message to the logs: Kubernetes client has been closed (this is expected if the application is shutting down.) Logging \u00b6 ExecutorPodsWatcher uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger for logging.","title":"ExecutorPodsWatcher"},{"location":"ExecutorPodsWatcher/#executorpodswatcher","text":"ExecutorPodsWatcher is a Kubernetes' Watcher[Pod] to be notified about executor pod updates (and update the ExecutorPodsSnapshotsStore of the parent ExecutorPodsWatchSnapshotSource ). ExecutorPodsWatcher is an internal class of ExecutorPodsWatchSnapshotSource with full access to its internals.","title":"ExecutorPodsWatcher"},{"location":"ExecutorPodsWatcher/#creating-instance","text":"ExecutorPodsWatcher takes no arguments to be created. ExecutorPodsWatcher is created when:","title":"Creating Instance"},{"location":"ExecutorPodsWatcher/#executor-pod-update","text":"eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes Client Watcher abstraction. eventReceived prints out the following DEBUG message to the logs: Received executor pod update for pod named [podName], action [action] eventReceived requests the ExecutorPodsSnapshotsStore (of the parent ExecutorPodsWatchSnapshotSource ) to updatePod .","title":" Executor Pod Update"},{"location":"ExecutorPodsWatcher/#close-notification","text":"onClose ( e : KubernetesClientException ) : Unit onClose is part of the Kubernetes Client Watcher abstraction. onClose prints out the following WARN message to the logs: Kubernetes client has been closed (this is expected if the application is shutting down.)","title":" Close Notification"},{"location":"ExecutorPodsWatcher/#logging","text":"ExecutorPodsWatcher uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsWatchSnapshotSource logger for logging.","title":"Logging"},{"location":"K8SSparkSubmitOperation/","text":"K8SSparkSubmitOperation \u00b6 K8SSparkSubmitOperation is a SparkSubmitOperation ( Apache Spark ). Killing Submission \u00b6 kill ( submissionId : String , conf : SparkConf ) : Unit kill is part of the SparkSubmitOperation ( Apache Spark ) abstraction. kill prints out the following message to standard error: Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set]. kill creates a KillApplication to execute it (with the input submissionId and SparkConf ). Displaying Submission Status \u00b6 printSubmissionStatus ( submissionId : String , conf : SparkConf ) : Unit printSubmissionStatus is part of the SparkSubmitOperation ( Apache Spark ) abstraction. printSubmissionStatus prints out the following message to standard error: Submitting a request for the status of submission [submissionId] in [spark.master]. printSubmissionStatus creates a ListStatus to execute it (with the input submissionId and SparkConf ). Checking Whether Master URL Supported \u00b6 supports ( master : String ) : Boolean supports is part of the SparkSubmitOperation ( Apache Spark ) abstraction. supports is true when the input master starts with k8s:// prefix. Executing Operation \u00b6 execute ( submissionId : String , sparkConf : SparkConf , op : K8sSubmitOp ) : Unit execute ...FIXME execute is used for kill and printSubmissionStatus .","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#k8ssparksubmitoperation","text":"K8SSparkSubmitOperation is a SparkSubmitOperation ( Apache Spark ).","title":"K8SSparkSubmitOperation"},{"location":"K8SSparkSubmitOperation/#killing-submission","text":"kill ( submissionId : String , conf : SparkConf ) : Unit kill is part of the SparkSubmitOperation ( Apache Spark ) abstraction. kill prints out the following message to standard error: Submitting a request to kill submission [submissionId] in [spark.master]. Grace period in secs: [[getGracePeriod] | not set]. kill creates a KillApplication to execute it (with the input submissionId and SparkConf ).","title":" Killing Submission"},{"location":"K8SSparkSubmitOperation/#displaying-submission-status","text":"printSubmissionStatus ( submissionId : String , conf : SparkConf ) : Unit printSubmissionStatus is part of the SparkSubmitOperation ( Apache Spark ) abstraction. printSubmissionStatus prints out the following message to standard error: Submitting a request for the status of submission [submissionId] in [spark.master]. printSubmissionStatus creates a ListStatus to execute it (with the input submissionId and SparkConf ).","title":" Displaying Submission Status"},{"location":"K8SSparkSubmitOperation/#checking-whether-master-url-supported","text":"supports ( master : String ) : Boolean supports is part of the SparkSubmitOperation ( Apache Spark ) abstraction. supports is true when the input master starts with k8s:// prefix.","title":" Checking Whether Master URL Supported"},{"location":"K8SSparkSubmitOperation/#executing-operation","text":"execute ( submissionId : String , sparkConf : SparkConf , op : K8sSubmitOp ) : Unit execute ...FIXME execute is used for kill and printSubmissionStatus .","title":" Executing Operation"},{"location":"KubernetesClientApplication/","text":"KubernetesClientApplication \u00b6 KubernetesClientApplication is a SparkApplication ( Apache Spark ) in Spark on Kubernetes in cluster deploy mode. Creating Instance \u00b6 KubernetesClientApplication takes no arguments to be created. KubernetesClientApplication is created when: SparkSubmit is requested to launch a Spark application (for kubernetes in cluster deploy mode ) Starting Spark Application \u00b6 start ( args : Array [ String ], conf : SparkConf ) : Unit start is part of the SparkApplication ( Apache Spark ) abstraction. start parses the command-line arguments ( args ) and runs . run \u00b6 run ( clientArguments : ClientArguments , sparkConf : SparkConf ) : Unit run generates a custom Spark Application ID of the format: spark-[randomUUID-without-dashes] run creates a KubernetesDriverConf (with the given ClientArguments , SparkConf and the custom Spark Application ID). run removes the k8s:// prefix from the spark.master configuration property (which has already been validated by SparkSubmit itself). run creates a LoggingPodStatusWatcherImpl (with the KubernetesDriverConf ). run creates a KubernetesClient (with the master URL, the namespace , and others). In the end, run creates a Client (with the KubernetesDriverConf , a new KubernetesDriverBuilder , the KubernetesClient , and the LoggingPodStatusWatcherImpl ) and requests it to run .","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#kubernetesclientapplication","text":"KubernetesClientApplication is a SparkApplication ( Apache Spark ) in Spark on Kubernetes in cluster deploy mode.","title":"KubernetesClientApplication"},{"location":"KubernetesClientApplication/#creating-instance","text":"KubernetesClientApplication takes no arguments to be created. KubernetesClientApplication is created when: SparkSubmit is requested to launch a Spark application (for kubernetes in cluster deploy mode )","title":"Creating Instance"},{"location":"KubernetesClientApplication/#starting-spark-application","text":"start ( args : Array [ String ], conf : SparkConf ) : Unit start is part of the SparkApplication ( Apache Spark ) abstraction. start parses the command-line arguments ( args ) and runs .","title":" Starting Spark Application"},{"location":"KubernetesClientApplication/#run","text":"run ( clientArguments : ClientArguments , sparkConf : SparkConf ) : Unit run generates a custom Spark Application ID of the format: spark-[randomUUID-without-dashes] run creates a KubernetesDriverConf (with the given ClientArguments , SparkConf and the custom Spark Application ID). run removes the k8s:// prefix from the spark.master configuration property (which has already been validated by SparkSubmit itself). run creates a LoggingPodStatusWatcherImpl (with the KubernetesDriverConf ). run creates a KubernetesClient (with the master URL, the namespace , and others). In the end, run creates a Client (with the KubernetesDriverConf , a new KubernetesDriverBuilder , the KubernetesClient , and the LoggingPodStatusWatcherImpl ) and requests it to run .","title":" run"},{"location":"KubernetesClusterManager/","text":"KubernetesClusterManager \u00b6 KubernetesClusterManager is an ExternalClusterManager ( Apache Spark ) that can create scheduler components for k8s master URLs. KubernetesClusterManager is registered with Apache Spark using META-INF/services/org.apache.spark.scheduler.ExternalClusterManager service file. Creating Instance \u00b6 KubernetesClusterManager takes no arguments to be created. KubernetesClusterManager is created when: SparkContext is requested for an ExternalClusterManager (when requested for a SchedulerBackend and TaskScheduler ) Creating SchedulerBackend \u00b6 createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend is part of the ExternalClusterManager ( Apache Spark ) abstraction. createSchedulerBackend creates a KubernetesClusterSchedulerBackend . Note createSchedulerBackend assumes that the given TaskScheduler is TaskSchedulerImpl ( Apache Spark ). createSchedulerBackend determines four internal values based on the spark.kubernetes.submitInDriver internal configuration property. spark.kubernetes.submitInDriver Enabled ( true ) Disabled ( false ) authConfPrefix spark.kubernetes.authenticate.driver.mounted spark.kubernetes.authenticate apiServerUri spark.kubernetes.driver.master Master URL with no k8s:// prefix defaultServiceAccountToken /var/run/secrets/kubernetes.io/serviceaccount/token defaultServiceAccountCaCrt /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Unless already defined, createSchedulerBackend sets the spark.kubernetes.executor.podNamePrefix configuration properties based on spark.app.name prefix. createSchedulerBackend creates a KubernetesClient for the Driver client type and the following: spark.kubernetes.namespace configuration property apiServerUri authConfPrefix defaultServiceAccountToken defaultServiceAccountCaCrt With spark.kubernetes.executor.podTemplateFile configuration property enabled, createSchedulerBackend loads the pod spec from the pod template file with the optional spark.kubernetes.executor.podTemplateContainerName configuration property. In the end, createSchedulerBackend creates a KubernetesClusterSchedulerBackend with the following: Java ScheduledExecutorService with kubernetes-executor-maintenance thread name ExecutorPodsSnapshotsStoreImpl with a Java ScheduledExecutorService with kubernetes-executor-snapshots-subscribers thread names and 2 threads ExecutorPodsLifecycleManager ExecutorPodsAllocator ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource with a Java ScheduledExecutorService with kubernetes-executor-pod-polling-sync thread name IllegalArgumentException \u00b6 With spark.kubernetes.submitInDriver enabled, createSchedulerBackend asserts that the name of the driver pod is configured (using spark.kubernetes.driver.pod.name configuration property) or else throws an IllegalArgumentException : If the application is deployed using spark-submit in cluster mode, the driver pod name must be provided. Creating TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler is part of the ExternalClusterManager ( Apache Spark ) abstraction. createTaskScheduler creates a TaskSchedulerImpl ( Apache Spark ). Initializing Scheduling Components \u00b6 initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize is part of the ExternalClusterManager ( Apache Spark ) abstraction. initialize requests the given TaskSchedulerImpl ( Apache Spark ) to initialize with the given SchedulerBackend ( Apache Spark ).","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#kubernetesclustermanager","text":"KubernetesClusterManager is an ExternalClusterManager ( Apache Spark ) that can create scheduler components for k8s master URLs. KubernetesClusterManager is registered with Apache Spark using META-INF/services/org.apache.spark.scheduler.ExternalClusterManager service file.","title":"KubernetesClusterManager"},{"location":"KubernetesClusterManager/#creating-instance","text":"KubernetesClusterManager takes no arguments to be created. KubernetesClusterManager is created when: SparkContext is requested for an ExternalClusterManager (when requested for a SchedulerBackend and TaskScheduler )","title":"Creating Instance"},{"location":"KubernetesClusterManager/#creating-schedulerbackend","text":"createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend is part of the ExternalClusterManager ( Apache Spark ) abstraction. createSchedulerBackend creates a KubernetesClusterSchedulerBackend . Note createSchedulerBackend assumes that the given TaskScheduler is TaskSchedulerImpl ( Apache Spark ). createSchedulerBackend determines four internal values based on the spark.kubernetes.submitInDriver internal configuration property. spark.kubernetes.submitInDriver Enabled ( true ) Disabled ( false ) authConfPrefix spark.kubernetes.authenticate.driver.mounted spark.kubernetes.authenticate apiServerUri spark.kubernetes.driver.master Master URL with no k8s:// prefix defaultServiceAccountToken /var/run/secrets/kubernetes.io/serviceaccount/token defaultServiceAccountCaCrt /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Unless already defined, createSchedulerBackend sets the spark.kubernetes.executor.podNamePrefix configuration properties based on spark.app.name prefix. createSchedulerBackend creates a KubernetesClient for the Driver client type and the following: spark.kubernetes.namespace configuration property apiServerUri authConfPrefix defaultServiceAccountToken defaultServiceAccountCaCrt With spark.kubernetes.executor.podTemplateFile configuration property enabled, createSchedulerBackend loads the pod spec from the pod template file with the optional spark.kubernetes.executor.podTemplateContainerName configuration property. In the end, createSchedulerBackend creates a KubernetesClusterSchedulerBackend with the following: Java ScheduledExecutorService with kubernetes-executor-maintenance thread name ExecutorPodsSnapshotsStoreImpl with a Java ScheduledExecutorService with kubernetes-executor-snapshots-subscribers thread names and 2 threads ExecutorPodsLifecycleManager ExecutorPodsAllocator ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource with a Java ScheduledExecutorService with kubernetes-executor-pod-polling-sync thread name","title":" Creating SchedulerBackend"},{"location":"KubernetesClusterManager/#illegalargumentexception","text":"With spark.kubernetes.submitInDriver enabled, createSchedulerBackend asserts that the name of the driver pod is configured (using spark.kubernetes.driver.pod.name configuration property) or else throws an IllegalArgumentException : If the application is deployed using spark-submit in cluster mode, the driver pod name must be provided.","title":"IllegalArgumentException"},{"location":"KubernetesClusterManager/#creating-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler is part of the ExternalClusterManager ( Apache Spark ) abstraction. createTaskScheduler creates a TaskSchedulerImpl ( Apache Spark ).","title":" Creating TaskScheduler"},{"location":"KubernetesClusterManager/#initializing-scheduling-components","text":"initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize is part of the ExternalClusterManager ( Apache Spark ) abstraction. initialize requests the given TaskSchedulerImpl ( Apache Spark ) to initialize with the given SchedulerBackend ( Apache Spark ).","title":" Initializing Scheduling Components"},{"location":"KubernetesClusterSchedulerBackend/","text":"KubernetesClusterSchedulerBackend \u00b6 KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend ( Apache Spark ) for Spark on Kubernetes . Creating Instance \u00b6 KubernetesClusterSchedulerBackend takes the following to be created: TaskSchedulerImpl ( Apache Spark ) SparkContext ( Apache Spark ) KubernetesClient Java's ScheduledExecutorService ExecutorPodsSnapshotsStore ExecutorPodsAllocator ExecutorPodsLifecycleManager ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource KubernetesClusterSchedulerBackend is created when: KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsLifecycleManager \u00b6 KubernetesClusterSchedulerBackend is given an ExecutorPodsLifecycleManager to be created . KubernetesClusterSchedulerBackend requests the ExecutorPodsLifecycleManager to start (with itself) when started . ExecutorPodsAllocator \u00b6 KubernetesClusterSchedulerBackend is given an ExecutorPodsAllocator to be created . When started , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the number of initial executors and starts it with application Id . When requested for the expected number of executors , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given total number of executors. When requested to isBlacklisted , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to isDeleted with a given executor. Initial Executors \u00b6 initialExecutors : Int KubernetesClusterSchedulerBackend calculates the initial target number of executors when created . initialExecutors is used when KubernetesClusterSchedulerBackend is requested to start and whether or not sufficient resources registered . Application Id \u00b6 applicationId () : String applicationId is part of the SchedulerBackend ( Apache Spark ) abstraction. applicationId is the value of spark.app.id configuration property if defined or the default applicationId . Sufficient Resources Registered \u00b6 sufficientResourcesRegistered () : Boolean sufficientResourcesRegistered is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. sufficientResourcesRegistered holds (is true ) when the totalRegisteredExecutors is at least the ratio of the initial executors . Minimum Resources Available Ratio \u00b6 minRegisteredRatio : Double minRegisteredRatio is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. minRegisteredRatio is 0.8 unless spark.scheduler.minRegisteredResourcesRatio configuration property is defined. Starting SchedulerBackend \u00b6 start () : Unit start is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. start creates a delegation token manager. start requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors . start requests the ExecutorPodsLifecycleManager to start (with this KubernetesClusterSchedulerBackend ). start requests the ExecutorPodsAllocator to start (with the applicationId ) start requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId ) start requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId ) Creating DriverEndpoint \u00b6 createDriverEndpoint () : DriverEndpoint createDriverEndpoint is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. createDriverEndpoint creates a KubernetesDriverEndpoint . Requesting Executors from Cluster Manager \u00b6 doRequestTotalExecutors ( requestedTotal : Int ) : Future [ Boolean ] doRequestTotalExecutors is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. doRequestTotalExecutors requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given requestedTotal . In the end, doRequestTotalExecutors returns a completed Future with true value. Stopping SchedulerBackend \u00b6 stop () : Unit stop is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. stop ...FIXME","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#kubernetesclusterschedulerbackend","text":"KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend ( Apache Spark ) for Spark on Kubernetes .","title":"KubernetesClusterSchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#creating-instance","text":"KubernetesClusterSchedulerBackend takes the following to be created: TaskSchedulerImpl ( Apache Spark ) SparkContext ( Apache Spark ) KubernetesClient Java's ScheduledExecutorService ExecutorPodsSnapshotsStore ExecutorPodsAllocator ExecutorPodsLifecycleManager ExecutorPodsWatchSnapshotSource ExecutorPodsPollingSnapshotSource KubernetesClusterSchedulerBackend is created when: KubernetesClusterManager is requested for a SchedulerBackend","title":"Creating Instance"},{"location":"KubernetesClusterSchedulerBackend/#executorpodslifecyclemanager","text":"KubernetesClusterSchedulerBackend is given an ExecutorPodsLifecycleManager to be created . KubernetesClusterSchedulerBackend requests the ExecutorPodsLifecycleManager to start (with itself) when started .","title":" ExecutorPodsLifecycleManager"},{"location":"KubernetesClusterSchedulerBackend/#executorpodsallocator","text":"KubernetesClusterSchedulerBackend is given an ExecutorPodsAllocator to be created . When started , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the number of initial executors and starts it with application Id . When requested for the expected number of executors , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given total number of executors. When requested to isBlacklisted , KubernetesClusterSchedulerBackend requests the ExecutorPodsAllocator to isDeleted with a given executor.","title":" ExecutorPodsAllocator"},{"location":"KubernetesClusterSchedulerBackend/#initial-executors","text":"initialExecutors : Int KubernetesClusterSchedulerBackend calculates the initial target number of executors when created . initialExecutors is used when KubernetesClusterSchedulerBackend is requested to start and whether or not sufficient resources registered .","title":" Initial Executors"},{"location":"KubernetesClusterSchedulerBackend/#application-id","text":"applicationId () : String applicationId is part of the SchedulerBackend ( Apache Spark ) abstraction. applicationId is the value of spark.app.id configuration property if defined or the default applicationId .","title":" Application Id"},{"location":"KubernetesClusterSchedulerBackend/#sufficient-resources-registered","text":"sufficientResourcesRegistered () : Boolean sufficientResourcesRegistered is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. sufficientResourcesRegistered holds (is true ) when the totalRegisteredExecutors is at least the ratio of the initial executors .","title":" Sufficient Resources Registered"},{"location":"KubernetesClusterSchedulerBackend/#minimum-resources-available-ratio","text":"minRegisteredRatio : Double minRegisteredRatio is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. minRegisteredRatio is 0.8 unless spark.scheduler.minRegisteredResourcesRatio configuration property is defined.","title":" Minimum Resources Available Ratio"},{"location":"KubernetesClusterSchedulerBackend/#starting-schedulerbackend","text":"start () : Unit start is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. start creates a delegation token manager. start requests the ExecutorPodsAllocator to setTotalExpectedExecutors to initialExecutors . start requests the ExecutorPodsLifecycleManager to start (with this KubernetesClusterSchedulerBackend ). start requests the ExecutorPodsAllocator to start (with the applicationId ) start requests the ExecutorPodsWatchSnapshotSource to start (with the applicationId ) start requests the ExecutorPodsPollingSnapshotSource to start (with the applicationId )","title":" Starting SchedulerBackend"},{"location":"KubernetesClusterSchedulerBackend/#creating-driverendpoint","text":"createDriverEndpoint () : DriverEndpoint createDriverEndpoint is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. createDriverEndpoint creates a KubernetesDriverEndpoint .","title":" Creating DriverEndpoint"},{"location":"KubernetesClusterSchedulerBackend/#requesting-executors-from-cluster-manager","text":"doRequestTotalExecutors ( requestedTotal : Int ) : Future [ Boolean ] doRequestTotalExecutors is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. doRequestTotalExecutors requests the ExecutorPodsAllocator to setTotalExpectedExecutors to the given requestedTotal . In the end, doRequestTotalExecutors returns a completed Future with true value.","title":" Requesting Executors from Cluster Manager"},{"location":"KubernetesClusterSchedulerBackend/#stopping-schedulerbackend","text":"stop () : Unit stop is part of the CoarseGrainedSchedulerBackend ( Apache Spark ) abstraction. stop ...FIXME","title":" Stopping SchedulerBackend"},{"location":"KubernetesConf/","text":"KubernetesConf \u00b6 KubernetesConf is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors ). Contract \u00b6 annotations \u00b6 annotations : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod environment \u00b6 environment : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod labels \u00b6 labels : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod DriverServiceFeatureStep is requested to getAdditionalKubernetesResources resourceNamePrefix \u00b6 resourceNamePrefix : String Prefix of resource names secretEnvNamesToKeyRefs \u00b6 secretEnvNamesToKeyRefs : Map [ String , String ] Used when: EnvSecretsFeatureStep is requested to configurePod secretNamesToMountPaths \u00b6 secretNamesToMountPaths : Map [ String , String ] Used when: MountSecretsFeatureStep is requested to configurePod Volume Specs \u00b6 volumes : Seq [ KubernetesVolumeSpec ] KubernetesVolumeSpec s with the following: volumeName mountPath mountSubPath mountReadOnly volumeConf Used when: MountVolumesFeatureStep is requested to configure a pod Implementations \u00b6 KubernetesDriverConf KubernetesExecutorConf Creating Instance \u00b6 KubernetesConf takes the following to be created: SparkConf Abstract Class KubernetesConf is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs . Namespace \u00b6 namespace : String namespace is the value of spark.kubernetes.namespace configuration property. namespace is used when: DriverServiceFeatureStep is requested to getAdditionalPodSystemProperties Client is requested to run KubernetesClientApplication is requested to start imagePullPolicy \u00b6 imagePullPolicy : String imagePullPolicy is the value of spark.kubernetes.container.image.pullPolicy configuration property. imagePullPolicy is used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod Creating KubernetesDriverConf \u00b6 createDriverConf ( sparkConf : SparkConf , appId : String , mainAppResource : MainAppResource , mainClass : String , appArgs : Array [ String ]) : KubernetesDriverConf Note The goal of createDriverConf is to validate executor volumes before creating a KubernetesDriverConf . createDriverConf parse volumes for executors (with spark.kubernetes.executor.volumes prefix). Note createDriverConf parses executor volumes in order to verify configuration before the driver pod is created. In the end, createDriverConf creates a KubernetesDriverConf . createDriverConf is used when: KubernetesClientApplication is requested to start Creating KubernetesExecutorConf \u00b6 createExecutorConf ( sparkConf : SparkConf , executorId : String , appId : String , driverPod : Option [ Pod ]) : KubernetesExecutorConf createExecutorConf ( does nothing more but ) creates a KubernetesExecutorConf for the given input arguments. createExecutorConf is used when: ExecutorPodsAllocator is requested to onNewSnapshots (and requests missing executors from Kubernetes) AppName-Based Unique Resource Name Prefix \u00b6 getResourceNamePrefix ( appName : String ) : String getResourceNamePrefix ...FIXME getResourceNamePrefix is used when: KubernetesDriverConf is requested for the resourceNamePrefix KubernetesExecutorConf is requested for the resourceNamePrefix KubernetesClusterManager is requested to createSchedulerBackend","title":"KubernetesConf"},{"location":"KubernetesConf/#kubernetesconf","text":"KubernetesConf is an abstraction of Kubernetes configuration metadata to build Spark pods (for the driver and executors ).","title":"KubernetesConf"},{"location":"KubernetesConf/#contract","text":"","title":"Contract"},{"location":"KubernetesConf/#annotations","text":"annotations : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" annotations"},{"location":"KubernetesConf/#environment","text":"environment : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod","title":" environment"},{"location":"KubernetesConf/#labels","text":"labels : Map [ String , String ] Used when: BasicDriverFeatureStep is requested to configurePod BasicExecutorFeatureStep is requested to configurePod DriverServiceFeatureStep is requested to getAdditionalKubernetesResources","title":" labels"},{"location":"KubernetesConf/#resourcenameprefix","text":"resourceNamePrefix : String Prefix of resource names","title":" resourceNamePrefix"},{"location":"KubernetesConf/#secretenvnamestokeyrefs","text":"secretEnvNamesToKeyRefs : Map [ String , String ] Used when: EnvSecretsFeatureStep is requested to configurePod","title":" secretEnvNamesToKeyRefs"},{"location":"KubernetesConf/#secretnamestomountpaths","text":"secretNamesToMountPaths : Map [ String , String ] Used when: MountSecretsFeatureStep is requested to configurePod","title":" secretNamesToMountPaths"},{"location":"KubernetesConf/#volume-specs","text":"volumes : Seq [ KubernetesVolumeSpec ] KubernetesVolumeSpec s with the following: volumeName mountPath mountSubPath mountReadOnly volumeConf Used when: MountVolumesFeatureStep is requested to configure a pod","title":" Volume Specs"},{"location":"KubernetesConf/#implementations","text":"KubernetesDriverConf KubernetesExecutorConf","title":"Implementations"},{"location":"KubernetesConf/#creating-instance","text":"KubernetesConf takes the following to be created: SparkConf Abstract Class KubernetesConf is an abstract class and cannot be created directly. It is created indirectly for the concrete KubernetesConfs .","title":"Creating Instance"},{"location":"KubernetesConf/#namespace","text":"namespace : String namespace is the value of spark.kubernetes.namespace configuration property. namespace is used when: DriverServiceFeatureStep is requested to getAdditionalPodSystemProperties Client is requested to run KubernetesClientApplication is requested to start","title":" Namespace"},{"location":"KubernetesConf/#imagepullpolicy","text":"imagePullPolicy : String imagePullPolicy is the value of spark.kubernetes.container.image.pullPolicy configuration property. imagePullPolicy is used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod","title":" imagePullPolicy"},{"location":"KubernetesConf/#creating-kubernetesdriverconf","text":"createDriverConf ( sparkConf : SparkConf , appId : String , mainAppResource : MainAppResource , mainClass : String , appArgs : Array [ String ]) : KubernetesDriverConf Note The goal of createDriverConf is to validate executor volumes before creating a KubernetesDriverConf . createDriverConf parse volumes for executors (with spark.kubernetes.executor.volumes prefix). Note createDriverConf parses executor volumes in order to verify configuration before the driver pod is created. In the end, createDriverConf creates a KubernetesDriverConf . createDriverConf is used when: KubernetesClientApplication is requested to start","title":" Creating KubernetesDriverConf"},{"location":"KubernetesConf/#creating-kubernetesexecutorconf","text":"createExecutorConf ( sparkConf : SparkConf , executorId : String , appId : String , driverPod : Option [ Pod ]) : KubernetesExecutorConf createExecutorConf ( does nothing more but ) creates a KubernetesExecutorConf for the given input arguments. createExecutorConf is used when: ExecutorPodsAllocator is requested to onNewSnapshots (and requests missing executors from Kubernetes)","title":" Creating KubernetesExecutorConf"},{"location":"KubernetesConf/#appname-based-unique-resource-name-prefix","text":"getResourceNamePrefix ( appName : String ) : String getResourceNamePrefix ...FIXME getResourceNamePrefix is used when: KubernetesDriverConf is requested for the resourceNamePrefix KubernetesExecutorConf is requested for the resourceNamePrefix KubernetesClusterManager is requested to createSchedulerBackend","title":" AppName-Based Unique Resource Name Prefix"},{"location":"KubernetesDriverBuilder/","text":"KubernetesDriverBuilder \u00b6 KubernetesDriverBuilder is used to build a specification of a driver pod . Creating Instance \u00b6 KubernetesDriverBuilder takes no arguments to be created. KubernetesDriverBuilder is created when: KubernetesClientApplication is requested to start KubernetesDriverSpec \u00b6 KubernetesDriverSpec is the following: SparkPod Driver Resources System Properties Building Driver Pod Specification \u00b6 buildFromFeatures ( conf : KubernetesDriverConf , client : KubernetesClient ) : KubernetesDriverSpec buildFromFeatures creates an initial driver pod specification. With spark.kubernetes.driver.podTemplateFile configuration property defined, buildFromFeatures loads it (with the given KubernetesClient and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification. buildFromFeatures builds a KubernetesDriverSpec (with the initial driver pod specification). In the end, buildFromFeatures configures the driver pod specification (with additional system properties and additional resources ) through a series of the feature steps: BasicDriverFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep DriverCommandFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep PodTemplateConfigMapStep LocalDirsFeatureStep buildFromFeatures is used when: Client is requested to run","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#kubernetesdriverbuilder","text":"KubernetesDriverBuilder is used to build a specification of a driver pod .","title":"KubernetesDriverBuilder"},{"location":"KubernetesDriverBuilder/#creating-instance","text":"KubernetesDriverBuilder takes no arguments to be created. KubernetesDriverBuilder is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"KubernetesDriverBuilder/#kubernetesdriverspec","text":"KubernetesDriverSpec is the following: SparkPod Driver Resources System Properties","title":" KubernetesDriverSpec"},{"location":"KubernetesDriverBuilder/#building-driver-pod-specification","text":"buildFromFeatures ( conf : KubernetesDriverConf , client : KubernetesClient ) : KubernetesDriverSpec buildFromFeatures creates an initial driver pod specification. With spark.kubernetes.driver.podTemplateFile configuration property defined, buildFromFeatures loads it (with the given KubernetesClient and the container name based on spark.kubernetes.driver.podTemplateContainerName configuration property) or defaults to an empty pod specification. buildFromFeatures builds a KubernetesDriverSpec (with the initial driver pod specification). In the end, buildFromFeatures configures the driver pod specification (with additional system properties and additional resources ) through a series of the feature steps: BasicDriverFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep DriverCommandFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep PodTemplateConfigMapStep LocalDirsFeatureStep buildFromFeatures is used when: Client is requested to run","title":" Building Driver Pod Specification"},{"location":"KubernetesDriverConf/","text":"KubernetesDriverConf \u00b6 KubernetesDriverConf is a KubernetesConf . Creating Instance \u00b6 KubernetesDriverConf takes the following to be created: SparkConf Application ID MainAppResource Name of the Main Class Application Arguments KubernetesDriverConf is created when: KubernetesClientApplication is requested to start (via KubernetesConf utility ) Volume Specs \u00b6 volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the driver (with the spark.kubernetes.driver.volumes. prefix) from the SparkConf .","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#kubernetesdriverconf","text":"KubernetesDriverConf is a KubernetesConf .","title":"KubernetesDriverConf"},{"location":"KubernetesDriverConf/#creating-instance","text":"KubernetesDriverConf takes the following to be created: SparkConf Application ID MainAppResource Name of the Main Class Application Arguments KubernetesDriverConf is created when: KubernetesClientApplication is requested to start (via KubernetesConf utility )","title":"Creating Instance"},{"location":"KubernetesDriverConf/#volume-specs","text":"volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the driver (with the spark.kubernetes.driver.volumes. prefix) from the SparkConf .","title":" Volume Specs"},{"location":"KubernetesDriverEndpoint/","text":"KubernetesDriverEndpoint \u00b6 KubernetesDriverEndpoint is a DriverEndpoint ( Apache Spark ). Intercepting Executor Lost Event \u00b6 onDisconnected ( rpcAddress : RpcAddress ) : Unit onDisconnected is part of the RpcEndpoint (Apache Spark) abstraction. onDisconnected disables the executor known by the RpcAddress (found in the Executors by RpcAddress Registry registry).","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#kubernetesdriverendpoint","text":"KubernetesDriverEndpoint is a DriverEndpoint ( Apache Spark ).","title":"KubernetesDriverEndpoint"},{"location":"KubernetesDriverEndpoint/#intercepting-executor-lost-event","text":"onDisconnected ( rpcAddress : RpcAddress ) : Unit onDisconnected is part of the RpcEndpoint (Apache Spark) abstraction. onDisconnected disables the executor known by the RpcAddress (found in the Executors by RpcAddress Registry registry).","title":" Intercepting Executor Lost Event"},{"location":"KubernetesExecutorBuilder/","text":"KubernetesExecutorBuilder \u00b6 Creating Instance \u00b6 KubernetesExecutorBuilder takes no arguments to be created ( and could really be a Scala utility object ). KubernetesExecutorBuilder is created when: KubernetesClusterManager is requested to create a SchedulerBackend (and creates a ExecutorPodsAllocator ) Building Executor Pod Specification \u00b6 buildFromFeatures ( conf : KubernetesExecutorConf , secMgr : SecurityManager , client : KubernetesClient ) : SparkPod When defined, buildFromFeatures loads the pod spec from the pod template file (based on the spark.kubernetes.executor.podTemplateFile and spark.kubernetes.executor.podTemplateContainerName configuration properties). Otherwise, buildFromFeatures starts from an initial empty pod specification. In the end, buildFromFeatures configures the executor pod specification through a series of the feature steps: BasicExecutorFeatureStep ExecutorKubernetesCredentialsFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep LocalDirsFeatureStep buildFromFeatures is used when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes)","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorBuilder/#kubernetesexecutorbuilder","text":"","title":"KubernetesExecutorBuilder"},{"location":"KubernetesExecutorBuilder/#creating-instance","text":"KubernetesExecutorBuilder takes no arguments to be created ( and could really be a Scala utility object ). KubernetesExecutorBuilder is created when: KubernetesClusterManager is requested to create a SchedulerBackend (and creates a ExecutorPodsAllocator )","title":"Creating Instance"},{"location":"KubernetesExecutorBuilder/#building-executor-pod-specification","text":"buildFromFeatures ( conf : KubernetesExecutorConf , secMgr : SecurityManager , client : KubernetesClient ) : SparkPod When defined, buildFromFeatures loads the pod spec from the pod template file (based on the spark.kubernetes.executor.podTemplateFile and spark.kubernetes.executor.podTemplateContainerName configuration properties). Otherwise, buildFromFeatures starts from an initial empty pod specification. In the end, buildFromFeatures configures the executor pod specification through a series of the feature steps: BasicExecutorFeatureStep ExecutorKubernetesCredentialsFeatureStep MountSecretsFeatureStep EnvSecretsFeatureStep MountVolumesFeatureStep LocalDirsFeatureStep buildFromFeatures is used when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes)","title":" Building Executor Pod Specification"},{"location":"KubernetesExecutorConf/","text":"KubernetesExecutorConf \u00b6 KubernetesExecutorConf is a KubernetesConf . Creating Instance \u00b6 KubernetesExecutorConf takes the following to be created: SparkConf Application ID Executor ID Optional Driver Pod KubernetesExecutorConf is created when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes via KubernetesConf utility ) Volume Specs \u00b6 volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the executor pod (with the spark.kubernetes.executor.volumes. prefix) from the SparkConf .","title":"KubernetesExecutorConf"},{"location":"KubernetesExecutorConf/#kubernetesexecutorconf","text":"KubernetesExecutorConf is a KubernetesConf .","title":"KubernetesExecutorConf"},{"location":"KubernetesExecutorConf/#creating-instance","text":"KubernetesExecutorConf takes the following to be created: SparkConf Application ID Executor ID Optional Driver Pod KubernetesExecutorConf is created when: ExecutorPodsAllocator is requested to handle executor pods snapshots (and requests missing executors from Kubernetes via KubernetesConf utility )","title":"Creating Instance"},{"location":"KubernetesExecutorConf/#volume-specs","text":"volumes : Seq [ KubernetesVolumeSpec ] volumes is part of the KubernetesConf abstraction. volumes parses volume specs for the executor pod (with the spark.kubernetes.executor.volumes. prefix) from the SparkConf .","title":" Volume Specs"},{"location":"KubernetesFeatureConfigStep/","text":"KubernetesFeatureConfigStep \u00b6 KubernetesFeatureConfigStep is an abstraction of Kubernetes pod features for drivers and executors. Contract \u00b6 Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod Used when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec Additional Kubernetes Resources \u00b6 getAdditionalKubernetesResources () : Seq [ HasMetadata ] Additional Kubernetes resources Default: empty Used when: KubernetesDriverBuilder is requested to build a driver pod spec Additional System Properties \u00b6 getAdditionalPodSystemProperties () : Map [ String , String ] Additional system properties of a driver pod (to be used for a ConfigMap ) Default: empty Used when: KubernetesDriverBuilder is requested to build a driver pod spec Implementations \u00b6 BasicDriverFeatureStep BasicExecutorFeatureStep DriverCommandFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep EnvSecretsFeatureStep ExecutorKubernetesCredentialsFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep LocalDirsFeatureStep MountSecretsFeatureStep MountVolumesFeatureStep PodTemplateConfigMapStep","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#kubernetesfeatureconfigstep","text":"KubernetesFeatureConfigStep is an abstraction of Kubernetes pod features for drivers and executors.","title":"KubernetesFeatureConfigStep"},{"location":"KubernetesFeatureConfigStep/#contract","text":"","title":"Contract"},{"location":"KubernetesFeatureConfigStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod Used when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec","title":" Configuring Pod"},{"location":"KubernetesFeatureConfigStep/#additional-kubernetes-resources","text":"getAdditionalKubernetesResources () : Seq [ HasMetadata ] Additional Kubernetes resources Default: empty Used when: KubernetesDriverBuilder is requested to build a driver pod spec","title":" Additional Kubernetes Resources"},{"location":"KubernetesFeatureConfigStep/#additional-system-properties","text":"getAdditionalPodSystemProperties () : Map [ String , String ] Additional system properties of a driver pod (to be used for a ConfigMap ) Default: empty Used when: KubernetesDriverBuilder is requested to build a driver pod spec","title":" Additional System Properties"},{"location":"KubernetesFeatureConfigStep/#implementations","text":"BasicDriverFeatureStep BasicExecutorFeatureStep DriverCommandFeatureStep DriverKubernetesCredentialsFeatureStep DriverServiceFeatureStep EnvSecretsFeatureStep ExecutorKubernetesCredentialsFeatureStep HadoopConfDriverFeatureStep KerberosConfDriverFeatureStep LocalDirsFeatureStep MountSecretsFeatureStep MountVolumesFeatureStep PodTemplateConfigMapStep","title":"Implementations"},{"location":"KubernetesUtils/","text":"KubernetesUtils \u00b6 Loading Pod from Template File \u00b6 loadPodFromTemplate ( kubernetesClient : KubernetesClient , templateFile : File , containerName : Option [ String ]) : SparkPod loadPodFromTemplate requests the given KubernetesClient to load a pod for the input template file. loadPodFromTemplate selectSparkContainer (with the pod and the input container name). In case of an Exception , loadPodFromTemplate prints out the following ERROR message to the logs: Encountered exception while attempting to load initial pod spec from file loadPodFromTemplate (re)throws a SparkException : Could not load pod from template file. loadPodFromTemplate is used when: KubernetesClusterManager is requested to createSchedulerBackend KubernetesDriverBuilder is requested to buildFromFeatures KubernetesExecutorBuilder is requested to buildFromFeatures Uploading Local Files to Hadoop DFS \u00b6 uploadAndTransformFileUris ( fileUris : Iterable [ String ], conf : Option [ SparkConf ] = None ) : Iterable [ String ] uploadAndTransformFileUris uploads local files in the given fileUris to Hadoop DFS (based on spark.kubernetes.file.upload.path configuration property). In the end, uploadAndTransformFileUris returns the target URIs. uploadAndTransformFileUris is used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties uploadFileUri \u00b6 uploadFileUri ( uri : String , conf : Option [ SparkConf ] = None ) : String uploadFileUri resolves the given uri to a well-formed file URI. uploadFileUri creates a new Hadoop Configuration and resolves the spark.kubernetes.file.upload.path configuration property to a Hadoop FileSystem . uploadFileUri creates ( mkdirs ) the Hadoop DFS path to upload the file of the format: [spark.kubernetes.file.upload.path]/[spark-upload-[randomUUID]] uploadFileUri prints out the following INFO message to the logs: Uploading file: [path] to dest: [targetUri]... In the end, uploadFileUri uploads the file to the target location (using Hadoop DFS's FileSystem.copyFromLocalFile ) and returns the target URI. SparkExceptions \u00b6 uploadFileUri throws a SparkException when: Uploading the uri fails: Uploading file [path] failed... spark.kubernetes.file.upload.path configuration property is not defined: Please specify spark.kubernetes.file.upload.path property. SparkConf is not defined: Spark configuration is missing... renameMainAppResource \u00b6 renameMainAppResource ( resource : String , conf : SparkConf ) : String renameMainAppResource is converted to spark-internal internal name when the given resource is local and resolvable . Otherwise, renameMainAppResource returns the given resource as-is. renameMainAppResource is used when: DriverCommandFeatureStep is requested for the base driver container (for a JavaMainAppResource application) isLocalAndResolvable \u00b6 isLocalAndResolvable ( resource : String ) : Boolean isLocalAndResolvable is true when the given resource is: Not internal Uses either file or no URI scheme (after converting to a well-formed URI) isLocalAndResolvable is used when: KubernetesUtils is requested to renameMainAppResource BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties isLocalDependency \u00b6 isLocalDependency ( uri : URI ) : Boolean An input URI is a local dependency when the scheme is null (undefined) or file . Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.KubernetesUtils logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.KubernetesUtils=ALL Refer to Logging .","title":"KubernetesUtils"},{"location":"KubernetesUtils/#kubernetesutils","text":"","title":"KubernetesUtils"},{"location":"KubernetesUtils/#loading-pod-from-template-file","text":"loadPodFromTemplate ( kubernetesClient : KubernetesClient , templateFile : File , containerName : Option [ String ]) : SparkPod loadPodFromTemplate requests the given KubernetesClient to load a pod for the input template file. loadPodFromTemplate selectSparkContainer (with the pod and the input container name). In case of an Exception , loadPodFromTemplate prints out the following ERROR message to the logs: Encountered exception while attempting to load initial pod spec from file loadPodFromTemplate (re)throws a SparkException : Could not load pod from template file. loadPodFromTemplate is used when: KubernetesClusterManager is requested to createSchedulerBackend KubernetesDriverBuilder is requested to buildFromFeatures KubernetesExecutorBuilder is requested to buildFromFeatures","title":" Loading Pod from Template File"},{"location":"KubernetesUtils/#uploading-local-files-to-hadoop-dfs","text":"uploadAndTransformFileUris ( fileUris : Iterable [ String ], conf : Option [ SparkConf ] = None ) : Iterable [ String ] uploadAndTransformFileUris uploads local files in the given fileUris to Hadoop DFS (based on spark.kubernetes.file.upload.path configuration property). In the end, uploadAndTransformFileUris returns the target URIs. uploadAndTransformFileUris is used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties","title":" Uploading Local Files to Hadoop DFS"},{"location":"KubernetesUtils/#uploadfileuri","text":"uploadFileUri ( uri : String , conf : Option [ SparkConf ] = None ) : String uploadFileUri resolves the given uri to a well-formed file URI. uploadFileUri creates a new Hadoop Configuration and resolves the spark.kubernetes.file.upload.path configuration property to a Hadoop FileSystem . uploadFileUri creates ( mkdirs ) the Hadoop DFS path to upload the file of the format: [spark.kubernetes.file.upload.path]/[spark-upload-[randomUUID]] uploadFileUri prints out the following INFO message to the logs: Uploading file: [path] to dest: [targetUri]... In the end, uploadFileUri uploads the file to the target location (using Hadoop DFS's FileSystem.copyFromLocalFile ) and returns the target URI.","title":" uploadFileUri"},{"location":"KubernetesUtils/#sparkexceptions","text":"uploadFileUri throws a SparkException when: Uploading the uri fails: Uploading file [path] failed... spark.kubernetes.file.upload.path configuration property is not defined: Please specify spark.kubernetes.file.upload.path property. SparkConf is not defined: Spark configuration is missing...","title":" SparkExceptions"},{"location":"KubernetesUtils/#renamemainappresource","text":"renameMainAppResource ( resource : String , conf : SparkConf ) : String renameMainAppResource is converted to spark-internal internal name when the given resource is local and resolvable . Otherwise, renameMainAppResource returns the given resource as-is. renameMainAppResource is used when: DriverCommandFeatureStep is requested for the base driver container (for a JavaMainAppResource application)","title":" renameMainAppResource"},{"location":"KubernetesUtils/#islocalandresolvable","text":"isLocalAndResolvable ( resource : String ) : Boolean isLocalAndResolvable is true when the given resource is: Not internal Uses either file or no URI scheme (after converting to a well-formed URI) isLocalAndResolvable is used when: KubernetesUtils is requested to renameMainAppResource BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties","title":" isLocalAndResolvable"},{"location":"KubernetesUtils/#islocaldependency","text":"isLocalDependency ( uri : URI ) : Boolean An input URI is a local dependency when the scheme is null (undefined) or file .","title":" isLocalDependency"},{"location":"KubernetesUtils/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.KubernetesUtils logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.KubernetesUtils=ALL Refer to Logging .","title":"Logging"},{"location":"KubernetesVolumeUtils/","text":"KubernetesVolumeUtils \u00b6 parseVolumesWithPrefix \u00b6 parseVolumesWithPrefix ( sparkConf : SparkConf , prefix : String ) : Seq [ KubernetesVolumeSpec ] parseVolumesWithPrefix requests the SparkConf to get all properties with the given prefix . parseVolumesWithPrefix extracts volume types and names (from the keys of the properties) and for every pair creates a KubernetesVolumeSpec with the following: volumeName mountPath based on [volumeType].[volumeName].mount.path key (in the properties) mountSubPath based on [volumeType].[volumeName].mount.subPath key (in the properties) if available or defaults to an empty path mountReadOnly based on [volumeType].[volumeName].mount.readOnly key (in the properties) if available or false volumeConf with a KubernetesVolumeSpecificConf based on the properties and the volumeType and volumeName of the volume parseVolumesWithPrefix is used when: KubernetesDriverConf is requested for volumes KubernetesExecutorConf is requested for volumes KubernetesConf utility is used to create a KubernetesDriverConf Extracting Volume Types and Names \u00b6 getVolumeTypesAndNames ( properties : Map [ String , String ]) : Set [( String , String )] getVolumeTypesAndNames splits the keys (in the given properties key-value collection) by . to a pair of a volume type and a name. Extracting Volume Configuration \u00b6 parseVolumeSpecificConf ( options : Map [ String , String ], volumeType : String , volumeName : String ) : KubernetesVolumeSpecificConf parseVolumeSpecificConf creates a KubernetesVolumeSpecificConf based on the given volumeType . volumeType Keys hostPath [volumeType].[volumeName].options.path persistentVolumeClaim [volumeType].[volumeName].options.claimName emptyDir [volumeType].[volumeName].options.medium [volumeType].[volumeName].options.sizeLimit parseVolumeSpecificConf throws an IllegalArgumentException for unsupported volumeType : Kubernetes Volume type `[volumeType]` is not supported","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#kubernetesvolumeutils","text":"","title":"KubernetesVolumeUtils"},{"location":"KubernetesVolumeUtils/#parsevolumeswithprefix","text":"parseVolumesWithPrefix ( sparkConf : SparkConf , prefix : String ) : Seq [ KubernetesVolumeSpec ] parseVolumesWithPrefix requests the SparkConf to get all properties with the given prefix . parseVolumesWithPrefix extracts volume types and names (from the keys of the properties) and for every pair creates a KubernetesVolumeSpec with the following: volumeName mountPath based on [volumeType].[volumeName].mount.path key (in the properties) mountSubPath based on [volumeType].[volumeName].mount.subPath key (in the properties) if available or defaults to an empty path mountReadOnly based on [volumeType].[volumeName].mount.readOnly key (in the properties) if available or false volumeConf with a KubernetesVolumeSpecificConf based on the properties and the volumeType and volumeName of the volume parseVolumesWithPrefix is used when: KubernetesDriverConf is requested for volumes KubernetesExecutorConf is requested for volumes KubernetesConf utility is used to create a KubernetesDriverConf","title":" parseVolumesWithPrefix"},{"location":"KubernetesVolumeUtils/#extracting-volume-types-and-names","text":"getVolumeTypesAndNames ( properties : Map [ String , String ]) : Set [( String , String )] getVolumeTypesAndNames splits the keys (in the given properties key-value collection) by . to a pair of a volume type and a name.","title":" Extracting Volume Types and Names"},{"location":"KubernetesVolumeUtils/#extracting-volume-configuration","text":"parseVolumeSpecificConf ( options : Map [ String , String ], volumeType : String , volumeName : String ) : KubernetesVolumeSpecificConf parseVolumeSpecificConf creates a KubernetesVolumeSpecificConf based on the given volumeType . volumeType Keys hostPath [volumeType].[volumeName].options.path persistentVolumeClaim [volumeType].[volumeName].options.claimName emptyDir [volumeType].[volumeName].options.medium [volumeType].[volumeName].options.sizeLimit parseVolumeSpecificConf throws an IllegalArgumentException for unsupported volumeType : Kubernetes Volume type `[volumeType]` is not supported","title":" Extracting Volume Configuration"},{"location":"LocalDirsFeatureStep/","text":"LocalDirsFeatureStep \u00b6 LocalDirsFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 LocalDirsFeatureStep takes the following to be created: KubernetesConf Default Local Directory (default: /var/data/spark-[randomUUID] ) LocalDirsFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod KubernetesExecutorBuilder is requested to build an executor pod spark.kubernetes.local.dirs.tmpfs \u00b6 LocalDirsFeatureStep uses spark.kubernetes.local.dirs.tmpfs configuration property when configuring a pod . Configuring Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod finds mount paths of the volume mounts with spark-local-dir- prefix name of the input SparkPod ( localDirs ). If there are no local directory mount paths, configurePod ...FIXME configurePod adds the local directory volumes to a new pod specification (there could be none). configurePod defines SPARK_LOCAL_DIRS environment variable as a comma-separated local directories and adds the local directory volume mounts to a new container specification (there could be none). In the end, configurePod creates a new SparkPod with the new pod and container.","title":"LocalDirsFeatureStep"},{"location":"LocalDirsFeatureStep/#localdirsfeaturestep","text":"LocalDirsFeatureStep is a KubernetesFeatureConfigStep .","title":"LocalDirsFeatureStep"},{"location":"LocalDirsFeatureStep/#creating-instance","text":"LocalDirsFeatureStep takes the following to be created: KubernetesConf Default Local Directory (default: /var/data/spark-[randomUUID] ) LocalDirsFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod KubernetesExecutorBuilder is requested to build an executor pod","title":"Creating Instance"},{"location":"LocalDirsFeatureStep/#sparkkuberneteslocaldirstmpfs","text":"LocalDirsFeatureStep uses spark.kubernetes.local.dirs.tmpfs configuration property when configuring a pod .","title":" spark.kubernetes.local.dirs.tmpfs"},{"location":"LocalDirsFeatureStep/#configuring-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod finds mount paths of the volume mounts with spark-local-dir- prefix name of the input SparkPod ( localDirs ). If there are no local directory mount paths, configurePod ...FIXME configurePod adds the local directory volumes to a new pod specification (there could be none). configurePod defines SPARK_LOCAL_DIRS environment variable as a comma-separated local directories and adds the local directory volume mounts to a new container specification (there could be none). In the end, configurePod creates a new SparkPod with the new pod and container.","title":" Configuring Pod"},{"location":"LoggingPodStatusWatcher/","text":"LoggingPodStatusWatcher \u00b6 LoggingPodStatusWatcher is an extension of Kubernetes' Watcher[Pod] for pod watchers that can watchOrStop . LoggingPodStatusWatcher is used to create a Client when a KubernetesClientApplication is requested to start . Contract \u00b6 watchOrStop \u00b6 watchOrStop ( submissionId : String ) : Unit Used when: Client is requested to run Implementations \u00b6 LoggingPodStatusWatcherImpl","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#loggingpodstatuswatcher","text":"LoggingPodStatusWatcher is an extension of Kubernetes' Watcher[Pod] for pod watchers that can watchOrStop . LoggingPodStatusWatcher is used to create a Client when a KubernetesClientApplication is requested to start .","title":"LoggingPodStatusWatcher"},{"location":"LoggingPodStatusWatcher/#contract","text":"","title":"Contract"},{"location":"LoggingPodStatusWatcher/#watchorstop","text":"watchOrStop ( submissionId : String ) : Unit Used when: Client is requested to run","title":" watchOrStop"},{"location":"LoggingPodStatusWatcher/#implementations","text":"LoggingPodStatusWatcherImpl","title":"Implementations"},{"location":"LoggingPodStatusWatcherImpl/","text":"LoggingPodStatusWatcherImpl \u00b6 LoggingPodStatusWatcherImpl is a LoggingPodStatusWatcher that monitors and logs the application status . Creating Instance \u00b6 LoggingPodStatusWatcherImpl takes the following to be created: KubernetesDriverConf LoggingPodStatusWatcherImpl is created when: KubernetesClientApplication is requested to start watchOrStop \u00b6 watchOrStop ( sId : String ) : Unit watchOrStop is part of the LoggingPodStatusWatcher abstraction. watchOrStop uses spark.kubernetes.submission.waitAppCompletion configuration property to control whether to wait for the Spark application to complete ( true ) or merely print out the following INFO message to the logs: Deployed Spark application [appName] with submission ID [sId] into Kubernetes While waiting for the Spark application to complete, watchOrStop prints out the following INFO message to the logs: Waiting for application [appName] with submission ID [sId] to finish... Until podCompleted flag is true , watchOrStop waits spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) Once podCompleted flag is true , watchOrStop prints out the following INFO messages to the logs: Container final statuses: [containersDescription] Application [appName] with submission ID [sId] finished When no pod is available, watchOrStop prints out the following INFO message to the logs: No containers were found in the driver pod. eventReceived \u00b6 eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes' Watcher abstraction. eventReceived brances off based on the given Action : For DELETED or ERROR actions, eventReceived closeWatch For any other actions, logLongStatus followed by closeWatch if hasCompleted . logLongStatus \u00b6 logLongStatus () : Unit logLongStatus prints out the following INFO message to the logs: State changed, new state: [formatPodState|unknown] hasCompleted \u00b6 hasCompleted () : Boolean hasCompleted is true when the phase is Succeeded or Failed . hasCompleted is used when: LoggingPodStatusWatcherImpl is requested to eventReceived (when an action is neither DELETED nor ERROR ) podCompleted Flag \u00b6 LoggingPodStatusWatcherImpl turns podCompleted off when created . Until podCompleted is on, LoggingPodStatusWatcherImpl waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) podCompleted turns podCompleted on when closeWatch . closeWatch \u00b6 closeWatch () : Unit closeWatch turns podCompleted on. closeWatch is used when: LoggingPodStatusWatcherImpl is requested to eventReceived and onClose Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL Refer to Logging .","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#loggingpodstatuswatcherimpl","text":"LoggingPodStatusWatcherImpl is a LoggingPodStatusWatcher that monitors and logs the application status .","title":"LoggingPodStatusWatcherImpl"},{"location":"LoggingPodStatusWatcherImpl/#creating-instance","text":"LoggingPodStatusWatcherImpl takes the following to be created: KubernetesDriverConf LoggingPodStatusWatcherImpl is created when: KubernetesClientApplication is requested to start","title":"Creating Instance"},{"location":"LoggingPodStatusWatcherImpl/#watchorstop","text":"watchOrStop ( sId : String ) : Unit watchOrStop is part of the LoggingPodStatusWatcher abstraction. watchOrStop uses spark.kubernetes.submission.waitAppCompletion configuration property to control whether to wait for the Spark application to complete ( true ) or merely print out the following INFO message to the logs: Deployed Spark application [appName] with submission ID [sId] into Kubernetes While waiting for the Spark application to complete, watchOrStop prints out the following INFO message to the logs: Waiting for application [appName] with submission ID [sId] to finish... Until podCompleted flag is true , watchOrStop waits spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) Once podCompleted flag is true , watchOrStop prints out the following INFO messages to the logs: Container final statuses: [containersDescription] Application [appName] with submission ID [sId] finished When no pod is available, watchOrStop prints out the following INFO message to the logs: No containers were found in the driver pod.","title":" watchOrStop"},{"location":"LoggingPodStatusWatcherImpl/#eventreceived","text":"eventReceived ( action : Action , pod : Pod ) : Unit eventReceived is part of the Kubernetes' Watcher abstraction. eventReceived brances off based on the given Action : For DELETED or ERROR actions, eventReceived closeWatch For any other actions, logLongStatus followed by closeWatch if hasCompleted .","title":" eventReceived"},{"location":"LoggingPodStatusWatcherImpl/#loglongstatus","text":"logLongStatus () : Unit logLongStatus prints out the following INFO message to the logs: State changed, new state: [formatPodState|unknown]","title":" logLongStatus"},{"location":"LoggingPodStatusWatcherImpl/#hascompleted","text":"hasCompleted () : Boolean hasCompleted is true when the phase is Succeeded or Failed . hasCompleted is used when: LoggingPodStatusWatcherImpl is requested to eventReceived (when an action is neither DELETED nor ERROR )","title":" hasCompleted"},{"location":"LoggingPodStatusWatcherImpl/#podcompleted-flag","text":"LoggingPodStatusWatcherImpl turns podCompleted off when created . Until podCompleted is on, LoggingPodStatusWatcherImpl waits the spark.kubernetes.report.interval configuration property and prints out the following INFO message to the logs: Application status for [appId] (phase: [phase]) podCompleted turns podCompleted on when closeWatch .","title":" podCompleted Flag"},{"location":"LoggingPodStatusWatcherImpl/#closewatch","text":"closeWatch () : Unit closeWatch turns podCompleted on. closeWatch is used when: LoggingPodStatusWatcherImpl is requested to eventReceived and onClose","title":" closeWatch"},{"location":"LoggingPodStatusWatcherImpl/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.submit.LoggingPodStatusWatcherImpl=ALL Refer to Logging .","title":"Logging"},{"location":"MountSecretsFeatureStep/","text":"MountSecretsFeatureStep \u00b6 MountSecretsFeatureStep is...FIXME","title":"MountSecretsFeatureStep"},{"location":"MountSecretsFeatureStep/#mountsecretsfeaturestep","text":"MountSecretsFeatureStep is...FIXME","title":"MountSecretsFeatureStep"},{"location":"MountVolumesFeatureStep/","text":"MountVolumesFeatureStep \u00b6 MountVolumesFeatureStep is a KubernetesFeatureConfigStep . Creating Instance \u00b6 MountVolumesFeatureStep takes the following to be created: KubernetesConf MountVolumesFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec Configuring Driver Pod \u00b6 configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod constructs the volumes and volume mounts from the volumes (of the KubernetesConf ) and creates a new SparkPod : Adds the volumes to the pod specification Adds the volume mounts to the container specification constructVolumes \u00b6 constructVolumes ( volumeSpecs : Iterable [ KubernetesVolumeSpec ]) : Iterable [( VolumeMount , Volume )] constructVolumes creates Kubernetes VolumeMount s and Volume s based on the given KubernetesVolumeSpec specs. VolumeMount s are built based on the following: mountPath mountReadOnly mountSubPath volumeName Volume s are build based on the type of the volume: HostPath PersistentVolumeClaim EmptyDir In the end, Volume s and VolumeMount s are wired together using volumeName .","title":"MountVolumesFeatureStep"},{"location":"MountVolumesFeatureStep/#mountvolumesfeaturestep","text":"MountVolumesFeatureStep is a KubernetesFeatureConfigStep .","title":"MountVolumesFeatureStep"},{"location":"MountVolumesFeatureStep/#creating-instance","text":"MountVolumesFeatureStep takes the following to be created: KubernetesConf MountVolumesFeatureStep is created when: KubernetesDriverBuilder is requested to build a driver pod spec KubernetesExecutorBuilder is requested to build an executor pod spec","title":"Creating Instance"},{"location":"MountVolumesFeatureStep/#configuring-driver-pod","text":"configurePod ( pod : SparkPod ) : SparkPod configurePod is part of the KubernetesFeatureConfigStep abstraction. configurePod constructs the volumes and volume mounts from the volumes (of the KubernetesConf ) and creates a new SparkPod : Adds the volumes to the pod specification Adds the volume mounts to the container specification","title":" Configuring Driver Pod"},{"location":"MountVolumesFeatureStep/#constructvolumes","text":"constructVolumes ( volumeSpecs : Iterable [ KubernetesVolumeSpec ]) : Iterable [( VolumeMount , Volume )] constructVolumes creates Kubernetes VolumeMount s and Volume s based on the given KubernetesVolumeSpec specs. VolumeMount s are built based on the following: mountPath mountReadOnly mountSubPath volumeName Volume s are build based on the type of the volume: HostPath PersistentVolumeClaim EmptyDir In the end, Volume s and VolumeMount s are wired together using volumeName .","title":" constructVolumes"},{"location":"PodTemplateConfigMapStep/","text":"PodTemplateConfigMapStep \u00b6 PodTemplateConfigMapStep is...FIXME","title":"PodTemplateConfigMapStep"},{"location":"PodTemplateConfigMapStep/#podtemplateconfigmapstep","text":"PodTemplateConfigMapStep is...FIXME","title":"PodTemplateConfigMapStep"},{"location":"PollRunnable/","text":"PollRunnable \u00b6 PollRunnable is a Java Runnable that ExecutorPodsPollingSnapshotSource uses to run regularly for current snapshots of the executor pods of the application . PollRunnable is an internal class of ExecutorPodsPollingSnapshotSource with full access to its internals. Creating Instance \u00b6 PollRunnable takes the following to be created: Application Id PollRunnable is created when: ExecutorPodsPollingSnapshotSource is requested to start Starting Thread \u00b6 run () : Unit run prints out the following DEBUG message to the logs: Resynchronizing full executor pod state from Kubernetes. run requests the KubernetesClient for Spark executor pods that are pods with the following labels and values: spark-app-selector as the application Id spark-role as executor In the end, run requests the ExecutorPodsSnapshotsStore to replace the snapshot . Logging \u00b6 PollRunnable uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.","title":"PollRunnable"},{"location":"PollRunnable/#pollrunnable","text":"PollRunnable is a Java Runnable that ExecutorPodsPollingSnapshotSource uses to run regularly for current snapshots of the executor pods of the application . PollRunnable is an internal class of ExecutorPodsPollingSnapshotSource with full access to its internals.","title":"PollRunnable"},{"location":"PollRunnable/#creating-instance","text":"PollRunnable takes the following to be created: Application Id PollRunnable is created when: ExecutorPodsPollingSnapshotSource is requested to start","title":"Creating Instance"},{"location":"PollRunnable/#starting-thread","text":"run () : Unit run prints out the following DEBUG message to the logs: Resynchronizing full executor pod state from Kubernetes. run requests the KubernetesClient for Spark executor pods that are pods with the following labels and values: spark-app-selector as the application Id spark-role as executor In the end, run requests the ExecutorPodsSnapshotsStore to replace the snapshot .","title":" Starting Thread"},{"location":"PollRunnable/#logging","text":"PollRunnable uses org.apache.spark.scheduler.cluster.k8s.ExecutorPodsPollingSnapshotSource logger for logging.","title":"Logging"},{"location":"SparkKubernetesClientFactory/","text":"SparkKubernetesClientFactory \u00b6 SparkKubernetesClientFactory is a Spark-opinionated builder for Kubernetes clients . Creating KubernetesClient \u00b6 createKubernetesClient ( master : String , namespace : Option [ String ], kubernetesAuthConfPrefix : String , clientType : ClientType.Value , sparkConf : SparkConf , defaultServiceAccountToken : Option [ File ], defaultServiceAccountCaCert : Option [ File ]) : KubernetesClient createKubernetesClient utility takes the OAuth token-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .oauthTokenFile (or defaults to the input defaultServiceAccountToken ) kubernetesAuthConfPrefix .oauthToken createKubernetesClient takes the spark.kubernetes.context configuraiton property ( kubeContext ). createKubernetesClient takes the certificate-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .caCertFile (or defaults to the input defaultServiceAccountCaCert ) kubernetesAuthConfPrefix .clientKeyFile kubernetesAuthConfPrefix .clientCertFile createKubernetesClient prints out the following INFO message to the logs: Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file createKubernetesClient builds a Kubernetes Config (based on the configuration properties). createKubernetesClient builds an OkHttpClient with a custom kubernetes-dispatcher dispatcher. In the end, createKubernetesClient creates a Kubernetes DefaultKubernetesClient (with the OkHttpClient and Config ). createKubernetesClient is used when: K8SSparkSubmitOperation is requested to execute KubernetesClientApplication is requested to start KubernetesClusterManager is requested to create a SchedulerBackend Exceptions \u00b6 createKubernetesClient throws an IllegalArgumentException when an OAuth token is specified through a file and a value: Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf]. Logging \u00b6 Enable ALL logging level for org.apache.spark.deploy.k8s.SparkKubernetesClientFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL Refer to Logging .","title":"SparkKubernetesClientFactory"},{"location":"SparkKubernetesClientFactory/#sparkkubernetesclientfactory","text":"SparkKubernetesClientFactory is a Spark-opinionated builder for Kubernetes clients .","title":"SparkKubernetesClientFactory"},{"location":"SparkKubernetesClientFactory/#creating-kubernetesclient","text":"createKubernetesClient ( master : String , namespace : Option [ String ], kubernetesAuthConfPrefix : String , clientType : ClientType.Value , sparkConf : SparkConf , defaultServiceAccountToken : Option [ File ], defaultServiceAccountCaCert : Option [ File ]) : KubernetesClient createKubernetesClient utility takes the OAuth token-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .oauthTokenFile (or defaults to the input defaultServiceAccountToken ) kubernetesAuthConfPrefix .oauthToken createKubernetesClient takes the spark.kubernetes.context configuraiton property ( kubeContext ). createKubernetesClient takes the certificate-related configuration properties from the input SparkConf : kubernetesAuthConfPrefix .caCertFile (or defaults to the input defaultServiceAccountCaCert ) kubernetesAuthConfPrefix .clientKeyFile kubernetesAuthConfPrefix .clientCertFile createKubernetesClient prints out the following INFO message to the logs: Auto-configuring K8S client using [context [kubeContext] | current context] from users K8S config file createKubernetesClient builds a Kubernetes Config (based on the configuration properties). createKubernetesClient builds an OkHttpClient with a custom kubernetes-dispatcher dispatcher. In the end, createKubernetesClient creates a Kubernetes DefaultKubernetesClient (with the OkHttpClient and Config ). createKubernetesClient is used when: K8SSparkSubmitOperation is requested to execute KubernetesClientApplication is requested to start KubernetesClusterManager is requested to create a SchedulerBackend","title":" Creating KubernetesClient"},{"location":"SparkKubernetesClientFactory/#exceptions","text":"createKubernetesClient throws an IllegalArgumentException when an OAuth token is specified through a file and a value: Cannot specify OAuth token through both a file [oauthTokenFileConf] and a value [oauthTokenConf].","title":"Exceptions"},{"location":"SparkKubernetesClientFactory/#logging","text":"Enable ALL logging level for org.apache.spark.deploy.k8s.SparkKubernetesClientFactory logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s.SparkKubernetesClientFactory=ALL Refer to Logging .","title":"Logging"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 spark.kubernetes.allocation.batch.delay \u00b6 Time (in millis) to wait between each round of executor allocation Default: 1s Used when: ExecutorPodsAllocator is created spark.kubernetes.allocation.batch.size \u00b6 Minimum number of executor pods to allocate at once in each round of executor allocation Default: 5 Used when: ExecutorPodsAllocator is created spark.kubernetes.authenticate \u00b6 FIXME spark.kubernetes.authenticate.driver.mounted \u00b6 FIXME spark.kubernetes.container.image \u00b6 Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined) Default: (undefined) spark.kubernetes.container.image.pullPolicy \u00b6 Kubernetes image pull policy: Always Never IfNotPresent Default: IfNotPresent Used when: KubernetesConf is requested for imagePullPolicy spark.kubernetes.context \u00b6 The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the KUBECONFIG environment variable or by defaulting to .kube/config under your home directory. If not specified then your current context is used. You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options. Default: (undefined) Used when: SparkKubernetesClientFactory is requested to create a KubernetesClient spark.kubernetes.driver.container.image \u00b6 Container image for drivers Default: spark.kubernetes.container.image Used when: BasicDriverFeatureStep is requested for a driverContainerImage spark.kubernetes.driver.master \u00b6 The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc spark.kubernetes.driver.pod.name \u00b6 Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed using spark-submit in cluster mode spark.kubernetes.driver.podTemplateContainerName \u00b6 Container name for a driver in the given pod template Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures spark.kubernetes.driver.podTemplateFile \u00b6 Pod template file for drivers Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures spark.kubernetes.driver.request.cores \u00b6 Specify the cpu request for the driver pod Default: (undefined) Used when: BasicDriverFeatureStep is requested to configure a pod spark.kubernetes.executor.apiPollingInterval \u00b6 Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors. Default: 30s Used when: ExecutorPodsPollingSnapshotSource is requested to start spark.kubernetes.executor.container.image \u00b6 Container image for executors Default: spark.kubernetes.container.image Used when: BasicExecutorFeatureStep is requested for a driverContainerImage spark.kubernetes.executor.deleteOnTermination \u00b6 When disabled ( false ), executor pods will not be deleted in case of failure or normal termination Default: true Used when: ExecutorPodsAllocator is requested to onNewSnapshots ExecutorPodsLifecycleManager is requested to onFinalNonDeletedState KubernetesClusterSchedulerBackend is requested to stop spark.kubernetes.executor.eventProcessingInterval \u00b6 Interval (in millis) between successive inspection of executor events sent from the Kubernetes API Default: 1s Used when: ExecutorPodsLifecycleManager is requested to start and register a new subscriber spark.kubernetes.executor.podNamePrefix \u00b6 (internal) Prefix to use in front of the executor pod names Default: (undefined) Unless defined, it is set explicitly when KubernetesClusterManager is requested to create a SchedulerBackend Used when: KubernetesExecutorConf is requested for the resourceNamePrefix spark.kubernetes.executor.podTemplateContainerName \u00b6 Container name to be used as a basis for executors in the given pod template Default: (undefined) Used when: KubernetesClusterManager is requested to create a SchedulerBackend KubernetesExecutorBuilder is requested to build an executor pod from features spark.kubernetes.executor.podTemplateFile \u00b6 Pod template file for executors Default: (undefined) Used when: KubernetesClusterManager is requested to create a SchedulerBackend KubernetesExecutorBuilder is requested to build a SparkPod from features PodTemplateConfigMapStep is created and requested to configurePod , getAdditionalPodSystemProperties , getAdditionalKubernetesResources spark.kubernetes.file.upload.path \u00b6 Hadoop DFS-compatible file system path where files from the local file system will be uploded to in cluster deploy mode. The subdirectories (one per Spark application) with the local files are of the format spark-upload-[uuid] . Default: (undefined) Used when: KubernetesUtils is requested to uploadFileUri spark.kubernetes.local.dirs.tmpfs \u00b6 If true , emptyDir volumes created to back SPARK_LOCAL_DIRS will have their medium set to Memory so that they will be created as tmpfs (i.e. RAM) backed volumes. This may improve performance but scratch space usage will count towards your pods memory limit so you may wish to request more memory. Default: false Used when: LocalDirsFeatureStep is requested to configure a pod spark.kubernetes.memoryOverheadFactor \u00b6 Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs Must be a double between (0, 1.0) Default: 0.1 Used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod spark.kubernetes.namespace \u00b6 Namespace for running the driver and executor pods Default: default Used when: KubernetesConf is requested for namespace KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsAllocator is created (and initializes namespace ) spark.kubernetes.report.interval \u00b6 Interval between reports of the current app status in cluster mode Default: 1s Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop spark.kubernetes.submission.waitAppCompletion \u00b6 In cluster deploy mode, whether to wait for the application to finish before exiting the launcher process. Default: true Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop spark.kubernetes.submitInDriver \u00b6 (internal) Whether executing in cluster deploy mode Default: false spark.kubernetes.submitInDriver is true in BasicDriverFeatureStep . Used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties KubernetesClusterManager is requested for a SchedulerBackend","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkkubernetesallocationbatchdelay","text":"Time (in millis) to wait between each round of executor allocation Default: 1s Used when: ExecutorPodsAllocator is created","title":" spark.kubernetes.allocation.batch.delay"},{"location":"configuration-properties/#sparkkubernetesallocationbatchsize","text":"Minimum number of executor pods to allocate at once in each round of executor allocation Default: 5 Used when: ExecutorPodsAllocator is created","title":" spark.kubernetes.allocation.batch.size"},{"location":"configuration-properties/#sparkkubernetesauthenticate","text":"FIXME","title":" spark.kubernetes.authenticate"},{"location":"configuration-properties/#sparkkubernetesauthenticatedrivermounted","text":"FIXME","title":" spark.kubernetes.authenticate.driver.mounted"},{"location":"configuration-properties/#sparkkubernetescontainerimage","text":"Container image to use for Spark containers (unless spark.kubernetes.driver.container.image or spark.kubernetes.executor.container.image are defined) Default: (undefined)","title":" spark.kubernetes.container.image"},{"location":"configuration-properties/#sparkkubernetescontainerimagepullpolicy","text":"Kubernetes image pull policy: Always Never IfNotPresent Default: IfNotPresent Used when: KubernetesConf is requested for imagePullPolicy","title":" spark.kubernetes.container.image.pullPolicy"},{"location":"configuration-properties/#sparkkubernetescontext","text":"The desired context from your K8S config file used to configure the K8S client for interacting with the cluster. Useful if your config file has multiple clusters or user identities defined. The client library used locates the config file via the KUBECONFIG environment variable or by defaulting to .kube/config under your home directory. If not specified then your current context is used. You can always override specific aspects of the config file provided configuration using other Spark on K8S configuration options. Default: (undefined) Used when: SparkKubernetesClientFactory is requested to create a KubernetesClient","title":" spark.kubernetes.context"},{"location":"configuration-properties/#sparkkubernetesdrivercontainerimage","text":"Container image for drivers Default: spark.kubernetes.container.image Used when: BasicDriverFeatureStep is requested for a driverContainerImage","title":" spark.kubernetes.driver.container.image"},{"location":"configuration-properties/#sparkkubernetesdrivermaster","text":"The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc","title":" spark.kubernetes.driver.master"},{"location":"configuration-properties/#sparkkubernetesdriverpodname","text":"Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed using spark-submit in cluster mode","title":" spark.kubernetes.driver.pod.name"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatecontainername","text":"Container name for a driver in the given pod template Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures","title":" spark.kubernetes.driver.podTemplateContainerName"},{"location":"configuration-properties/#sparkkubernetesdriverpodtemplatefile","text":"Pod template file for drivers Default: (undefined) Used when: KubernetesDriverBuilder is requested to buildFromFeatures","title":" spark.kubernetes.driver.podTemplateFile"},{"location":"configuration-properties/#sparkkubernetesdriverrequestcores","text":"Specify the cpu request for the driver pod Default: (undefined) Used when: BasicDriverFeatureStep is requested to configure a pod","title":" spark.kubernetes.driver.request.cores"},{"location":"configuration-properties/#sparkkubernetesexecutorapipollinginterval","text":"Interval (in millis) between polls against the Kubernetes API server to inspect the state of executors. Default: 30s Used when: ExecutorPodsPollingSnapshotSource is requested to start","title":" spark.kubernetes.executor.apiPollingInterval"},{"location":"configuration-properties/#sparkkubernetesexecutorcontainerimage","text":"Container image for executors Default: spark.kubernetes.container.image Used when: BasicExecutorFeatureStep is requested for a driverContainerImage","title":" spark.kubernetes.executor.container.image"},{"location":"configuration-properties/#sparkkubernetesexecutordeleteontermination","text":"When disabled ( false ), executor pods will not be deleted in case of failure or normal termination Default: true Used when: ExecutorPodsAllocator is requested to onNewSnapshots ExecutorPodsLifecycleManager is requested to onFinalNonDeletedState KubernetesClusterSchedulerBackend is requested to stop","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"configuration-properties/#sparkkubernetesexecutoreventprocessinginterval","text":"Interval (in millis) between successive inspection of executor events sent from the Kubernetes API Default: 1s Used when: ExecutorPodsLifecycleManager is requested to start and register a new subscriber","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"configuration-properties/#sparkkubernetesexecutorpodnameprefix","text":"(internal) Prefix to use in front of the executor pod names Default: (undefined) Unless defined, it is set explicitly when KubernetesClusterManager is requested to create a SchedulerBackend Used when: KubernetesExecutorConf is requested for the resourceNamePrefix","title":" spark.kubernetes.executor.podNamePrefix"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatecontainername","text":"Container name to be used as a basis for executors in the given pod template Default: (undefined) Used when: KubernetesClusterManager is requested to create a SchedulerBackend KubernetesExecutorBuilder is requested to build an executor pod from features","title":" spark.kubernetes.executor.podTemplateContainerName"},{"location":"configuration-properties/#sparkkubernetesexecutorpodtemplatefile","text":"Pod template file for executors Default: (undefined) Used when: KubernetesClusterManager is requested to create a SchedulerBackend KubernetesExecutorBuilder is requested to build a SparkPod from features PodTemplateConfigMapStep is created and requested to configurePod , getAdditionalPodSystemProperties , getAdditionalKubernetesResources","title":" spark.kubernetes.executor.podTemplateFile"},{"location":"configuration-properties/#sparkkubernetesfileuploadpath","text":"Hadoop DFS-compatible file system path where files from the local file system will be uploded to in cluster deploy mode. The subdirectories (one per Spark application) with the local files are of the format spark-upload-[uuid] . Default: (undefined) Used when: KubernetesUtils is requested to uploadFileUri","title":" spark.kubernetes.file.upload.path"},{"location":"configuration-properties/#sparkkuberneteslocaldirstmpfs","text":"If true , emptyDir volumes created to back SPARK_LOCAL_DIRS will have their medium set to Memory so that they will be created as tmpfs (i.e. RAM) backed volumes. This may improve performance but scratch space usage will count towards your pods memory limit so you may wish to request more memory. Default: false Used when: LocalDirsFeatureStep is requested to configure a pod","title":" spark.kubernetes.local.dirs.tmpfs"},{"location":"configuration-properties/#sparkkubernetesmemoryoverheadfactor","text":"Memory Overhead Factor that will allocate memory to non-JVM jobs which in the case of JVM tasks will default to 0.10 and 0.40 for non-JVM jobs Must be a double between (0, 1.0) Default: 0.1 Used when: BasicDriverFeatureStep is requested to configure a pod BasicExecutorFeatureStep is requested to configure a pod","title":" spark.kubernetes.memoryOverheadFactor"},{"location":"configuration-properties/#sparkkubernetesnamespace","text":"Namespace for running the driver and executor pods Default: default Used when: KubernetesConf is requested for namespace KubernetesClusterManager is requested for a SchedulerBackend ExecutorPodsAllocator is created (and initializes namespace )","title":" spark.kubernetes.namespace"},{"location":"configuration-properties/#sparkkubernetesreportinterval","text":"Interval between reports of the current app status in cluster mode Default: 1s Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop","title":" spark.kubernetes.report.interval"},{"location":"configuration-properties/#sparkkubernetessubmissionwaitappcompletion","text":"In cluster deploy mode, whether to wait for the application to finish before exiting the launcher process. Default: true Used when: LoggingPodStatusWatcherImpl is requested to watchOrStop","title":" spark.kubernetes.submission.waitAppCompletion"},{"location":"configuration-properties/#sparkkubernetessubmitindriver","text":"(internal) Whether executing in cluster deploy mode Default: false spark.kubernetes.submitInDriver is true in BasicDriverFeatureStep . Used when: BasicDriverFeatureStep is requested to getAdditionalPodSystemProperties KubernetesClusterManager is requested for a SchedulerBackend","title":" spark.kubernetes.submitInDriver"},{"location":"overview/","text":"Spark on Kubernetes \u00b6 Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Apache Spark supports Kubernetes resource manager as a scheduler using KubernetesClusterManager and KubernetesClusterSchedulerBackend for k8s:// -prefixed master URLs (that point at Kubernetes API servers ). Kubernetes GA in Spark 3.1.1 \u00b6 As per SPARK-33005 Kubernetes GA Preparation , Spark 3.1.1 comes with many improvements for Kubernetes support and is expected to get General Availability (GA) marker \ud83c\udf89 Note There will never be 3.1.0. Volumes \u00b6 Volumes and volume mounts are configured using spark.kubernetes.[type].volumes. -prefixed configuration properties with type being driver or executor (for the driver and executor pods, respectively). KubernetesVolumeUtils utility is used to extract volume configuration based on the volume type: Volume Type Configuration Property emptyDir [volumesPrefix].[volumeType].[volumeName].options.medium [volumesPrefix].[volumeType].[volumeName].options.sizeLimit hostPath [volumesPrefix].[volumeType].[volumeName].options.path persistentVolumeClaim [volumesPrefix].[volumeType].[volumeName].options.claimName Executor volumes ( spark.kubernetes.executor.volumes. -prefixed configuration properties) are parsed right when KubernetesConf utility is used for a KubernetesDriverConf (and a driver pod created). That makes executor volumes required when driver volumes are defined. Static File Resources \u00b6 File resources are resources with file or no URI scheme (that are then considered file -based indirectly). In Spark applications, file resources can be the main application jar and pyspark or R files ( primary resource ) as well as files referenced by spark.jars and spark.files configuration properties (or their --jars and --files options of spark-submit , respectively). When deployed in cluster mode, Spark on Kubernetes uploads file resources of a Spark application to a Hadoop DFS-compatible file system defined by the required spark.kubernetes.file.upload.path configuration property. Local URI Scheme \u00b6 A special case of static file resources are local resources that are resources with local URI scheme. They are considered already available on every Spark node (and are not added to a Spark file server for distribution when SparkContext is requested to add such file ). In Spark on Kubernetes, local resources are used for primary application resource that are already included in a container image. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Executor Pods State Synchronization \u00b6 Spark on Kubernetes uses ExecutorPodsPollingSnapshotSource for polling Kubernetes API server for executor pods state snapshot of a Spark application every polling interval (based on spark.kubernetes.executor.apiPollingInterval configuration property). ExecutorPodsPollingSnapshotSource is given an ExecutorPodsSnapshotsStore that is requested to replaceSnapshot regularly. ExecutorPodsSnapshotsStore keeps track of executor pods state snapshots and allows subscribers to be regularly updated (e.g. ExecutorPodsAllocator and ExecutorPodsLifecycleManager ). Dynamic Allocation of Executors \u00b6 Spark on Kubernetes supports Dynamic Allocation of Executors using ExecutorPodsAllocator . The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark . Internal Resource Marker \u00b6 Spark on Kubernetes uses spark-internal special name in cluster deploy mode for internal application resources (that are supposed to be part of an image). Given renameMainAppResource , DriverCommandFeatureStep will re-write local file -scheme-based primary application resources to spark-internal special name when requested for the base driver container (for a JavaMainAppResource application). Demo \u00b6 This demo is a follow-up to Demo: Running Spark Application on minikube . Run it first. Note --deploy-mode cluster and the application jar is \"locally resolvable\" (i.e. uses file: scheme indirectly). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --verbose \\ ~/dev/meetups/spark-meetup/spark-docker-example/target/scala-2.12/spark-docker-example_2.12-0.1.0.jar $ kubectl get po -l spark-role=driver NAME READY STATUS RESTARTS AGE spark-docker-example-dfd7d076e7099718-driver 0/1 Error 0 7m25s Note spark-internal in the below output. $ kubectl describe po spark-docker-example-dfd7d076e7099718-driver ... Containers: spark-kubernetes-driver: ... Args: driver --properties-file /opt/spark/conf/spark.properties --class meetup.SparkApp spark-internal ... Demo \u00b6 spark-shell on minikube Running Spark Application on minikube Resources \u00b6 Official documentation Spark on Kubernetes by Scott Haines (video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert","title":"Spark on Kubernetes"},{"location":"overview/#spark-on-kubernetes","text":"Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Apache Spark supports Kubernetes resource manager as a scheduler using KubernetesClusterManager and KubernetesClusterSchedulerBackend for k8s:// -prefixed master URLs (that point at Kubernetes API servers ).","title":"Spark on Kubernetes"},{"location":"overview/#kubernetes-ga-in-spark-311","text":"As per SPARK-33005 Kubernetes GA Preparation , Spark 3.1.1 comes with many improvements for Kubernetes support and is expected to get General Availability (GA) marker \ud83c\udf89 Note There will never be 3.1.0.","title":"Kubernetes GA in Spark 3.1.1"},{"location":"overview/#volumes","text":"Volumes and volume mounts are configured using spark.kubernetes.[type].volumes. -prefixed configuration properties with type being driver or executor (for the driver and executor pods, respectively). KubernetesVolumeUtils utility is used to extract volume configuration based on the volume type: Volume Type Configuration Property emptyDir [volumesPrefix].[volumeType].[volumeName].options.medium [volumesPrefix].[volumeType].[volumeName].options.sizeLimit hostPath [volumesPrefix].[volumeType].[volumeName].options.path persistentVolumeClaim [volumesPrefix].[volumeType].[volumeName].options.claimName Executor volumes ( spark.kubernetes.executor.volumes. -prefixed configuration properties) are parsed right when KubernetesConf utility is used for a KubernetesDriverConf (and a driver pod created). That makes executor volumes required when driver volumes are defined.","title":"Volumes"},{"location":"overview/#static-file-resources","text":"File resources are resources with file or no URI scheme (that are then considered file -based indirectly). In Spark applications, file resources can be the main application jar and pyspark or R files ( primary resource ) as well as files referenced by spark.jars and spark.files configuration properties (or their --jars and --files options of spark-submit , respectively). When deployed in cluster mode, Spark on Kubernetes uploads file resources of a Spark application to a Hadoop DFS-compatible file system defined by the required spark.kubernetes.file.upload.path configuration property.","title":"Static File Resources"},{"location":"overview/#local-uri-scheme","text":"A special case of static file resources are local resources that are resources with local URI scheme. They are considered already available on every Spark node (and are not added to a Spark file server for distribution when SparkContext is requested to add such file ). In Spark on Kubernetes, local resources are used for primary application resource that are already included in a container image. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar","title":"Local URI Scheme"},{"location":"overview/#executor-pods-state-synchronization","text":"Spark on Kubernetes uses ExecutorPodsPollingSnapshotSource for polling Kubernetes API server for executor pods state snapshot of a Spark application every polling interval (based on spark.kubernetes.executor.apiPollingInterval configuration property). ExecutorPodsPollingSnapshotSource is given an ExecutorPodsSnapshotsStore that is requested to replaceSnapshot regularly. ExecutorPodsSnapshotsStore keeps track of executor pods state snapshots and allows subscribers to be regularly updated (e.g. ExecutorPodsAllocator and ExecutorPodsLifecycleManager ).","title":"Executor Pods State Synchronization"},{"location":"overview/#dynamic-allocation-of-executors","text":"Spark on Kubernetes supports Dynamic Allocation of Executors using ExecutorPodsAllocator . The Internals of Apache Spark Learn more about Dynamic Allocation of Executors in The Internals of Apache Spark .","title":"Dynamic Allocation of Executors"},{"location":"overview/#internal-resource-marker","text":"Spark on Kubernetes uses spark-internal special name in cluster deploy mode for internal application resources (that are supposed to be part of an image). Given renameMainAppResource , DriverCommandFeatureStep will re-write local file -scheme-based primary application resources to spark-internal special name when requested for the base driver container (for a JavaMainAppResource application).","title":" Internal Resource Marker"},{"location":"overview/#demo","text":"This demo is a follow-up to Demo: Running Spark Application on minikube . Run it first. Note --deploy-mode cluster and the application jar is \"locally resolvable\" (i.e. uses file: scheme indirectly). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --verbose \\ ~/dev/meetups/spark-meetup/spark-docker-example/target/scala-2.12/spark-docker-example_2.12-0.1.0.jar $ kubectl get po -l spark-role=driver NAME READY STATUS RESTARTS AGE spark-docker-example-dfd7d076e7099718-driver 0/1 Error 0 7m25s Note spark-internal in the below output. $ kubectl describe po spark-docker-example-dfd7d076e7099718-driver ... Containers: spark-kubernetes-driver: ... Args: driver --properties-file /opt/spark/conf/spark.properties --class meetup.SparkApp spark-internal ...","title":" Demo"},{"location":"overview/#demo_1","text":"spark-shell on minikube Running Spark Application on minikube","title":"Demo"},{"location":"overview/#resources","text":"Official documentation Spark on Kubernetes by Scott Haines (video) Getting Started with Apache Spark on Kubernetes by Jean-Yves Stephan and Julien Dumazert","title":"Resources"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: spark-shell on minikube Running Spark Application on minikube Spark and Local Filesystem in minikube","title":"Demos"},{"location":"demo/#demos","text":"The following demos are available: spark-shell on minikube Running Spark Application on minikube Spark and Local Filesystem in minikube","title":"Demos"},{"location":"demo/running-spark-application-on-minikube/","text":"Demo: Running Spark Application on minikube \u00b6 This demo shows how to deploy a Spark application to Kubernetes (using minikube ). Tip Start with Demo: spark-shell on minikube . Note k is an alias of kubectl . Start Cluster \u00b6 Quoting Prerequisites : We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor. Unless already started, start minikube with enough resources. minikube start --cpus 4 --memory 8192 Building Spark Application Image \u00b6 Note that the image the Spark application project's image extends from (using FROM command) should be jaceklaskowski/spark:v3.0.1 as follows: FROM jaceklaskowski/spark:v3.0.1 Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) In your Spark application project execute the command to build and push a Docker image to minikube's Docker repository. sbt clean docker:publishLocal List available images (that should include your Spark application project's docker image, e.g. spark-docker-example ). $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE spark-docker-example 0.1.0 41fdb7a71b62 4 seconds ago 510MB jaceklaskowski/spark v3.0.1 d045e9e4572b About an hour ago 504MB openjdk 11-jre-slim 57a8cfbe60f3 4 weeks ago 205MB kubernetesui/dashboard v2.1.0 9a07b5b4bfac 4 weeks ago 226MB k8s.gcr.io/kube-proxy v1.20.0 10cc881966cf 4 weeks ago 118MB k8s.gcr.io/kube-controller-manager v1.20.0 b9fa1895dcaa 4 weeks ago 116MB k8s.gcr.io/kube-scheduler v1.20.0 3138b6e3d471 4 weeks ago 46.4MB k8s.gcr.io/kube-apiserver v1.20.0 ca9843d3b545 4 weeks ago 122MB gcr.io/k8s-minikube/storage-provisioner v4 85069258b98a 5 weeks ago 29.7MB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 4 months ago 253MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 9 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 11 months ago 683kB Creating Namespace \u00b6 This step is optional, but gives a better exposure to the Kubernetes-related features supported by Apache Spark. k create namespace spark-demo Tip Use kubens (from kubectx project) to switch between Kubernetes namespaces smoothly. kubens spark-demo Create Service Account \u00b6 Create a service account spark and a cluster role binding spark-role . Tip Learn more from the Spark official documentation . Without this step you could face the following exception message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. Declaratively \u00b6 Use the following rbac.yml file. --- apiVersion: v1 kind: ServiceAccount metadata: name: spark namespace: spark-demo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-role namespace: spark-demo subjects: - kind: ServiceAccount name: spark namespace: spark-demo roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io --- Create the resources in the Kubernetes cluster. k create -f rbac.yml Tip With declarative approach (using rbac.yml ) cleaning up becomes as simple as kubectl delete -f rbac.yml . Imperatively \u00b6 k create serviceaccount spark k create clusterrolebinding spark-role \\ --clusterrole edit \\ --serviceaccount spark-demo:spark Submitting Spark Application to minikube \u00b6 cd $SPARK_HOME K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') Please note the configuration properties (some not really necessary but make the demo easier to guide you through, e.g. spark.kubernetes.driver.pod.name ). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar After a few seconds, you should see the following messages: 20/12/14 18:34:59 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Pending) 20/12/14 18:34:59 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Running container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: running container started at: 2020-12-14T17:34:59Z 20/12/14 18:35:00 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Running) And then... 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Succeeded) 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Container final statuses: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed Accessing web UI \u00b6 Find the driver pod ( k get po ) k port-forward spark-demo-minikube 4040:4040 Accessing Logs \u00b6 Access the logs of the driver. k logs -f spark-demo-minikube Reviewing Spark Application Configuration (ConfigMap) \u00b6 k get cm k describe cm [driverPod]-conf-map Describe the driver pod and review volumes ( .spec.volumes ) and volume mounts ( .spec.containers[].volumeMounts ). k describe po spark-demo-minikube $ k get po spark-demo-minikube -o=jsonpath='{.spec.volumes}' | jq [ { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-f76bf776ec818be5-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-24krm\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-24krm\" } } ] $ k get po spark-demo-minikube -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/var/data/spark-b5d0a070-ff9a-41a3-91aa-82059ceba5b0\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-24krm\", \"readOnly\": true } ] Spark Application Management \u00b6 K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') $ ./bin/spark-submit --status \"spark-demo:spark-docker-example-*\" --master k8s://$K8S_SERVER ... Application status (driver): pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed Listing Services \u00b6 $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE spark-docker-example-3de43976e3a46fcf-driver-svc ClusterIP None <none> 7078/TCP,7079/TCP,4040/TCP 101s Stopping Cluster \u00b6 minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all","title":"Running Spark Application on minikube"},{"location":"demo/running-spark-application-on-minikube/#demo-running-spark-application-on-minikube","text":"This demo shows how to deploy a Spark application to Kubernetes (using minikube ). Tip Start with Demo: spark-shell on minikube . Note k is an alias of kubectl .","title":"Demo: Running Spark Application on minikube"},{"location":"demo/running-spark-application-on-minikube/#start-cluster","text":"Quoting Prerequisites : We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor. Unless already started, start minikube with enough resources. minikube start --cpus 4 --memory 8192","title":"Start Cluster"},{"location":"demo/running-spark-application-on-minikube/#building-spark-application-image","text":"Note that the image the Spark application project's image extends from (using FROM command) should be jaceklaskowski/spark:v3.0.1 as follows: FROM jaceklaskowski/spark:v3.0.1 Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) In your Spark application project execute the command to build and push a Docker image to minikube's Docker repository. sbt clean docker:publishLocal List available images (that should include your Spark application project's docker image, e.g. spark-docker-example ). $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE spark-docker-example 0.1.0 41fdb7a71b62 4 seconds ago 510MB jaceklaskowski/spark v3.0.1 d045e9e4572b About an hour ago 504MB openjdk 11-jre-slim 57a8cfbe60f3 4 weeks ago 205MB kubernetesui/dashboard v2.1.0 9a07b5b4bfac 4 weeks ago 226MB k8s.gcr.io/kube-proxy v1.20.0 10cc881966cf 4 weeks ago 118MB k8s.gcr.io/kube-controller-manager v1.20.0 b9fa1895dcaa 4 weeks ago 116MB k8s.gcr.io/kube-scheduler v1.20.0 3138b6e3d471 4 weeks ago 46.4MB k8s.gcr.io/kube-apiserver v1.20.0 ca9843d3b545 4 weeks ago 122MB gcr.io/k8s-minikube/storage-provisioner v4 85069258b98a 5 weeks ago 29.7MB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 4 months ago 253MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 9 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 11 months ago 683kB","title":"Building Spark Application Image"},{"location":"demo/running-spark-application-on-minikube/#creating-namespace","text":"This step is optional, but gives a better exposure to the Kubernetes-related features supported by Apache Spark. k create namespace spark-demo Tip Use kubens (from kubectx project) to switch between Kubernetes namespaces smoothly. kubens spark-demo","title":"Creating Namespace"},{"location":"demo/running-spark-application-on-minikube/#create-service-account","text":"Create a service account spark and a cluster role binding spark-role . Tip Learn more from the Spark official documentation . Without this step you could face the following exception message: Forbidden!Configured service account doesn't have access. Service account may have been revoked.","title":"Create Service Account"},{"location":"demo/running-spark-application-on-minikube/#declaratively","text":"Use the following rbac.yml file. --- apiVersion: v1 kind: ServiceAccount metadata: name: spark namespace: spark-demo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-role namespace: spark-demo subjects: - kind: ServiceAccount name: spark namespace: spark-demo roleRef: kind: ClusterRole name: edit apiGroup: rbac.authorization.k8s.io --- Create the resources in the Kubernetes cluster. k create -f rbac.yml Tip With declarative approach (using rbac.yml ) cleaning up becomes as simple as kubectl delete -f rbac.yml .","title":"Declaratively"},{"location":"demo/running-spark-application-on-minikube/#imperatively","text":"k create serviceaccount spark k create clusterrolebinding spark-role \\ --clusterrole edit \\ --serviceaccount spark-demo:spark","title":"Imperatively"},{"location":"demo/running-spark-application-on-minikube/#submitting-spark-application-to-minikube","text":"cd $SPARK_HOME K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') Please note the configuration properties (some not really necessary but make the demo easier to guide you through, e.g. spark.kubernetes.driver.pod.name ). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar After a few seconds, you should see the following messages: 20/12/14 18:34:59 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Pending) 20/12/14 18:34:59 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Running container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: running container started at: 2020-12-14T17:34:59Z 20/12/14 18:35:00 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Running) And then... 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: State changed, new state: pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Application status for spark-b1f8840227074b62996f66b915044ee6 (phase: Succeeded) 20/12/14 18:35:06 INFO LoggingPodStatusWatcherImpl: Container final statuses: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed","title":"Submitting Spark Application to minikube"},{"location":"demo/running-spark-application-on-minikube/#accessing-web-ui","text":"Find the driver pod ( k get po ) k port-forward spark-demo-minikube 4040:4040","title":"Accessing web UI"},{"location":"demo/running-spark-application-on-minikube/#accessing-logs","text":"Access the logs of the driver. k logs -f spark-demo-minikube","title":"Accessing Logs"},{"location":"demo/running-spark-application-on-minikube/#reviewing-spark-application-configuration-configmap","text":"k get cm k describe cm [driverPod]-conf-map Describe the driver pod and review volumes ( .spec.volumes ) and volume mounts ( .spec.containers[].volumeMounts ). k describe po spark-demo-minikube $ k get po spark-demo-minikube -o=jsonpath='{.spec.volumes}' | jq [ { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-f76bf776ec818be5-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-24krm\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-24krm\" } } ] $ k get po spark-demo-minikube -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/var/data/spark-b5d0a070-ff9a-41a3-91aa-82059ceba5b0\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-24krm\", \"readOnly\": true } ]","title":"Reviewing Spark Application Configuration (ConfigMap)"},{"location":"demo/running-spark-application-on-minikube/#spark-application-management","text":"K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') $ ./bin/spark-submit --status \"spark-demo:spark-docker-example-*\" --master k8s://$K8S_SERVER ... Application status (driver): pod name: spark-docker-example-3c07aa766251ce43-driver namespace: spark-demo labels: spark-app-selector -> spark-b1f8840227074b62996f66b915044ee6, spark-role -> driver pod uid: a8c06d26-ad8a-4b78-96e1-3e0be00a4da8 creation time: 2020-12-14T17:34:58Z service account name: spark volumes: spark-local-dir-1, spark-conf-volume, spark-token-tsd97 node name: minikube start time: 2020-12-14T17:34:58Z phase: Succeeded container status: container name: spark-kubernetes-driver container image: spark-docker-example:0.1.0 container state: terminated container started at: 2020-12-14T17:34:59Z container finished at: 2020-12-14T17:35:05Z exit code: 0 termination reason: Completed","title":"Spark Application Management"},{"location":"demo/running-spark-application-on-minikube/#listing-services","text":"$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE spark-docker-example-3de43976e3a46fcf-driver-svc ClusterIP None <none> 7078/TCP,7079/TCP,4040/TCP 101s","title":"Listing Services"},{"location":"demo/running-spark-application-on-minikube/#stopping-cluster","text":"minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all","title":"Stopping Cluster"},{"location":"demo/spark-and-local-filesystem-in-minikube/","text":"Demo: Spark and Local Filesystem in minikube \u00b6 The motivation of the demo is to set up a Spark application deployed to minikube to access files on a local filesystem. Tip Start with Demo: Running Spark Application on minikube . The demo uses spark-submit --files and spark.kubernetes.file.upload.path configuration property to upload a static file to a directory that is then mounted to Spark application pods. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Mounting Filesystems \u00b6 Let's start off by mounting a host directory ( /tmp/spark-k8s ) to minikube. Quoting Mounting filesystems of minikube's official documentation: To mount a directory from the host into the guest using the mount subcommand $ minikube mount /tmp/spark-k8s:/tmp/spark-k8s \ud83d\udcc1 Mounting host path /tmp/spark-k8s into VM as /tmp/spark-k8s ... \u25aa Mount type: \u25aa User ID: docker \u25aa Group ID: docker \u25aa Version: 9p2000.L \u25aa Message Size: 262144 \u25aa Permissions: 755 (-rwxr-xr-x) \u25aa Options: map[] \u25aa Bind Address: 127.0.0.1:53125 \ud83d\ude80 Userspace file server: ufs starting \u2705 Successfully mounted /tmp/spark-k8s to /tmp/spark-k8s \ud83d\udccc NOTE: This process must stay alive for the mount to be accessible ... Using Kubernetes Volumes \u00b6 Quoting Using Kubernetes Volumes of Apache Spark's official documentation: users can mount the following types of Kubernetes volumes into the driver and executor pods: hostPath: mounts a file or directory from the host node\u2019s filesystem into a pod. emptyDir: an initially empty volume created when a pod is assigned to a node. persistentVolumeClaim: used to mount a PersistentVolume into a pod. hostPath \u00b6 Let's use Kubernetes' hostPath that requires spark.kubernetes.*.volumes -prefixed configuration properties and the name of the volume (under the volumes field in the pod specification): --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s The demo uses configuration properties to set up a hostPath volume type with spark-k8s name and /tmp/spark-k8s path on the host (for the driver and executors separately). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar Reviewing Volumes \u00b6 k describe po spark-demo-minikube $ k get po spark-demo-minikube -o=jsonpath='{.spec.volumes}' | jq [ { \"hostPath\": { \"path\": \"/tmp/spark-k8s\", \"type\": \"\" }, \"name\": \"spark-k8s\" }, { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-85c0cb76f277cdbe-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-zd6jt\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-zd6jt\" } } ] $ k get po spark-demo-minikube -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/tmp/spark-k8s\", \"name\": \"spark-k8s\" }, { \"mountPath\": \"/var/data/spark-87d1ba7c-819a-4274-82ca-98c1e135c136\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-zd6jt\", \"readOnly\": true } ] Log into the driver pod container and review /tmp/spark-k8s directory. There should be at least one spark-upload directory. $ k exec -it spark-demo-minikube -- bash $ ls -ltr /tmp/spark-k8s total 5 ... drwxr-xr-x 1 1000 999 96 Jan 11 17:21 spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08 $ cat /tmp/spark-k8s/spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08/test.me Hello World","title":"Spark and Local Filesystem in minikube"},{"location":"demo/spark-and-local-filesystem-in-minikube/#demo-spark-and-local-filesystem-in-minikube","text":"The motivation of the demo is to set up a Spark application deployed to minikube to access files on a local filesystem. Tip Start with Demo: Running Spark Application on minikube . The demo uses spark-submit --files and spark.kubernetes.file.upload.path configuration property to upload a static file to a directory that is then mounted to Spark application pods. ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar","title":"Demo: Spark and Local Filesystem in minikube"},{"location":"demo/spark-and-local-filesystem-in-minikube/#mounting-filesystems","text":"Let's start off by mounting a host directory ( /tmp/spark-k8s ) to minikube. Quoting Mounting filesystems of minikube's official documentation: To mount a directory from the host into the guest using the mount subcommand $ minikube mount /tmp/spark-k8s:/tmp/spark-k8s \ud83d\udcc1 Mounting host path /tmp/spark-k8s into VM as /tmp/spark-k8s ... \u25aa Mount type: \u25aa User ID: docker \u25aa Group ID: docker \u25aa Version: 9p2000.L \u25aa Message Size: 262144 \u25aa Permissions: 755 (-rwxr-xr-x) \u25aa Options: map[] \u25aa Bind Address: 127.0.0.1:53125 \ud83d\ude80 Userspace file server: ufs starting \u2705 Successfully mounted /tmp/spark-k8s to /tmp/spark-k8s \ud83d\udccc NOTE: This process must stay alive for the mount to be accessible ...","title":"Mounting Filesystems"},{"location":"demo/spark-and-local-filesystem-in-minikube/#using-kubernetes-volumes","text":"Quoting Using Kubernetes Volumes of Apache Spark's official documentation: users can mount the following types of Kubernetes volumes into the driver and executor pods: hostPath: mounts a file or directory from the host node\u2019s filesystem into a pod. emptyDir: an initially empty volume created when a pod is assigned to a node. persistentVolumeClaim: used to mount a PersistentVolume into a pod.","title":"Using Kubernetes Volumes"},{"location":"demo/spark-and-local-filesystem-in-minikube/#hostpath","text":"Let's use Kubernetes' hostPath that requires spark.kubernetes.*.volumes -prefixed configuration properties and the name of the volume (under the volumes field in the pod specification): --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s The demo uses configuration properties to set up a hostPath volume type with spark-k8s name and /tmp/spark-k8s path on the host (for the driver and executors separately). ./bin/spark-submit \\ --master k8s://$K8S_SERVER \\ --deploy-mode cluster \\ --files test.me --conf spark.kubernetes.file.upload.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.driver.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.mount.path=/tmp/spark-k8s \\ --conf spark.kubernetes.executor.volumes.hostPath.spark-k8s.options.path=/tmp/spark-k8s \\ --name spark-docker-example \\ --class meetup.SparkApp \\ --conf spark.kubernetes.container.image=spark-docker-example:0.1.0 \\ --conf spark.kubernetes.driver.pod.name=spark-demo-minikube \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --verbose \\ local:///opt/docker/lib/meetup.spark-docker-example-0.1.0.jar","title":"hostPath"},{"location":"demo/spark-and-local-filesystem-in-minikube/#reviewing-volumes","text":"k describe po spark-demo-minikube $ k get po spark-demo-minikube -o=jsonpath='{.spec.volumes}' | jq [ { \"hostPath\": { \"path\": \"/tmp/spark-k8s\", \"type\": \"\" }, \"name\": \"spark-k8s\" }, { \"emptyDir\": {}, \"name\": \"spark-local-dir-1\" }, { \"configMap\": { \"defaultMode\": 420, \"name\": \"spark-docker-example-85c0cb76f277cdbe-driver-conf-map\" }, \"name\": \"spark-conf-volume\" }, { \"name\": \"spark-token-zd6jt\", \"secret\": { \"defaultMode\": 420, \"secretName\": \"spark-token-zd6jt\" } } ] $ k get po spark-demo-minikube -o=jsonpath='{.spec.containers[].volumeMounts}' | jq [ { \"mountPath\": \"/tmp/spark-k8s\", \"name\": \"spark-k8s\" }, { \"mountPath\": \"/var/data/spark-87d1ba7c-819a-4274-82ca-98c1e135c136\", \"name\": \"spark-local-dir-1\" }, { \"mountPath\": \"/opt/spark/conf\", \"name\": \"spark-conf-volume\" }, { \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"name\": \"spark-token-zd6jt\", \"readOnly\": true } ] Log into the driver pod container and review /tmp/spark-k8s directory. There should be at least one spark-upload directory. $ k exec -it spark-demo-minikube -- bash $ ls -ltr /tmp/spark-k8s total 5 ... drwxr-xr-x 1 1000 999 96 Jan 11 17:21 spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08 $ cat /tmp/spark-k8s/spark-upload-5ded6fb9-b5e7-4a09-9d0f-d3c8c85add08/test.me Hello World","title":"Reviewing Volumes"},{"location":"demo/spark-shell-on-minikube/","text":"Demo: spark-shell on minikube \u00b6 This demo shows how to run spark-shell on Kubernetes (using minikube ). Minikube \u00b6 Quoting the official documentation of Apache Spark: Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose, or customized to match an individual application\u2019s needs. It can be found in the kubernetes/dockerfiles/ directory. Start Cluster \u00b6 $ minikube start --cpus 4 --memory 8192 \ud83d\ude04 minikube v1.16.0 on Darwin 11.1 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udd25 Creating docker container (CPUs=4, Memory=8192MB) ... \ud83d\udc33 Preparing Kubernetes v1.20.0 on Docker 20.10.0 ... \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default $ kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:55020 KubeDNS is running at https://127.0.0.1:55020/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /Users/jacek/.minikube/ca.crt server: https://127.0.0.1:55020 name: minikube contexts: - context: cluster: minikube namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /Users/jacek/.minikube/profiles/minikube/client.crt client-key: /Users/jacek/.minikube/profiles/minikube/client.key $ kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-465jt 1/1 Running 0 47s kube-system etcd-minikube 0/1 Running 0 62s kube-system kube-apiserver-minikube 1/1 Running 0 62s kube-system kube-controller-manager-minikube 0/1 Running 0 62s kube-system kube-proxy-67plz 1/1 Running 0 48s kube-system kube-scheduler-minikube 0/1 Running 0 62s kube-system storage-provisioner 1/1 Running 1 62s Accessing Kubernetes Dashboard \u00b6 $ minikube dashboard \ud83d\udd0c Enabling dashboard ... \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... \ud83c\udf89 Opening http://127.0.0.1:55244/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... Building Spark Images \u00b6 In a separate terminal... cd $SPARK_HOME Tip Review kubernetes/dockerfiles/spark (in your Spark installation) or resource-managers/kubernetes/docker (in the Spark source code). Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) Build and publish the Spark images. Note -m option to point the shell script to use minikube's Docker daemon. $ ./bin/docker-image-tool.sh \\ -m \\ -b java_image_tag=11-jre-slim \\ -r jaceklaskowski \\ -t v3.0.1 \\ build ... Successfully tagged jaceklaskowski/spark:v3.0.1 List available images. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE jaceklaskowski/spark v3.0.1 d045e9e4572b 10 seconds ago 504MB openjdk 11-jre-slim 57a8cfbe60f3 4 weeks ago 205MB kubernetesui/dashboard v2.1.0 9a07b5b4bfac 4 weeks ago 226MB k8s.gcr.io/kube-proxy v1.20.0 10cc881966cf 4 weeks ago 118MB k8s.gcr.io/kube-apiserver v1.20.0 ca9843d3b545 4 weeks ago 122MB k8s.gcr.io/kube-scheduler v1.20.0 3138b6e3d471 4 weeks ago 46.4MB k8s.gcr.io/kube-controller-manager v1.20.0 b9fa1895dcaa 4 weeks ago 116MB gcr.io/k8s-minikube/storage-provisioner v4 85069258b98a 5 weeks ago 29.7MB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 4 months ago 253MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 9 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 11 months ago 683kB Creating Namespace \u00b6 Tip Learn more in Creating a new namespace . kubectl create namespace spark-demo $ kubectl get ns NAME STATUS AGE default Active 7m30s kube-node-lease Active 7m31s kube-public Active 7m31s kube-system Active 7m31s kubernetes-dashboard Active 5m51s spark-demo Active 11s Set spark-demo as the default namespace using kubens tool. kubens spark-demo Spark Logging \u00b6 Enable ALL logging level for Kubernetes-related loggers to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s=ALL log4j.logger.org.apache.spark.scheduler.cluster.k8s=ALL Refer to Logging . Launching spark-shell \u00b6 cd $SPARK_HOME K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') ./bin/spark-shell \\ --master k8s://$K8S_SERVER \\ --conf spark.kubernetes.container.image=jaceklaskowski/spark:v3.0.1 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --verbose 21/01/11 18:01:01 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using context minikube from users K8S config file 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Starting watch for pods with labels spark-app-selector=spark-application-1610384461937, spark-role=executor. 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Pod allocation status: 0 running, 0 pending, 0 unacknowledged. 21/01/11 18:01:02 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes. 21/01/11 18:01:02 DEBUG ExecutorPodsPollingSnapshotSource: Starting to check for executor pod state every 30000 ms. 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Requested executor with id 1 from Kubernetes. 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action ADDED 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action MODIFIED 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Requested executor with id 2 from Kubernetes. 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action MODIFIED 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Still waiting for 2 executors before requesting more. 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action ADDED 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action MODIFIED 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action MODIFIED 21/01/11 18:01:03 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action MODIFIED 21/01/11 18:01:03 DEBUG ExecutorPodsAllocator: Pod allocation status: 1 running, 1 pending, 0 unacknowledged. 21/01/11 18:01:03 DEBUG ExecutorPodsAllocator: Still waiting for 1 executors before requesting more. 21/01/11 18:01:04 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action MODIFIED 21/01/11 18:01:04 DEBUG ExecutorPodsAllocator: Pod allocation status: 2 running, 0 pending, 0 unacknowledged. 21/01/11 18:01:04 DEBUG ExecutorPodsAllocator: Current number of running executors is equal to the number of requested executors. Not scaling up further. 21/01/11 18:01:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.68.106:55654) with ID 1 21/01/11 18:01:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.68.106:55655) with ID 2 21/01/11 18:01:07 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8 Spark context Web UI available at http://192.168.68.106:4040 Spark context available as 'sc' (master = k8s://https://127.0.0.1:55020, app id = spark-application-1610384461937). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.9) Type in expressions to have them evaluated. Type :help for more information. scala> spark.version res0: String = 3.0.1 scala> sc.master res1: String = k8s://https://127.0.0.1:55020 web UIs \u00b6 Open web UI of the Spark application at http://localhost:4040/ . Review the pods in the Kubernetes UI. Make sure to switch to spark-demo namespace. Scaling Executors Up and Down \u00b6 Just for some more fun, in spark-shell , request two more executors and observe the logs. sc.requestTotalExecutors(numExecutors = 4, localityAwareTasks = 0, hostToLocalTaskCount = Map.empty) sc.killExecutors(Seq(\"1\", \"3\")) Review the number of executors at http://localhost:4040/executors/ and in the Kubernetes UI. Stopping Cluster \u00b6 minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all","title":"spark-shell on minikube"},{"location":"demo/spark-shell-on-minikube/#demo-spark-shell-on-minikube","text":"This demo shows how to run spark-shell on Kubernetes (using minikube ).","title":"Demo: spark-shell on minikube"},{"location":"demo/spark-shell-on-minikube/#minikube","text":"Quoting the official documentation of Apache Spark: Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose, or customized to match an individual application\u2019s needs. It can be found in the kubernetes/dockerfiles/ directory.","title":"Minikube"},{"location":"demo/spark-shell-on-minikube/#start-cluster","text":"$ minikube start --cpus 4 --memory 8192 \ud83d\ude04 minikube v1.16.0 on Darwin 11.1 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udd25 Creating docker container (CPUs=4, Memory=8192MB) ... \ud83d\udc33 Preparing Kubernetes v1.20.0 on Docker 20.10.0 ... \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default $ kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:55020 KubeDNS is running at https://127.0.0.1:55020/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /Users/jacek/.minikube/ca.crt server: https://127.0.0.1:55020 name: minikube contexts: - context: cluster: minikube namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /Users/jacek/.minikube/profiles/minikube/client.crt client-key: /Users/jacek/.minikube/profiles/minikube/client.key $ kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-465jt 1/1 Running 0 47s kube-system etcd-minikube 0/1 Running 0 62s kube-system kube-apiserver-minikube 1/1 Running 0 62s kube-system kube-controller-manager-minikube 0/1 Running 0 62s kube-system kube-proxy-67plz 1/1 Running 0 48s kube-system kube-scheduler-minikube 0/1 Running 0 62s kube-system storage-provisioner 1/1 Running 1 62s","title":"Start Cluster"},{"location":"demo/spark-shell-on-minikube/#accessing-kubernetes-dashboard","text":"$ minikube dashboard \ud83d\udd0c Enabling dashboard ... \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... \ud83c\udf89 Opening http://127.0.0.1:55244/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...","title":"Accessing Kubernetes Dashboard"},{"location":"demo/spark-shell-on-minikube/#building-spark-images","text":"In a separate terminal... cd $SPARK_HOME Tip Review kubernetes/dockerfiles/spark (in your Spark installation) or resource-managers/kubernetes/docker (in the Spark source code). Point the shell to minikube's Docker daemon. eval $(minikube -p minikube docker-env) Build and publish the Spark images. Note -m option to point the shell script to use minikube's Docker daemon. $ ./bin/docker-image-tool.sh \\ -m \\ -b java_image_tag=11-jre-slim \\ -r jaceklaskowski \\ -t v3.0.1 \\ build ... Successfully tagged jaceklaskowski/spark:v3.0.1 List available images. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE jaceklaskowski/spark v3.0.1 d045e9e4572b 10 seconds ago 504MB openjdk 11-jre-slim 57a8cfbe60f3 4 weeks ago 205MB kubernetesui/dashboard v2.1.0 9a07b5b4bfac 4 weeks ago 226MB k8s.gcr.io/kube-proxy v1.20.0 10cc881966cf 4 weeks ago 118MB k8s.gcr.io/kube-apiserver v1.20.0 ca9843d3b545 4 weeks ago 122MB k8s.gcr.io/kube-scheduler v1.20.0 3138b6e3d471 4 weeks ago 46.4MB k8s.gcr.io/kube-controller-manager v1.20.0 b9fa1895dcaa 4 weeks ago 116MB gcr.io/k8s-minikube/storage-provisioner v4 85069258b98a 5 weeks ago 29.7MB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 4 months ago 253MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 6 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 9 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 11 months ago 683kB","title":"Building Spark Images"},{"location":"demo/spark-shell-on-minikube/#creating-namespace","text":"Tip Learn more in Creating a new namespace . kubectl create namespace spark-demo $ kubectl get ns NAME STATUS AGE default Active 7m30s kube-node-lease Active 7m31s kube-public Active 7m31s kube-system Active 7m31s kubernetes-dashboard Active 5m51s spark-demo Active 11s Set spark-demo as the default namespace using kubens tool. kubens spark-demo","title":"Creating Namespace"},{"location":"demo/spark-shell-on-minikube/#spark-logging","text":"Enable ALL logging level for Kubernetes-related loggers to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.k8s=ALL log4j.logger.org.apache.spark.scheduler.cluster.k8s=ALL Refer to Logging .","title":"Spark Logging"},{"location":"demo/spark-shell-on-minikube/#launching-spark-shell","text":"cd $SPARK_HOME K8S_SERVER=$(kubectl config view --output=jsonpath='{.clusters[].cluster.server}') ./bin/spark-shell \\ --master k8s://$K8S_SERVER \\ --conf spark.kubernetes.container.image=jaceklaskowski/spark:v3.0.1 \\ --conf spark.kubernetes.context=minikube \\ --conf spark.kubernetes.namespace=spark-demo \\ --verbose 21/01/11 18:01:01 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using context minikube from users K8S config file 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Starting watch for pods with labels spark-app-selector=spark-application-1610384461937, spark-role=executor. 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Pod allocation status: 0 running, 0 pending, 0 unacknowledged. 21/01/11 18:01:02 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes. 21/01/11 18:01:02 DEBUG ExecutorPodsPollingSnapshotSource: Starting to check for executor pod state every 30000 ms. 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Requested executor with id 1 from Kubernetes. 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action ADDED 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action MODIFIED 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Requested executor with id 2 from Kubernetes. 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action MODIFIED 21/01/11 18:01:02 DEBUG ExecutorPodsAllocator: Still waiting for 2 executors before requesting more. 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action ADDED 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action MODIFIED 21/01/11 18:01:02 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action MODIFIED 21/01/11 18:01:03 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-1, action MODIFIED 21/01/11 18:01:03 DEBUG ExecutorPodsAllocator: Pod allocation status: 1 running, 1 pending, 0 unacknowledged. 21/01/11 18:01:03 DEBUG ExecutorPodsAllocator: Still waiting for 1 executors before requesting more. 21/01/11 18:01:04 DEBUG ExecutorPodsWatchSnapshotSource: Received executor pod update for pod named spark-shell-28fe8176f264cdab-exec-2, action MODIFIED 21/01/11 18:01:04 DEBUG ExecutorPodsAllocator: Pod allocation status: 2 running, 0 pending, 0 unacknowledged. 21/01/11 18:01:04 DEBUG ExecutorPodsAllocator: Current number of running executors is equal to the number of requested executors. Not scaling up further. 21/01/11 18:01:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.68.106:55654) with ID 1 21/01/11 18:01:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.68.106:55655) with ID 2 21/01/11 18:01:07 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8 Spark context Web UI available at http://192.168.68.106:4040 Spark context available as 'sc' (master = k8s://https://127.0.0.1:55020, app id = spark-application-1610384461937). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.9) Type in expressions to have them evaluated. Type :help for more information. scala> spark.version res0: String = 3.0.1 scala> sc.master res1: String = k8s://https://127.0.0.1:55020","title":"Launching spark-shell"},{"location":"demo/spark-shell-on-minikube/#web-uis","text":"Open web UI of the Spark application at http://localhost:4040/ . Review the pods in the Kubernetes UI. Make sure to switch to spark-demo namespace.","title":"web UIs"},{"location":"demo/spark-shell-on-minikube/#scaling-executors-up-and-down","text":"Just for some more fun, in spark-shell , request two more executors and observe the logs. sc.requestTotalExecutors(numExecutors = 4, localityAwareTasks = 0, hostToLocalTaskCount = Map.empty) sc.killExecutors(Seq(\"1\", \"3\")) Review the number of executors at http://localhost:4040/executors/ and in the Kubernetes UI.","title":"Scaling Executors Up and Down"},{"location":"demo/spark-shell-on-minikube/#stopping-cluster","text":"minikube stop Optionally (e.g. to start from scratch next time), delete all of the minikube clusters: minikube delete --all","title":"Stopping Cluster"}]}